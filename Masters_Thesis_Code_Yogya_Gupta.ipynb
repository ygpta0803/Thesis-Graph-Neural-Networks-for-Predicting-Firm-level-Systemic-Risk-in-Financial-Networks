{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "wh8BeTetNoJl",
        "svLjdU6rQmtd",
        "Cnetg0liRrfu",
        "gk2F4lmzS03d",
        "WUQMt3pmT_do",
        "iYn7wvdxUqIF",
        "DqRBqGmrWB9e",
        "87PiqzxsbzM7",
        "5AogavhzdrH6",
        "9KJbVairfIAA"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdnTqzUXNGkU"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# -------------------- Standard library --------------------\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "import warnings\n",
        "from datetime import datetime, timedelta\n",
        "from calendar import monthrange\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# -------------------- PyTorch & PyG --------------------\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "try:\n",
        "    # core data container for PyTorch Geometric\n",
        "    from torch_geometric.data import Data\n",
        "    # pick what you actually use; add more as needed\n",
        "    from torch_geometric.nn import GCNConv  # e.g., GCN; swap/add TAGConv, SAGEConv, etc.\n",
        "except Exception as e:\n",
        "    raise ImportError(\n",
        "        \"torch_geometric is not available. Install the versions pinned in requirements.txt \"\n",
        "        \"or remove GNN code paths.\"\n",
        "    ) from e\n",
        "\n",
        "# -------------------- Time-series / econometrics --------------------\n",
        "# VAR is used for FEVD labels, etc.\n",
        "from statsmodels.tsa.api import VAR\n",
        "\n",
        "# -------------------- Data sources (market / news) --------------------\n",
        "# yfinance for prices\n",
        "import yfinance as yf\n",
        "\n",
        "# news/sentiment dependencies\n",
        "try:\n",
        "    import requests\n",
        "    from bs4 import BeautifulSoup\n",
        "    import feedparser\n",
        "    from dateutil import parser as dtp\n",
        "    from urllib.parse import urlencode, quote_plus\n",
        "\n",
        "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "    _TRANSFORMERS_AVAILABLE = True\n",
        "except Exception:\n",
        "    _TRANSFORMERS_AVAILABLE = False\n",
        "\n",
        "# -------------------- Global display & warnings --------------------\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.set_option(\"display.max_columns\", 200)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data Preprocessing\n",
        "\n",
        "This section builds the full weekly panel used throughout the dissertation. It:\n",
        "\n",
        "- Downloads and filters daily adjusted close for the candidate ticker set, then keeps only instruments with nearly complete coverage (strict missingness and start/end tolerances).\n",
        "\n",
        "- Aggregates to weekly (Fri) and computes features: returns, rolling vol (4w, 8w), beta(26w) vs equal-weight market, size, turnover (weekly volume ÷ 26w avg), and VIX replicated per node.\n",
        "\n",
        "- Constructs rolling correlation graphs (104w window, |ρ| ≥ 0.6) and exports degree and eigenvector centralities as weekly features.\n",
        "\n",
        "- Scrapes/loads headlines, scores with FinBERT, and produces weekly sentiment mean/vol/change features (shifted by +1 week to avoid leakage)"
      ],
      "metadata": {
        "id": "wh8BeTetNoJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================ Section 1: Data preprocessing ============================\n",
        "from pathlib import Path\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import networkx as nx\n",
        "from datetime import timedelta\n",
        "from typing import Tuple, Dict, List\n",
        "\n",
        "# --- config ---\n",
        "CFG = {\n",
        "    \"seed\": 42,\n",
        "    \"data_dir\": \"data\",              # root for artifacts\n",
        "    \"start\": \"2017-01-01\",\n",
        "    \"end\":   \"2025-01-01\",\n",
        "    \"weekly_freq\": \"W-FRI\",\n",
        "    \"cor_win_weeks\": 104,            # for graph window\n",
        "    \"corr_threshold\": 0.60,          # |rho| >= THETA\n",
        "    \"beta_win\": 26,                  # weeks\n",
        "    \"turnover_win\": 26,              # weeks\n",
        "    \"vol_wins\": [4, 8],\n",
        "    \"tickers\": [\n",
        "        \"JPM\",\"BAC\",\"WFC\",\"C\",\"GS\",\"MS\",\"PNC\",\"USB\",\"BK\",\"STT\",\n",
        "        \"TFC\",\"MTB\",\"NTRS\",\"FITB\",\"HBAN\",\"CMA\",\"ZION\",\"RF\",\"KEY\",\"WAL\",\n",
        "        \"ALLY\",\"COF\",\"AXP\",\"CFR\",\n",
        "        # collapsed/absorbed to be filtered by coverage\n",
        "        \"SIVB\",\"SBNY\",\"CS\",\"PACW\"\n",
        "    ],\n",
        "}\n",
        "\n",
        "# -------------------- helpers --------------------\n",
        "def ensure_dir(p: Path) -> None:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def save_parquet(df: pd.DataFrame, path: Path, log: logging.Logger) -> None:\n",
        "    df.to_parquet(path)\n",
        "    log.info(f\"Saved {path} | shape={df.shape} | null%≈{df.isna().mean().mean():.2%}\")\n",
        "\n",
        "# -------------------- prices --------------------\n",
        "def download_prices(tickers: List[str], start: str, end: str, auto_adjust: bool = True) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Download daily prices via yfinance and return a wide Close matrix (index=date, columns=tickers).\n",
        "    \"\"\"\n",
        "    data = yf.download(\n",
        "        tickers, start=start, end=end,\n",
        "        auto_adjust=auto_adjust, progress=False, group_by=\"ticker\", threads=True\n",
        "    )\n",
        "    if isinstance(tickers, str): tickers = [tickers]\n",
        "\n",
        "    if len(tickers) == 1:\n",
        "        close = data[\"Close\"].to_frame(tickers[0])\n",
        "    else:\n",
        "        cols = {}\n",
        "        for t in tickers:\n",
        "            try:\n",
        "                cols[t] = data[t][\"Close\"]\n",
        "            except Exception:\n",
        "                cols[t] = pd.Series(dtype=float)\n",
        "        close = pd.DataFrame(cols)\n",
        "    return close.sort_index()\n",
        "\n",
        "def filter_full_coverage(\n",
        "    px: pd.DataFrame,\n",
        "    start: str, end: str,\n",
        "    max_nan_ratio: float = 0.01,\n",
        "    tol_start_days: int = 10,\n",
        "    tol_end_days: int = 10\n",
        ") -> Tuple[List[str], Dict[str, str]]:\n",
        "    \"\"\"\n",
        "    Keep tickers whose first/last valid dates are near the requested window and with low missingness.\n",
        "    Returns (kept_tickers, dropped_reason).\n",
        "    \"\"\"\n",
        "    start = pd.to_datetime(start)\n",
        "    end = pd.to_datetime(end)\n",
        "    kept, dropped = [], {}\n",
        "    for t in px.columns:\n",
        "        s = px[t].dropna()\n",
        "        if s.empty:\n",
        "            dropped[t] = \"no data\"; continue\n",
        "        first_ok = s.index.min() <= (start + timedelta(days=tol_start_days))\n",
        "        last_ok  = s.index.max() >= (end   - timedelta(days=tol_end_days))\n",
        "        nan_ratio = float(px[t].isna().mean())\n",
        "        if not first_ok:\n",
        "            dropped[t] = f\"starts too late (first={s.index.min().date()})\"; continue\n",
        "        if not last_ok:\n",
        "            dropped[t] = f\"ends early (last={s.index.max().date()})\"; continue\n",
        "        if nan_ratio > max_nan_ratio:\n",
        "            dropped[t] = f\"too many NaNs ({nan_ratio:.2%})\"; continue\n",
        "        kept.append(t)\n",
        "    return kept, dropped\n",
        "\n",
        "# -------------------- weekly features --------------------\n",
        "def compute_weekly_core_features(\n",
        "    px_daily: pd.DataFrame,\n",
        "    weekly_freq: str,\n",
        "    beta_win: int,\n",
        "    vol_wins: List[int],\n",
        "    turnover_win: int,\n",
        "    log: logging.Logger\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Build weekly features: returns, vols, beta vs EW market, size (log price), turnover, VIX replicated.\n",
        "    \"\"\"\n",
        "    tickers = list(px_daily.columns)\n",
        "\n",
        "    # Weekly close & returns\n",
        "    px_w = px_daily.resample(weekly_freq).last().ffill().dropna(how=\"all\")\n",
        "    ret_w = px_w.pct_change().dropna()\n",
        "\n",
        "    # Rolling vol\n",
        "    feats = {\"ret\": ret_w}\n",
        "    for w in vol_wins:\n",
        "        feats[f\"vol{w}\"] = ret_w.rolling(w, min_periods=w).std()\n",
        "\n",
        "    # Beta(26w) vs EW market\n",
        "    mkt = ret_w.mean(axis=1)\n",
        "    def rolling_beta(series, market, win=beta_win):\n",
        "        cov = series.rolling(win, min_periods=win).cov(market)\n",
        "        var = market.rolling(win, min_periods=win).var()\n",
        "        return cov / var\n",
        "    feats[\"beta26\"] = ret_w.apply(lambda col: rolling_beta(col, mkt, beta_win))\n",
        "\n",
        "    # Size proxy (log price)\n",
        "    feats[\"size\"] = np.log(px_w.replace(0, np.nan))\n",
        "\n",
        "    # Turnover from volume\n",
        "    log.info(\"Downloading volume for turnover…\")\n",
        "    raw = yf.download(tickers, start=px_daily.index.min().date(), end=px_daily.index.max().date(),\n",
        "                      auto_adjust=False, progress=False)\n",
        "    # normalize to wide \"Volume\" frame\n",
        "    if isinstance(raw.columns, pd.MultiIndex):\n",
        "        if \"Volume\" in raw.columns.levels[0]:\n",
        "            vol_raw = raw[\"Volume\"]\n",
        "        else:\n",
        "            # ticker-first layout\n",
        "            vols = {t: raw[(t, \"Volume\")] if (t, \"Volume\") in raw.columns else pd.Series(dtype=float) for t in tickers}\n",
        "            vol_raw = pd.DataFrame(vols)\n",
        "    else:\n",
        "        if \"Volume\" in raw.columns:\n",
        "            colname = tickers[0] if len(tickers) == 1 else \"Volume\"\n",
        "            v = raw[\"Volume\"]\n",
        "            vol_raw = v.to_frame(colname) if len(tickers) == 1 else v\n",
        "        else:\n",
        "            vol_raw = pd.DataFrame({t: pd.Series(dtype=float) for t in tickers})\n",
        "\n",
        "    vol_raw = vol_raw.sort_index().reindex(px_daily.index).ffill(limit=3)\n",
        "    vol_w = vol_raw.resample(weekly_freq).sum().reindex(px_w.index)\n",
        "    turnover = (vol_w / vol_w.rolling(turnover_win, min_periods=turnover_win).mean()).replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    feats[\"turnover\"] = turnover\n",
        "\n",
        "    # VIX replicated per node\n",
        "    log.info(\"Downloading VIX…\")\n",
        "    vix_close = yf.download(\"^VIX\", start=px_daily.index.min().date(), end=px_daily.index.max().date(),\n",
        "                            auto_adjust=False, progress=False)[\"Close\"]\n",
        "    vix_w = vix_close.resample(weekly_freq).last().reindex(px_w.index).ffill()\n",
        "    feats[\"vix\"] = pd.DataFrame(\n",
        "        np.repeat(vix_w.values.reshape(-1,1), len(tickers), axis=1),\n",
        "        index=vix_w.index, columns=tickers\n",
        "    )\n",
        "\n",
        "    return feats\n",
        "\n",
        "# -------------------- graph centralities --------------------\n",
        "def compute_centralities(\n",
        "    ret_w: pd.DataFrame,\n",
        "    window_weeks: int,\n",
        "    theta: float,\n",
        "    log: logging.Logger\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    For each target week t, compute degree and eigenvector centrality from the correlation\n",
        "    graph built on returns over [t-window, t).\n",
        "    \"\"\"\n",
        "    dates = ret_w.index\n",
        "    tickers = list(ret_w.columns)\n",
        "    deg_rows, ec_rows = [], []\n",
        "\n",
        "    log.info(f\"Computing centralities over {len(dates)} weeks (W={window_weeks}, θ={theta})…\")\n",
        "    for d in dates:\n",
        "        t_idx = ret_w.index.get_loc(d)\n",
        "        if t_idx < window_weeks:\n",
        "            # not enough history yet\n",
        "            zero = pd.Series({t: 0.0 for t in tickers}, name=d)\n",
        "            deg_rows.append(zero.copy()); ec_rows.append(zero.copy()); continue\n",
        "\n",
        "        win = ret_w.iloc[t_idx-window_weeks:t_idx]\n",
        "        C = win.corr().abs().fillna(0.0)\n",
        "\n",
        "        G = nx.Graph()\n",
        "        G.add_nodes_from(tickers)\n",
        "        # thresholded weighted edges\n",
        "        for i in range(len(tickers)):\n",
        "            for j in range(i+1, len(tickers)):\n",
        "                w = float(C.iat[i, j])\n",
        "                if w >= theta:\n",
        "                    G.add_edge(tickers[i], tickers[j], weight=w)\n",
        "\n",
        "        deg_s = pd.Series(nx.degree_centrality(G), name=d).reindex(tickers).fillna(0.0)\n",
        "        try:\n",
        "            ec_s = pd.Series(nx.eigenvector_centrality_numpy(G, weight=\"weight\"), name=d).reindex(tickers).fillna(0.0)\n",
        "        except Exception:\n",
        "            ec_s = pd.Series({t: 0.0 for t in tickers}, name=d)\n",
        "\n",
        "        deg_rows.append(deg_s); ec_rows.append(ec_s)\n",
        "\n",
        "    DEG = pd.DataFrame(deg_rows).sort_index()\n",
        "    EVC = pd.DataFrame(ec_rows).sort_index()\n",
        "    return {\"degree\": DEG, \"eigencent\": EVC}\n",
        "\n",
        "# -------------------- optional: weekly FinBERT sentiment --------------------\n",
        "def build_weekly_sentiment(\n",
        "    news_df: pd.DataFrame,\n",
        "    weekly_freq: str\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Expect columns: ['Ticker','Date','Headline'] at minimum.\n",
        "    Returns wide weekly panels: sent_mean, sent_vol, sent_change.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "        import torch\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\"Transformers not installed. Install optional deps to compute sentiment.\") from e\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device); model.eval()\n",
        "\n",
        "    def finbert_scores(texts, batch_size=32, max_len=512):\n",
        "        scores = []\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch = [t if isinstance(t, str) and t.strip() else \"\" for t in texts[i:i+batch_size]]\n",
        "            enc = tokenizer(batch, return_tensors=\"pt\", truncation=True, padding=True, max_length=max_len)\n",
        "            enc = {k: v.to(device) for k, v in enc.items()}\n",
        "            with torch.no_grad():\n",
        "                out = model(**enc)\n",
        "                probs = torch.nn.functional.softmax(out.logits, dim=-1)  # [neg, neu, pos]\n",
        "            batch_scores = (probs[:,2] - probs[:,0]).detach().cpu().numpy().tolist()\n",
        "            for j, b in enumerate(batch):\n",
        "                if b == \"\": batch_scores[j] = np.nan\n",
        "            scores.extend(batch_scores)\n",
        "        return np.array(scores, dtype=float)\n",
        "\n",
        "    df = news_df.copy()\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"Ticker\",\"Date\",\"Headline\"])\n",
        "    df[\"sentiment\"] = finbert_scores(df[\"Headline\"].tolist())\n",
        "    df[\"WeekEnd\"] = df[\"Date\"].dt.to_period(weekly_freq).dt.to_timestamp(weekly_freq)\n",
        "\n",
        "    weekly = (df.groupby([\"Ticker\",\"WeekEnd\"])[\"sentiment\"]\n",
        "                .agg(sent_mean=\"mean\", sent_vol=\"std\").reset_index())\n",
        "    weekly[\"sent_change\"] = (weekly.sort_values([\"Ticker\",\"WeekEnd\"])\n",
        "                                   .groupby(\"Ticker\")[\"sent_mean\"].diff())\n",
        "\n",
        "    def wide(col):\n",
        "        w = weekly.pivot(index=\"WeekEnd\", columns=\"Ticker\", values=col).sort_index()\n",
        "        w.index.name = \"Date\"; return w.astype(\"float64\")\n",
        "\n",
        "    return {\n",
        "        \"sent_mean\": wide(\"sent_mean\").fillna(0.0),\n",
        "        \"sent_vol\":  wide(\"sent_vol\").fillna(0.0),\n",
        "        \"sent_change\": wide(\"sent_change\").fillna(0.0),\n",
        "    }\n",
        "\n",
        "# -------------------- main runner for preprocessing --------------------\n",
        "def run_preprocessing(cfg: dict, log: logging.Logger) -> None:\n",
        "    base = Path(cfg[\"data_dir\"]); ensure_dir(base)\n",
        "    feats_dir = base / \"feat\"; ensure_dir(feats_dir)\n",
        "\n",
        "    # Prices\n",
        "    log.info(\"Downloading adjusted close prices…\")\n",
        "    px = download_prices(cfg[\"tickers\"], cfg[\"start\"], cfg[\"end\"], auto_adjust=True).ffill(limit=3)\n",
        "    kept, dropped = filter_full_coverage(px, cfg[\"start\"], cfg[\"end\"], max_nan_ratio=0.01,\n",
        "                                         tol_start_days=10, tol_end_days=10)\n",
        "    log.info(f\"KEPT ({len(kept)}): {kept}\")\n",
        "    for k,v in dropped.items(): log.info(f\"DROPPED {k}: {v}\")\n",
        "    px_good = px[kept]\n",
        "    save_parquet(px_good, base / \"prices_daily.parquet\", log)\n",
        "\n",
        "    # Weekly features\n",
        "    feats = compute_weekly_core_features(\n",
        "        px_good, cfg[\"weekly_freq\"], cfg[\"beta_win\"], cfg[\"vol_wins\"], cfg[\"turnover_win\"], log\n",
        "    )\n",
        "    # persist\n",
        "    for name, df in feats.items():\n",
        "        # consistent names with your original files\n",
        "        out_name = {\n",
        "            \"ret\":\"feat_ret\", \"beta26\":\"feat_beta26\", \"size\":\"feat_size\",\n",
        "            \"turnover\":\"feat_turnover\", \"vix\":\"feat_vix\",\n",
        "            f\"vol{cfg['vol_wins'][0]}\": f\"feat_vol{cfg['vol_wins'][0]}\",\n",
        "            f\"vol{cfg['vol_wins'][1]}\": f\"feat_vol{cfg['vol_wins'][1]}\",\n",
        "        }.get(name, f\"feat_{name}\")\n",
        "        save_parquet(df.reindex(columns=kept).astype(float), feats_dir / f\"{out_name}.parquet\", log)\n",
        "\n",
        "    # Centralities (from returns)\n",
        "    ret_w = feats[\"ret\"].sort_index()\n",
        "    cents = compute_centralities(ret_w, cfg[\"cor_win_weeks\"], cfg[\"corr_threshold\"], log)\n",
        "    save_parquet(cents[\"degree\"],    feats_dir / \"feat_degree.parquet\", log)\n",
        "    save_parquet(cents[\"eigencent\"], feats_dir / \"feat_eigencent.parquet\", log)\n",
        "\n",
        "    log.info(\"Preprocessing complete.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jEV-m3HJNsgN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Feature Engineering\n",
        "\n",
        "This step merges all saved weekly feature panels into a single (Week, Ticker) long table, handles warm-up weeks (zero-only signals), trims to the first fully informative week, and standardises features:\n",
        "\n",
        "Local, node-varying features (e.g., returns, degree, eigenvector, turnover, vols, beta, size) are z-scored within each week to preserve cross-sectional structure without peeking across time.\n",
        "\n",
        "Global, market-wide features (e.g., VIX) are standardised using training-period statistics only to avoid leakage."
      ],
      "metadata": {
        "id": "svLjdU6rQmtd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================ Section 2: Feature Engineering ============================\n",
        "from pathlib import Path\n",
        "import os, json, numpy as np, pandas as pd\n",
        "from functools import reduce\n",
        "\n",
        "# ---- config  ----\n",
        "FEAT_DIR = Path(\"data/feat\")         # where feat_*.parquet live\n",
        "OUT_DIR  = Path(\"data\")              # output folder\n",
        "RET_FILE = \"feat_ret.parquet\"        # required returns panel\n",
        "WEEKLY_FREQ = \"W-FRI\"\n",
        "\n",
        "# To avoid leakage in global z-scales; set last week of training split (e.g. \"2023-12-29\")\n",
        "TRAIN_END: str | None = None  # e.g., \"2023-12-29\"\n",
        "\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# -------------------- helpers --------------------\n",
        "def to_wfri_index(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Coerce index to week-ending Friday timestamps and sort.\"\"\"\n",
        "    idx = pd.to_datetime(df.index)\n",
        "    df = df.copy()\n",
        "    df.index = idx.to_period(WEEKLY_FREQ).end_time.normalize()\n",
        "    return df.sort_index()\n",
        "\n",
        "def melt_feature(df_wide: pd.DataFrame, name: str) -> pd.DataFrame:\n",
        "    \"\"\"Wide (weeks × tickers) -> long (Week, Ticker, name).\"\"\"\n",
        "    d = df_wide.copy()\n",
        "    d[\"Week\"] = d.index\n",
        "    return d.melt(id_vars=\"Week\", var_name=\"Ticker\", value_name=name)\n",
        "\n",
        "def zscore_per_week(df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
        "    \"\"\"Within-week z-score for node-level columns (cross-sectional standardization).\"\"\"\n",
        "    g = df.groupby(\"Week\", sort=False)\n",
        "    for c in cols:\n",
        "        mu = g[c].transform(\"mean\")\n",
        "        sd = g[c].transform(\"std\").replace(0, np.nan)\n",
        "        df[c + \"_z\"] = (df[c] - mu) / sd\n",
        "    return df\n",
        "\n",
        "def mark_all_zero_weeks_as_nan(df_long: pd.DataFrame, col: str) -> None:\n",
        "    \"\"\"Treat weeks where a feature is identically zero across all tickers as missing (warm-up).\"\"\"\n",
        "    if col not in df_long.columns:\n",
        "        return\n",
        "    zero_weeks = (\n",
        "        df_long.groupby(\"Week\")[col]\n",
        "               .apply(lambda s: np.nanmax(np.abs(s.fillna(0.0))) == 0.0)\n",
        "    )\n",
        "    zw = zero_weeks[zero_weeks].index\n",
        "    if len(zw):\n",
        "        df_long.loc[df_long[\"Week\"].isin(zw), col] = np.nan\n",
        "\n",
        "def first_valid_week(df_long: pd.DataFrame, cols: list[str]) -> pd.Timestamp | None:\n",
        "    \"\"\"Latest among the first weeks where each key column becomes available.\"\"\"\n",
        "    firsts = []\n",
        "    for c in cols:\n",
        "        if c in df_long.columns:\n",
        "            w = df_long.dropna(subset=[c])[\"Week\"].min()\n",
        "            if pd.notna(w): firsts.append(pd.to_datetime(w))\n",
        "    return max(firsts) if firsts else None\n",
        "\n",
        "# -------------------- 1) Load weekly panels --------------------\n",
        "ret_path = FEAT_DIR / RET_FILE\n",
        "if not ret_path.exists():\n",
        "    raise FileNotFoundError(f\"Missing returns parquet: {ret_path}\")\n",
        "\n",
        "wk_returns = pd.read_parquet(ret_path)\n",
        "wk_returns = wk_returns.set_index(\"Week\") if \"Week\" in wk_returns.columns else wk_returns\n",
        "wk_returns = to_wfri_index(wk_returns)\n",
        "\n",
        "# Load all other feat_*.parquet, skipping returns and any labels\n",
        "feat_dict: dict[str, pd.DataFrame] = {}\n",
        "for p in FEAT_DIR.glob(\"*.parquet\"):\n",
        "    stem = p.stem.lower()\n",
        "    if p.name == RET_FILE or stem.startswith(\"labels\") or \"labels\" in stem:\n",
        "        continue\n",
        "    df = pd.read_parquet(p)\n",
        "    df = df.set_index(\"Week\") if \"Week\" in df.columns else df\n",
        "    df = to_wfri_index(df)\n",
        "    feat_dict[p.stem] = df\n",
        "\n",
        "print(f\"[INFO] wk_returns shape: {wk_returns.shape} | loaded other panels: {len(feat_dict)}\")\n",
        "\n",
        "# -------------------- 2) Melt & merge into long --------------------\n",
        "long_parts = [melt_feature(wk_returns, \"feat_ret\")]\n",
        "for name, df in feat_dict.items():\n",
        "    long_parts.append(melt_feature(df, name))\n",
        "\n",
        "feat_long = reduce(lambda a, b: a.merge(b, on=[\"Week\", \"Ticker\"], how=\"outer\"), long_parts)\n",
        "feat_long = feat_long.sort_values([\"Week\", \"Ticker\"]).reset_index(drop=True)\n",
        "\n",
        "# Drop columns that are entirely NaN\n",
        "value_cols = [c for c in feat_long.columns if c not in [\"Week\",\"Ticker\"]]\n",
        "all_null = [c for c in value_cols if feat_long[c].isna().all()]\n",
        "if all_null:\n",
        "    feat_long = feat_long.drop(columns=all_null)\n",
        "    value_cols = [c for c in feat_long.columns if c not in [\"Week\",\"Ticker\"]]\n",
        "    print(\"[INFO] Dropped all-NaN columns:\", all_null)\n",
        "\n",
        "# Force numeric (non-numeric → NaN)\n",
        "for c in value_cols:\n",
        "    feat_long[c] = pd.to_numeric(feat_long[c], errors=\"coerce\")\n",
        "\n",
        "# -------------------- 3) Handle warm-up all-zero weeks --------------------\n",
        "warmup_prefixes = [\"feat_vol4\", \"feat_vol8\", \"feat_turnover\", \"feat_degree\", \"feat_eigencent\", \"feat_beta26\"]\n",
        "warmup_cols = [c for c in value_cols if any(c.startswith(px) for px in warmup_prefixes)]\n",
        "for c in warmup_cols:\n",
        "    mark_all_zero_weeks_as_nan(feat_long, c)\n",
        "\n",
        "# -------------------- 4) Trim to first fully informative week --------------------\n",
        "# choose key cols to require (if missing, they’re ignored)\n",
        "candidate_keys = [\"feat_ret\",\"feat_size\",\"feat_vol4\",\"feat_vol8\",\"feat_degree\",\"feat_eigencent\",\"feat_turnover\",\"feat_beta26\"]\n",
        "key_cols = [c for c in candidate_keys if c in feat_long.columns]\n",
        "\n",
        "fv = first_valid_week(feat_long, key_cols)\n",
        "if fv is not None:\n",
        "    feat_long = feat_long[feat_long[\"Week\"] >= fv].copy()\n",
        "    wk_returns = wk_returns[wk_returns.index >= fv].copy()\n",
        "print(\"[INFO] First valid week used:\", fv)\n",
        "\n",
        "# -------------------- 5) Detect global features & z-score locals --------------------\n",
        "# Globals = constant across tickers within a week (e.g., feat_vix)\n",
        "grp = feat_long.groupby(\"Week\", sort=False)\n",
        "global_feats = []\n",
        "for c in value_cols:\n",
        "    nun = grp[c].nunique(dropna=True)\n",
        "    if (nun.fillna(1) <= 1).all():\n",
        "        global_feats.append(c)\n",
        "local_feats = [c for c in value_cols if c not in global_feats]\n",
        "\n",
        "print(\"[INFO] Global features:\", global_feats)\n",
        "print(\"[INFO] Local features to per-week z-score: count=\", len(local_feats))\n",
        "\n",
        "# Remove any stale *_z from prior runs\n",
        "feat_long = feat_long[[c for c in feat_long.columns if not c.endswith(\"_z\")]].copy()\n",
        "\n",
        "# Cross-sectional (within-week) z for locals - no temporal leakage\n",
        "feat_long = zscore_per_week(feat_long, local_feats)\n",
        "\n",
        "# Global z for globals - use TRAIN_END to avoid leakage; else whole-sample (fallback)\n",
        "if TRAIN_END is not None:\n",
        "    train_cut = pd.to_datetime(TRAIN_END)\n",
        "    mask_train = pd.to_datetime(feat_long[\"Week\"]) <= train_cut\n",
        "else:\n",
        "    mask_train = slice(None)  # all rows\n",
        "\n",
        "for c in global_feats:\n",
        "    mu = feat_long.loc[mask_train, c].mean()\n",
        "    sd = feat_long.loc[mask_train, c].std()\n",
        "    sd = sd if (sd and not np.isnan(sd) and sd != 0) else 1.0\n",
        "    feat_long[c + \"_global_z\"] = (feat_long[c] - mu) / sd\n",
        "\n",
        "# Modeling columns = all *_z (locals) + *_global_z (globals)\n",
        "X_cols = [c for c in feat_long.columns if c.endswith(\"_z\")]\n",
        "feat_long[X_cols] = feat_long[X_cols].fillna(0.0)\n",
        "\n",
        "# -------------------- 6) Save artifacts --------------------\n",
        "full_path  = OUT_DIR / \"node_features_long.parquet\"\n",
        "model_path = OUT_DIR / \"node_features_model.parquet\"\n",
        "feat_long.to_parquet(full_path, index=False)\n",
        "feat_long[[\"Week\",\"Ticker\"] + X_cols].to_parquet(model_path, index=False)\n",
        "with open(OUT_DIR / \"X_cols.json\",\"w\") as f:\n",
        "    json.dump(X_cols, f, indent=2)\n",
        "\n",
        "# -------------------- 7) Print summary --------------------\n",
        "wk_min = pd.to_datetime(feat_long[\"Week\"]).min().date() if len(feat_long) else None\n",
        "wk_max = pd.to_datetime(feat_long[\"Week\"]).max().date() if len(feat_long) else None\n",
        "print(\"\\n=== FEATURE PIPELINE SUMMARY ===\")\n",
        "print(\"Weeks:\", wk_min, \"→\", wk_max,\n",
        "      \"| unique weeks:\", feat_long[\"Week\"].nunique(),\n",
        "      \"| tickers:\", feat_long[\"Ticker\"].nunique())\n",
        "print(\"X_cols (#):\", len(X_cols))\n",
        "print(\"Saved:\")\n",
        "print(\" -\", full_path)\n",
        "print(\" -\", model_path)\n",
        "print(\" -\", OUT_DIR / \"X_cols.json\")\n"
      ],
      "metadata": {
        "id": "HqAL4kTgQ-cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Label Assignment\n",
        "\n",
        "We construct semi-supervised labels by fitting a rolling VAR(l) on weekly returns over a lookback window and computing FEVD at horizon H. For each in-window ticker, its “from-others” share is the FEVD row sum excluding its own diagonal term. We then shift by +1 week to form targets\n",
        "$\n",
        "y_{i, t+1}\n",
        "$\n",
        "\t​\n",
        "used in prediction (forecasting next-week spillover while FEVD itself is computed at horizon H=12 weeks).\n",
        "\n",
        "Safeguards:\n",
        "\n",
        "- strict coverage filter within each window,\n",
        "\n",
        "- adaptive pruning (max N, collinearity drop, feasibility bound),\n",
        "\n",
        "- deterministic “jitter” to avoid singular residual covariance in edge cases."
      ],
      "metadata": {
        "id": "Cnetg0liRrfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================ Section 3: Label assignment (VAR–FEVD) ============================\n",
        "from __future__ import annotations\n",
        "from pathlib import Path\n",
        "import logging, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from statsmodels.tsa.api import VAR\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ---- config  ----\n",
        "CFG_LBL = {\n",
        "    \"out_dir\": \"data\",\n",
        "    \"win\": 40,             # rolling window length (weeks)\n",
        "    \"var_lags\": 1,\n",
        "    \"fevd_h\": 12,          # FEVD horizon H (in weeks)\n",
        "    \"coverage\": 0.60,      # per-column non-null coverage threshold inside the window\n",
        "    \"min_t\": 30,           # minimum time points after dropping NaNs\n",
        "    \"max_n\": 25,           # max variables in the VAR after pruning\n",
        "    \"collinear\": 0.90,     # drop one of any pair with |rho| >= this threshold\n",
        "    \"jitter\": 1e-8,        # small deterministic noise to improve stability\n",
        "    \"deterministic_jitter\": True,  # make jitter deterministic per window\n",
        "}\n",
        "\n",
        "def _det_jitter(shape: tuple[int, int], seed_key: int, scale: float) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Deterministic tiny noise to avoid singular covariance. Depends on window end-week.\n",
        "    \"\"\"\n",
        "    if scale <= 0:\n",
        "        return np.zeros(shape)\n",
        "    rng = np.random.default_rng(seed_key)\n",
        "    return rng.normal(loc=0.0, scale=scale, size=shape)\n",
        "\n",
        "def _prune_window(\n",
        "    W: pd.DataFrame,\n",
        "    lags: int,\n",
        "    max_n: int,\n",
        "    collinear: float,\n",
        "    jitter: float,\n",
        "    det_seed: int | None = None\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Clean/prune a windowed return matrix for stable VAR–FEVD:\n",
        "      1) drop zero-variance cols\n",
        "      2) cap dimensionality to top-std columns\n",
        "      3) iteratively drop near-collinear columns (|rho| >= collinear)\n",
        "      4) demean\n",
        "      5) ensure feasibility: T >= (lags+1)*N + 5  (heuristic)\n",
        "      6) add tiny deterministic jitter (optional)\n",
        "    \"\"\"\n",
        "    # 1) drop zero-variance\n",
        "    std = W.std()\n",
        "    W = W.loc[:, std[std > 0].index]\n",
        "    if W.shape[1] < 2:\n",
        "        return W\n",
        "\n",
        "    # 2) cap N\n",
        "    if W.shape[1] > max_n:\n",
        "        top = std.sort_values(ascending=False).index[:max_n]\n",
        "        W = W.loc[:, top]\n",
        "\n",
        "    # 3) drop near-collinear\n",
        "    while W.shape[1] > 1:\n",
        "        C = W.corr().abs().fillna(0.0)\n",
        "        np.fill_diagonal(C.values, 0.0)\n",
        "        i, j = np.unravel_index(np.argmax(C.values), C.shape)\n",
        "        if C.values[i, j] < collinear:\n",
        "            break\n",
        "        cols = W.columns.tolist()\n",
        "        # drop the lower-std of the pair\n",
        "        drop_col = cols[i] if W[cols[i]].std() < W[cols[j]].std() else cols[j]\n",
        "        W = W.drop(columns=[drop_col])\n",
        "\n",
        "    # 4) demean\n",
        "    W = W - W.mean()\n",
        "\n",
        "    # 5) feasibility cap (T too small for given N)\n",
        "    T, N = W.shape\n",
        "    if T < (lags + 1) * N + 5:\n",
        "        keep = W.std().sort_values(ascending=False).index\n",
        "        maxN = max(2, (T - 5) // (lags + 1))\n",
        "        W = W.loc[:, keep[:maxN]]\n",
        "        T, N = W.shape\n",
        "\n",
        "    # 6) deterministic jitter\n",
        "    if jitter > 0.0:\n",
        "        seed = (det_seed if det_seed is not None else 0)\n",
        "        W = W + _det_jitter(W.shape, seed_key=seed, scale=jitter)\n",
        "\n",
        "    return W\n",
        "\n",
        "def _fit_var_fevd_stable(\n",
        "    W: pd.DataFrame,\n",
        "    fevd_h: int,\n",
        "    lags: int\n",
        ") -> tuple[np.ndarray, list[str]]:\n",
        "    \"\"\"\n",
        "    Fit VAR(lags) and return (FEVD matrix at horizon H, variable names).\n",
        "    Row-normalized so each row sums to 1.\n",
        "    \"\"\"\n",
        "    res = VAR(W).fit(maxlags=lags, ic=None, trend='c')\n",
        "    names = list(res.names) if hasattr(res, \"names\") else list(res.model.endog_names)\n",
        "    # Statsmodels FEVD: .decomp has shape [h, k, k]; select horizon fevd_h and take last (python index h-1)\n",
        "    fevd = res.fevd(fevd_h)\n",
        "    M = np.asarray(fevd.decomp[fevd_h - 1])  # ensure “at H”, not “up to H”\n",
        "    k = min(len(names), M.shape[0], M.shape[1])\n",
        "    names, M = names[:k], M[:k, :k]\n",
        "\n",
        "    row_sums = M.sum(axis=1, keepdims=True)\n",
        "    row_sums[row_sums == 0] = 1.0\n",
        "    M = M / row_sums\n",
        "    return M, names\n",
        "\n",
        "def make_labels_from_others_hardened(\n",
        "    wk_returns: pd.DataFrame,\n",
        "    win: int,\n",
        "    fevd_h: int,\n",
        "    lags: int,\n",
        "    coverage: float,\n",
        "    min_t: int,\n",
        "    max_n: int,\n",
        "    collinear: float,\n",
        "    jitter: float,\n",
        "    deterministic_jitter: bool,\n",
        "    log: logging.Logger | None = None\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Rolling FEVD labels:\n",
        "      - Build window [t-win : t) of weekly returns.\n",
        "      - Keep columns with coverage >= coverage; drop rows with any NaNs.\n",
        "      - Prune window for stability.\n",
        "      - Compute FEVD at horizon H; “from_others” = row sum minus diagonal.\n",
        "      - Shift by +1 week to form y_next (semi-supervised labels).\n",
        "    Returns long DataFrame [Week, Ticker, from_others, y_next].\n",
        "    \"\"\"\n",
        "    weeks = wk_returns.index.tolist()\n",
        "    stats = {\"too_few_cols\":0, \"too_few_rows\":0, \"feasibility_fail\":0, \"var_fail\":0, \"ok\":0}\n",
        "    rows, mismatches = [], 0\n",
        "\n",
        "    rng_seed_base = 17_003  # base for deterministic jitter seeds\n",
        "\n",
        "    for t in tqdm(range(win, len(weeks)), desc=\"FEVD (from-others, hardened)\"):\n",
        "        W_end = weeks[t]\n",
        "        window = wk_returns.iloc[t-win:t]\n",
        "\n",
        "        # per-column coverage within the window\n",
        "        cov = window.notna().mean()\n",
        "        keep_cols = cov[cov >= coverage].index.tolist()\n",
        "        if len(keep_cols) < 3:\n",
        "            stats[\"too_few_cols\"] += 1\n",
        "            continue\n",
        "\n",
        "        W = window[keep_cols].dropna(how=\"any\")\n",
        "        if W.shape[0] < min_t:\n",
        "            stats[\"too_few_rows\"] += 1\n",
        "            continue\n",
        "\n",
        "        det_seed = (hash(str(W_end)) ^ rng_seed_base) & 0xFFFFFFFF if deterministic_jitter else None\n",
        "        Wp = _prune_window(W, lags=lags, max_n=max_n, collinear=collinear, jitter=jitter, det_seed=det_seed)\n",
        "\n",
        "        T, N = Wp.shape\n",
        "        if N < 2 or T < (lags + 1) * N + 5:\n",
        "            stats[\"feasibility_fail\"] += 1\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            M, names = _fit_var_fevd_stable(Wp, fevd_h=fevd_h, lags=lags)\n",
        "            FROM = M.sum(axis=1) - np.diag(M)  # off-diagonal row sums\n",
        "            k = min(len(names), len(FROM))\n",
        "            if k < 2:\n",
        "                stats[\"var_fail\"] += 1\n",
        "                continue\n",
        "            if len(names) != len(FROM):\n",
        "                mismatches += 1\n",
        "            rows.append(pd.DataFrame({\"Week\": W_end, \"Ticker\": names[:k], \"from_others\": FROM[:k]}))\n",
        "            stats[\"ok\"] += 1\n",
        "        except Exception:\n",
        "            stats[\"var_fail\"] += 1\n",
        "            continue\n",
        "\n",
        "    labels = (pd.concat(rows, ignore_index=True)\n",
        "              if rows else pd.DataFrame(columns=[\"Week\",\"Ticker\",\"from_others\"]))\n",
        "    labels = labels.sort_values([\"Ticker\",\"Week\"])\n",
        "    # Shift by +1 week to create the prediction target\n",
        "    labels[\"y_next\"] = labels.groupby(\"Ticker\")[\"from_others\"].shift(-1)\n",
        "    labels = labels.dropna(subset=[\"y_next\"]).reset_index(drop=True)\n",
        "\n",
        "    # reporting\n",
        "    if log:\n",
        "        log.info(f\"FEVD summary: {stats} | mismatches trimmed: {mismatches}\")\n",
        "        if not labels.empty:\n",
        "            log.info(f\"Label span: {labels['Week'].min()} → {labels['Week'].max()} \"\n",
        "                     f\"| rows: {len(labels)} | tickers: {labels['Ticker'].nunique()}\")\n",
        "    else:\n",
        "        print(\"---- FEVD summary ----\")\n",
        "        print(stats, \"| mismatches trimmed:\", mismatches)\n",
        "        if not labels.empty:\n",
        "            print(\"Label span:\", labels['Week'].min(), \"→\", labels['Week'].max(),\n",
        "                  \"| rows:\", len(labels), \"| tickers:\", labels['Ticker'].nunique())\n",
        "    return labels\n",
        "\n",
        "# -------------------- runner --------------------\n",
        "if __name__ == \"__main__\":\n",
        "    from utils import get_logger, seed_everything, save_run_metadata  # from earlier\n",
        "    seed_everything(42)\n",
        "    log = get_logger(\"labels\")\n",
        "\n",
        "    out_dir = Path(CFG_LBL[\"out_dir\"]); out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Load weekly returns created in Section 1/2\n",
        "    ret_path = Path(\"data/feat/feat_ret.parquet\")\n",
        "    assert ret_path.exists(), \"Missing data/feat/feat_ret.parquet. Run preprocessing first.\"\n",
        "    wk_returns = pd.read_parquet(ret_path).sort_index()\n",
        "\n",
        "    # Compute labels\n",
        "    labels = make_labels_from_others_hardened(\n",
        "        wk_returns=wk_returns,\n",
        "        win=CFG_LBL[\"win\"],\n",
        "        fevd_h=CFG_LBL[\"fevd_h\"],\n",
        "        lags=CFG_LBL[\"var_lags\"],\n",
        "        coverage=CFG_LBL[\"coverage\"],\n",
        "        min_t=CFG_LBL[\"min_t\"],\n",
        "        max_n=CFG_LBL[\"max_n\"],\n",
        "        collinear=CFG_LBL[\"collinear\"],\n",
        "        jitter=CFG_LBL[\"jitter\"],\n",
        "        deterministic_jitter=CFG_LBL[\"deterministic_jitter\"],\n",
        "        log=log\n",
        "    )\n",
        "\n",
        "    # Save\n",
        "    labels_path = out_dir / \"labels_from_next.parquet\"\n",
        "    labels.to_parquet(labels_path, index=False)\n",
        "    log.info(f\"Saved labels → {labels_path}\")\n",
        "\n",
        "    # Quick check: coverage by year\n",
        "    if not labels.empty:\n",
        "        labels[\"Year\"] = pd.to_datetime(labels[\"Week\"]).dt.year\n",
        "        log.info(\"\\nLabels per year:\\n\" + str(labels.groupby(\"Year\").size()))\n"
      ],
      "metadata": {
        "id": "s0LdKvPnRx-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Graph construction (semi-supervised weekly graphs)\n",
        "\n",
        "For each target week *t*, we build a node-attributed graph:\n",
        "\n",
        "- **Nodes:** tickers with feature vectors $X_{i,t}$  \n",
        "  (standardized features from Section 2).\n",
        "\n",
        "- **Edges:** thresholded correlations $|\\rho_{ij}| \\geq \\theta$  \n",
        "  computed over a rolling history $[t-W,\\,t)$ using weekly returns  \n",
        "  (z-scored if available, else raw).\n",
        "\n",
        "- **Labels & mask:** targets $y_{i,t}$ (next-week “from-others” labels)  \n",
        "  are present only for a subset of nodes; we store a boolean **train mask** per graph  \n",
        "  for semi-supervised training.\n",
        "\n",
        "We save one **Data** object per week as `graph_YYYY-MM-DD.pt` plus an index (`graphs_index.csv`) with counts.  \n",
        "Finally, we create time-ordered splits (70/15/15) and an optional quick visual of a chosen week.\n"
      ],
      "metadata": {
        "id": "gk2F4lmzS03d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================ Section 4: Graph Construction ============================\n",
        "from pathlib import Path\n",
        "import json, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from itertools import combinations\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "# ---- config  ----\n",
        "CFG_G = {\n",
        "    \"data_dir\": \"data\",\n",
        "    \"graphs_dir\": \"graphs\",\n",
        "    \"corr_win\": 52,          # history window (weeks) for edges\n",
        "    \"edge_thr\": 0.20,        # |corr| threshold\n",
        "    \"min_edges\": 2,          # require at least this many undirected edges\n",
        "    \"min_labels\": 1,         # require >= this many labeled nodes in week\n",
        "    \"ret_col_pref\": [\"feat_ret_z\", \"feat_ret\"],  # try z-scored first\n",
        "}\n",
        "\n",
        "DATA_DIR      = Path(CFG_G[\"data_dir\"])\n",
        "OUT_GRAPH_DIR = Path(CFG_G[\"graphs_dir\"])\n",
        "OUT_GRAPH_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "FEATURES_PATH   = DATA_DIR / \"node_features_model.parquet\"   # Week, Ticker, *_z / *_global_z only\n",
        "XCOLS_PATH      = DATA_DIR / \"X_cols.json\"\n",
        "LABELS_PATH     = DATA_DIR / \"labels_from_next.parquet\"\n",
        "GRAPH_INDEX_CSV = OUT_GRAPH_DIR / \"graphs_index.csv\"\n",
        "SPLIT_JSON      = OUT_GRAPH_DIR / \"split_weeks.json\"\n",
        "\n",
        "# -------------------- load artifacts --------------------\n",
        "feat = pd.read_parquet(FEATURES_PATH)      # (Week, Ticker, X_cols)\n",
        "labels = pd.read_parquet(LABELS_PATH)      # (Week, Ticker, y_next)\n",
        "with open(XCOLS_PATH, \"r\") as f:\n",
        "    X_cols = json.load(f)\n",
        "\n",
        "assert {\"Week\",\"Ticker\"}.issubset(feat.columns),   \"Features must have Week, Ticker\"\n",
        "assert {\"Week\",\"Ticker\",\"y_next\"}.issubset(labels.columns), \"Labels must have Week, Ticker, y_next\"\n",
        "\n",
        "# choose return column for correlations\n",
        "RET_COL = None\n",
        "for c in CFG_G[\"ret_col_pref\"]:\n",
        "    if c in feat.columns:\n",
        "        RET_COL = c; break\n",
        "if RET_COL is None:\n",
        "    raise ValueError(\"Neither feat_ret_z nor feat_ret present for correlation edges.\")\n",
        "\n",
        "# Keep only modeling columns + Week, Ticker (but we’ll still access RET_COL for edges if it’s not in X_cols)\n",
        "use_cols = [\"Week\",\"Ticker\"] + sorted(set(X_cols) | {RET_COL})\n",
        "feat = feat[use_cols].copy()\n",
        "\n",
        "# normalize time\n",
        "feat[\"Week\"]   = pd.to_datetime(feat[\"Week\"])\n",
        "labels[\"Week\"] = pd.to_datetime(labels[\"Week\"])\n",
        "\n",
        "# weeks with features AND labels (we want at least some labels per week)\n",
        "weeks_all   = sorted(feat[\"Week\"].unique())\n",
        "weeks_label = set(labels[\"Week\"].unique())\n",
        "weeks = [w for w in weeks_all if w in weeks_label]\n",
        "\n",
        "# -------------------- helpers --------------------\n",
        "def build_corr_matrix(df_feat_hist: pd.DataFrame, tickers: list[str], ret_col: str) -> np.ndarray | None:\n",
        "    \"\"\"\n",
        "    Compute correlation over the history window for given tickers using ret_col.\n",
        "    Returns an NxN numpy array (diag=0), or None if history too short.\n",
        "    \"\"\"\n",
        "    df_ret_hist = df_feat_hist[[\"Week\",\"Ticker\", ret_col]].copy()\n",
        "    wide = df_ret_hist.pivot(index=\"Week\", columns=\"Ticker\", values=ret_col)\n",
        "    wide = wide.reindex(columns=tickers)\n",
        "    wide = wide.dropna(how=\"all\", axis=0)\n",
        "    if wide.shape[0] < 3:   # too short\n",
        "        return None\n",
        "    corr = wide.corr().to_numpy()\n",
        "    corr = np.nan_to_num(corr, nan=0.0)\n",
        "    np.fill_diagonal(corr, 0.0)\n",
        "    return corr\n",
        "\n",
        "graph_records = []\n",
        "\n",
        "# -------------------- build graphs --------------------\n",
        "for t in range(CFG_G[\"corr_win\"], len(weeks)):\n",
        "    week = weeks[t]\n",
        "    hist_weeks = weeks[t-CFG_G[\"corr_win\"]:t]\n",
        "\n",
        "    # features for week t\n",
        "    df_feat_w = feat.loc[feat[\"Week\"] == week].copy()\n",
        "    if df_feat_w.empty:\n",
        "        continue\n",
        "\n",
        "    # deterministic ticker order\n",
        "    tickers = sorted(df_feat_w[\"Ticker\"].unique().tolist())\n",
        "    df_feat_w = df_feat_w.set_index(\"Ticker\").reindex(tickers).reset_index()\n",
        "\n",
        "    # assemble X\n",
        "    # Ensure all X_cols exist (if some globals weren’t present, they were dropped earlier)\n",
        "    missing_x = [c for c in X_cols if c not in df_feat_w.columns]\n",
        "    if missing_x:\n",
        "        # if any missing, create zeros (safe because *_z / *_global_z already standardized)\n",
        "        for c in missing_x:\n",
        "            df_feat_w[c] = 0.0\n",
        "\n",
        "    X = df_feat_w[X_cols].to_numpy(dtype=np.float32)\n",
        "    X = np.nan_to_num(X, nan=0.0)\n",
        "\n",
        "    # labels + mask\n",
        "    df_lab_w = labels.loc[labels[\"Week\"] == week, [\"Ticker\",\"y_next\"]].set_index(\"Ticker\")\n",
        "    y_series = df_lab_w.reindex(tickers)[\"y_next\"]\n",
        "    mask = ~y_series.isna().to_numpy()\n",
        "    if mask.sum() < CFG_G[\"min_labels\"]:\n",
        "        continue\n",
        "    y_vec = y_series.fillna(0.0).to_numpy(dtype=np.float32)\n",
        "\n",
        "    # correlation edges from history\n",
        "    df_hist = feat.loc[feat[\"Week\"].isin(hist_weeks)].copy()\n",
        "    if df_hist.empty:\n",
        "        continue\n",
        "    corr = build_corr_matrix(df_hist, tickers, ret_col=RET_COL)\n",
        "    if corr is None:\n",
        "        continue\n",
        "\n",
        "    # threshold to edge list (undirected, store both directions)\n",
        "    N = len(tickers)\n",
        "    edge_idx, edge_wt = [], []\n",
        "    thr = CFG_G[\"edge_thr\"]\n",
        "    for i, j in combinations(range(N), 2):\n",
        "        cij = float(corr[i, j])\n",
        "        if abs(cij) >= thr:\n",
        "            edge_idx.append([i, j]);  edge_wt.append(cij)\n",
        "            edge_idx.append([j, i]);  edge_wt.append(cij)\n",
        "\n",
        "    if (len(edge_idx) // 2) < CFG_G[\"min_edges\"]:\n",
        "        continue\n",
        "\n",
        "    edge_index = torch.tensor(edge_idx, dtype=torch.long).t().contiguous()\n",
        "    edge_weight = torch.tensor(edge_wt, dtype=torch.float32)\n",
        "    x = torch.tensor(X, dtype=torch.float32)\n",
        "    y = torch.tensor(y_vec, dtype=torch.float32)\n",
        "    train_mask = torch.tensor(mask, dtype=torch.bool)\n",
        "\n",
        "    g = Data(x=x, edge_index=edge_index, edge_weight=edge_weight, y=y)\n",
        "    g.train_mask = train_mask\n",
        "    g.tickers = tickers           # list[str]\n",
        "    g.week = str(pd.to_datetime(week).date())  # \"YYYY-MM-DD\"\n",
        "\n",
        "    fname = f\"graph_{g.week}.pt\"\n",
        "    fpath = OUT_GRAPH_DIR / fname\n",
        "    torch.save(g, fpath)\n",
        "\n",
        "    graph_records.append({\n",
        "        \"Week\": pd.to_datetime(week),\n",
        "        \"file\": fname,\n",
        "        \"num_nodes\": N,\n",
        "        \"num_edges\": int(edge_index.shape[1]),\n",
        "        \"num_labeled\": int(train_mask.sum())\n",
        "    })\n",
        "\n",
        "# -------------------- index & splits --------------------\n",
        "idx = pd.DataFrame(graph_records).sort_values(\"Week\").reset_index(drop=True)\n",
        "idx.to_csv(GRAPH_INDEX_CSV, index=False)\n",
        "\n",
        "print(f\"Saved {len(idx)} graphs to {OUT_GRAPH_DIR}\")\n",
        "if len(idx):\n",
        "    print(idx.tail(5))\n",
        "else:\n",
        "    print(\"No graphs built — check corr_win, edge_thr, or label availability.\")\n",
        "\n",
        "# time-ordered 70/15/15 split\n",
        "if len(idx) > 0:\n",
        "    W = len(idx)\n",
        "    i_tr_end = math.floor(0.70 * W)         # exclusive\n",
        "    i_va_end = i_tr_end + math.floor(0.15 * W)\n",
        "\n",
        "    train_weeks = idx.loc[:i_tr_end-1, \"Week\"].dt.date.astype(str).tolist()\n",
        "    val_weeks   = idx.loc[i_tr_end:i_va_end-1, \"Week\"].dt.date.astype(str).tolist()\n",
        "    test_weeks  = idx.loc[i_va_end:, \"Week\"].dt.date.astype(str).tolist()\n",
        "\n",
        "    with open(SPLIT_JSON, \"w\") as f:\n",
        "        json.dump({\"train_weeks\": train_weeks, \"val_weeks\": val_weeks, \"test_weeks\": test_weeks}, f, indent=2)\n",
        "\n",
        "    print(\"Saved:\", SPLIT_JSON)\n",
        "    print(f\"Counts → train {len(train_weeks)}, val {len(val_weeks)}, test {len(test_weeks)}\")\n",
        "\n",
        "# -------------------- Visual --------------------\n",
        "def _normalize_weeks(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Add normalized week representations for querying (datetime, date str, raw str).\"\"\"\n",
        "    d = df.copy()\n",
        "    d[\"_week_dt\"] = pd.to_datetime(d[\"Week\"], errors=\"coerce\")\n",
        "    d[\"_week_date\"] = d[\"_week_dt\"].dt.date.astype(str)\n",
        "    d[\"_week_str\"] = d[\"Week\"].astype(str)\n",
        "    return d\n",
        "\n",
        "def plot_graph_pretty(week, graph_index: pd.DataFrame, base_dir: Path, topk_per_node: int | None = None,\n",
        "                      node_color=\"skyblue\", seed: int = 42):\n",
        "    gi = _normalize_weeks(graph_index.copy())\n",
        "    # normalize query\n",
        "    w_dt = pd.to_datetime(week, errors=\"coerce\")\n",
        "    w_date = (w_dt.date().isoformat() if pd.notna(w_dt) else str(week))\n",
        "    row = gi.loc[gi[\"_week_date\"] == w_date]\n",
        "    if row.empty:\n",
        "        row = gi.loc[gi[\"_week_str\"] == str(week)]\n",
        "    if row.empty and pd.notna(w_dt):\n",
        "        row = gi.loc[gi[\"_week_dt\"] == w_dt]\n",
        "    if row.empty:\n",
        "        print(f\"Week {week} not found. Try: {gi['_week_date'].head().tolist()} …\")\n",
        "        return None\n",
        "\n",
        "    fpath = base_dir / row.iloc[0][\"file\"]\n",
        "    g = torch.load(fpath, weights_only=False, map_location=\"cpu\")\n",
        "\n",
        "    edges_np = g.edge_index.cpu().numpy().T\n",
        "    w_np     = g.edge_weight.cpu().numpy()\n",
        "    tickers  = list(g.tickers)\n",
        "\n",
        "    G = nx.DiGraph()\n",
        "    for i, tk in enumerate(tickers):\n",
        "        G.add_node(i, ticker=tk, label=float(g.y[i].item()))\n",
        "\n",
        "    # optional top-k pruning\n",
        "    edges_iter = list(zip(edges_np, w_np))\n",
        "    if topk_per_node:\n",
        "        out_dict = {}\n",
        "        for (src, dst), w in edges_iter:\n",
        "            out_dict.setdefault(int(src), []).append((int(dst), float(w)))\n",
        "        edges_iter = []\n",
        "        for src, lst in out_dict.items():\n",
        "            lst_sorted = sorted(lst, key=lambda x: abs(x[1]), reverse=True)[:topk_per_node]\n",
        "            edges_iter.extend([((src, dst), w) for dst, w in lst_sorted])\n",
        "\n",
        "    for (s, d), w in edges_iter:\n",
        "        G.add_edge(int(s), int(d), weight=float(w))\n",
        "\n",
        "    labels_dict = nx.get_node_attributes(G, \"label\")\n",
        "    max_abs = max(1e-9, max(abs(v) for v in labels_dict.values()))\n",
        "    node_sizes = [300 + 2000 * (abs(labels_dict[i]) / max_abs) for i in G.nodes]\n",
        "\n",
        "    eweights = np.array([G[u][v][\"weight\"] for u, v in G.edges()], dtype=float)\n",
        "    if len(eweights) == 0:\n",
        "        print(\"Graph has no edges to plot.\")\n",
        "        return g\n",
        "    norm_vals = (eweights + 1.0) / 2.0\n",
        "    edge_colors = plt.cm.bwr(norm_vals)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    pos = nx.spring_layout(G, seed=seed, k=1.7, iterations=100)\n",
        "\n",
        "    nx.draw_networkx_nodes(G, pos, ax=ax, node_size=node_sizes, node_color=node_color,\n",
        "                           alpha=0.9, edgecolors=\"black\", linewidths=0.8)\n",
        "    nx.draw_networkx_edges(G, pos, ax=ax, edge_color=edge_colors, alpha=0.6)\n",
        "    nx.draw_networkx_labels(G, pos, ax=ax, labels={i: G.nodes[i][\"ticker\"] for i in G.nodes}, font_size=8)\n",
        "\n",
        "    sm = plt.cm.ScalarMappable(cmap=\"bwr\", norm=plt.Normalize(vmin=-1, vmax=1))\n",
        "    sm.set_array([])\n",
        "    fig.colorbar(sm, ax=ax, label=\"Correlation\")\n",
        "\n",
        "    ax.set_title(f\"Graph for Week {row.iloc[0]['_week_date']}\\n(Node size ∝ |label|, Edge color ∝ corr)\")\n",
        "    ax.axis(\"off\")\n",
        "    plt.show()\n",
        "    return g\n",
        "\n",
        "# Quick visual\n",
        "gi = pd.read_csv(GRAPH_INDEX_CSV)\n",
        "g = plot_graph_pretty(\"2021-01-08\", gi, base_dir=OUT_GRAPH_DIR, topk_per_node=5)\n",
        "if g is not None:\n",
        "  print(\"x:\", g.x.shape, \"| y:\", g.y.shape, \"| edges:\", g.edge_index.shape)\n"
      ],
      "metadata": {
        "id": "Sh5ud3oNS-Sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Baseline models (econometric benchmarks)\n",
        "\n",
        "We benchmark the GNN against two classical baselines, using the same time splits produced from the graph index (70/15/15, time-ordered):\n",
        "\n",
        "- **VAR–FEVD:** Rolling VAR($l$) on weekly returns with pruning and coverage guards, producing the “from-others” share at horizon $H$.  \n",
        "  We evaluate next-week predictions $y_{t+1}$.\n",
        "\n",
        "- **Scalar BEKK(1,1):** Python implementation (grid-searched $a, b$) on a capped subset of tickers, rolling the conditional covariance $H_t$  \n",
        "  forward and mapping to a spillover share.\n",
        "\n",
        "We report test-set RMSE/MAE/$R^2$, panel-weekly RMSE curves, per-ticker RMSE distributions, and Diebold–Mariano tests (overall and per ticker).  \n",
        "These results are referenced in the thesis baseline section.\n"
      ],
      "metadata": {
        "id": "WUQMt3pmT_do"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================ Section 5: Baseline Models ============================\n",
        "from __future__ import annotations\n",
        "from pathlib import Path\n",
        "import os, json, warnings, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Tuple\n",
        "from statsmodels.tsa.api import VAR\n",
        "\n",
        "# ---------- Paths  ----------\n",
        "BASE_DIR   = Path(\".\")\n",
        "FEAT_DIR   = BASE_DIR / \"data\" / \"feat\"          # feat_ret.parquet\n",
        "DATA_DIR   = BASE_DIR / \"data\"                   # labels_from_next.parquet\n",
        "GRAPH_DIR  = BASE_DIR / \"graphs\"                 # split_weeks.json\n",
        "OUT_DIR    = BASE_DIR / \"baselines\"              # outputs here\n",
        "FIG_DIR    = BASE_DIR / \"figures\"\n",
        "TAB_DIR    = BASE_DIR / \"tables\"\n",
        "for p in [OUT_DIR, FIG_DIR, TAB_DIR]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "RET_FILE   = FEAT_DIR / \"feat_ret.parquet\"\n",
        "LABEL_FILE = DATA_DIR / \"labels_from_next.parquet\"\n",
        "SPLIT_JSON = GRAPH_DIR / \"split_weeks.json\"\n",
        "\n",
        "# ---------- Config ----------\n",
        "ROLL_WINDOW_WEEKS = 40\n",
        "VAR_LAGS          = 1\n",
        "FEVD_H            = 12\n",
        "COVERAGE          = 0.70\n",
        "MIN_T             = 30\n",
        "MAX_N             = 25\n",
        "COLLINEAR         = 0.90\n",
        "JITTER            = 1e-8\n",
        "\n",
        "# Scalar BEKK (Python)\n",
        "BEKK_MAX_N        = 10\n",
        "BEKK_MIN_T        = 25\n",
        "COVERAGE_BEKK     = 0.95\n",
        "DROP_COLLINEAR_BK = 0.995\n",
        "RIDGE_H           = 1e-8\n",
        "JITTER_BEKK       = 0.0\n",
        "\n",
        "# Rolling refits on test (uses info up to t to predict t+1)\n",
        "ALLOW_UPDATE_ON_TEST = True\n",
        "\n",
        "# Fast smoke test toggle\n",
        "FAST_SMOKE_TEST = False\n",
        "if FAST_SMOKE_TEST:\n",
        "    BEKK_MAX_N = 6\n",
        "    BEKK_MIN_T = 20\n",
        "    TEST_WEEKS_LIMIT = 4\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# ---------- Helpers ----------\n",
        "def to_wfri_index(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    idx = pd.to_datetime(df.index)\n",
        "    df = df.copy()\n",
        "    df.index = idx.to_period(\"W-FRI\").end_time.dt.normalize()\n",
        "    return df.sort_index()\n",
        "\n",
        "def weeks_str_to_wfri(weeks: List[str]) -> pd.DatetimeIndex:\n",
        "    if not weeks:\n",
        "        return pd.DatetimeIndex([])\n",
        "    w = pd.to_datetime(pd.Series(weeks))\n",
        "    w = w.dt.to_period(\"W-FRI\").dt.end_time.dt.normalize()\n",
        "    return pd.DatetimeIndex(sorted(w.unique()))\n",
        "\n",
        "def _prune_window(W: pd.DataFrame,\n",
        "                  lags: int = VAR_LAGS,\n",
        "                  max_n: int = MAX_N,\n",
        "                  collinear: float = COLLINEAR) -> pd.DataFrame:\n",
        "    \"\"\"Replicates pruning used in label construction.\"\"\"\n",
        "    std = W.std()\n",
        "    keep_idx = std[std > 0].index\n",
        "    W = W.loc[:, keep_idx]\n",
        "    if W.shape[1] < 2:\n",
        "        return W\n",
        "    if W.shape[1] > max_n:\n",
        "        top = std.loc[keep_idx].sort_values(ascending=False).index[:max_n]\n",
        "        W = W.loc[:, top]\n",
        "    while True and W.shape[1] > 1:\n",
        "        C = W.corr().abs()\n",
        "        np.fill_diagonal(C.values, 0.0)\n",
        "        i, j = np.unravel_index(np.nanargmax(C.values), C.shape)\n",
        "        if C.values[i, j] < collinear:\n",
        "            break\n",
        "        cols = W.columns.tolist()\n",
        "        drop_col = cols[i] if W[cols[i]].std() < W[cols[j]].std() else cols[j]\n",
        "        W = W.drop(columns=[drop_col])\n",
        "    W = W - W.mean()\n",
        "    T, N = W.shape\n",
        "    if T < (lags + 1) * N + 5:\n",
        "        keep = W.std().sort_values(ascending=False).index\n",
        "        maxN = max(2, (T - 5) // (lags + 1))\n",
        "        W = W[keep[:maxN]]\n",
        "    if JITTER > 0:\n",
        "        W = W + np.random.normal(scale=JITTER, size=W.shape)\n",
        "    return W\n",
        "\n",
        "def fit_var_fevd_matrix(W: pd.DataFrame, fevd_h: int = FEVD_H, lags: int = VAR_LAGS):\n",
        "    \"\"\"Return (row-normalized FEVD at horizon H, variable names).\"\"\"\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\")\n",
        "        res = VAR(W).fit(maxlags=lags, ic=None, trend='c')\n",
        "        names = list(res.names) if hasattr(res, \"names\") else list(res.model.endog_names)\n",
        "        fevd = res.fevd(fevd_h)\n",
        "        M = np.asarray(fevd.decomp[fevd_h - 1])  # 'at H'\n",
        "    k = min(len(names), M.shape[0], M.shape[1])\n",
        "    names, M = names[:k], M[:k, :k]\n",
        "    row_sums = M.sum(axis=1, keepdims=True)\n",
        "    row_sums[row_sums == 0] = 1.0\n",
        "    M = M / row_sums\n",
        "    return M, names\n",
        "\n",
        "def from_others_from_fevd(M: np.ndarray) -> np.ndarray:\n",
        "    return M.sum(axis=1) - np.diag(M)\n",
        "\n",
        "def from_others_from_cov(H: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Spillover share from covariance (BEKK path).\"\"\"\n",
        "    H2 = H ** 2\n",
        "    row_sum = H2.sum(axis=1)\n",
        "    own = np.diag(H2)\n",
        "    spill = np.maximum(row_sum - own, 0.0)\n",
        "    with np.errstate(invalid='ignore', divide='ignore'):\n",
        "        share = spill / (row_sum + 1e-12)\n",
        "    return share\n",
        "\n",
        "def rmse(a, b):\n",
        "    a, b = np.asarray(a, float), np.asarray(b, float)\n",
        "    return float(np.sqrt(np.nanmean((a - b) ** 2)))\n",
        "\n",
        "def mae(a, b):\n",
        "    a, b = np.asarray(a, float), np.asarray(b, float)\n",
        "    return float(np.nanmean(np.abs(a - b)))\n",
        "\n",
        "def r2(a, b):\n",
        "    a, b = np.asarray(a, float), np.asarray(b, float)\n",
        "    ybar = np.nanmean(a)\n",
        "    ss_res = np.nansum((a - b) ** 2)\n",
        "    ss_tot = np.nansum((a - ybar) ** 2)\n",
        "    return float(1 - ss_res / (ss_tot + 1e-12))\n",
        "\n",
        "from math import erf, sqrt\n",
        "def _ncdf(x: float) -> float:\n",
        "    return 0.5 * (1.0 + erf(x / sqrt(2.0)))\n",
        "\n",
        "def dm_test(loss_a, loss_b, max_lag: int | None = None):\n",
        "    \"\"\"Diebold–Mariano on two loss series, Newey–West long-run variance.\"\"\"\n",
        "    a = np.asarray(loss_a, dtype=\"float64\").reshape(-1)\n",
        "    b = np.asarray(loss_b, dtype=\"float64\").reshape(-1)\n",
        "    d = a - b\n",
        "    mask = np.isfinite(d)\n",
        "    d = d[mask]\n",
        "    T = d.size\n",
        "    if T < 10:\n",
        "        return np.nan, np.nan\n",
        "    d = d - d.mean()\n",
        "    if max_lag is None:\n",
        "        max_lag = max(0, int(T ** (1/3)))\n",
        "    gamma0 = float(np.dot(d, d) / T)\n",
        "    var = gamma0\n",
        "    for k in range(1, max_lag + 1):\n",
        "        w = 1.0 - k / (max_lag + 1.0)\n",
        "        cov = float(np.dot(d[:-k], d[k:]) / T)\n",
        "        var += 2.0 * w * cov\n",
        "    var_mean = var / T\n",
        "    if not np.isfinite(var_mean) or var_mean <= 0:\n",
        "        return np.nan, np.nan\n",
        "    dm = float(d.mean() / np.sqrt(var_mean))\n",
        "    p  = float(2 * (1 - _ncdf(abs(dm))))\n",
        "    return dm, p\n",
        "\n",
        "def make_spd(H: np.ndarray, eps: float = RIDGE_H) -> np.ndarray:\n",
        "    H = 0.5 * (H + H.T)\n",
        "    try:\n",
        "        wmin = float(np.nanmin(np.linalg.eigvalsh(H)))\n",
        "    except Exception:\n",
        "        wmin = float(\"nan\")\n",
        "    if (not np.isfinite(wmin)) or (wmin < eps):\n",
        "        add = eps if not np.isfinite(wmin) else (eps - wmin)\n",
        "        H = H + add * np.eye(H.shape[0])\n",
        "    return H\n",
        "\n",
        "# ---------- Load data ----------\n",
        "if not RET_FILE.exists():\n",
        "    raise FileNotFoundError(f\"Missing returns parquet: {RET_FILE}\")\n",
        "wk_returns = pd.read_parquet(RET_FILE)\n",
        "wk_returns = wk_returns.set_index(\"Week\") if \"Week\" in wk_returns.columns else wk_returns\n",
        "wk_returns = to_wfri_index(wk_returns)\n",
        "\n",
        "if not LABEL_FILE.exists():\n",
        "    raise FileNotFoundError(f\"Missing labels parquet: {LABEL_FILE}\")\n",
        "labels = pd.read_parquet(LABEL_FILE)\n",
        "labels[\"Week\"] = pd.to_datetime(labels[\"Week\"]).dt.to_period(\"W-FRI\").dt.end_time.dt.normalize()\n",
        "\n",
        "if not SPLIT_JSON.exists():\n",
        "    raise FileNotFoundError(f\"Missing split file: {SPLIT_JSON}\")\n",
        "with open(SPLIT_JSON, \"r\") as f:\n",
        "    split = json.load(f)\n",
        "\n",
        "train_weeks = weeks_str_to_wfri(split.get(\"train_weeks\", []))\n",
        "val_weeks   = weeks_str_to_wfri(split.get(\"val_weeks\", []))\n",
        "test_weeks  = weeks_str_to_wfri(split.get(\"test_weeks\", []))\n",
        "\n",
        "# intersect with returns index\n",
        "train_weeks = train_weeks.intersection(wk_returns.index)\n",
        "val_weeks   = val_weeks.intersection(wk_returns.index)\n",
        "test_weeks  = test_weeks.intersection(wk_returns.index)\n",
        "\n",
        "pretest_weeks = train_weeks.union(val_weeks)\n",
        "if len(pretest_weeks) < 10 or len(test_weeks) < 5:\n",
        "    raise ValueError(f\"Split too small: pretest={len(pretest_weeks)}, test={len(test_weeks)}\")\n",
        "\n",
        "if FAST_SMOKE_TEST:\n",
        "    test_weeks = pd.DatetimeIndex(sorted(test_weeks))[:TEST_WEEKS_LIMIT]\n",
        "\n",
        "print(f\"[SPLIT] train={len(train_weeks)}, val={len(val_weeks)}, test={len(test_weeks)}\")\n",
        "\n",
        "# ---------- VAR–FEVD forecasts ----------\n",
        "def var_fevd_next_forecasts_long(ret_df: pd.DataFrame,\n",
        "                                 pretest_idx: pd.DatetimeIndex,\n",
        "                                 test_idx: pd.DatetimeIndex,\n",
        "                                 allow_update: bool) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    last_pre = pretest_idx.max()\n",
        "    for t in test_idx:\n",
        "        window_end = t if allow_update else min(t, last_pre)\n",
        "        if window_end not in ret_df.index:\n",
        "            continue\n",
        "        end_loc = ret_df.index.get_loc(window_end)\n",
        "        start_loc = max(0, end_loc - ROLL_WINDOW_WEEKS + 1) if ROLL_WINDOW_WEEKS else 0\n",
        "        window = ret_df.iloc[start_loc:end_loc + 1]\n",
        "\n",
        "        cov = window.notna().mean()\n",
        "        keep_cols = cov[cov >= COVERAGE].index.tolist()\n",
        "        W = window[keep_cols].dropna(how=\"any\")\n",
        "        if len(keep_cols) < 3 or W.shape[0] < MIN_T:\n",
        "            continue\n",
        "\n",
        "        Wp = _prune_window(W, lags=VAR_LAGS, max_n=MAX_N, collinear=COLLINEAR)\n",
        "        T, N = Wp.shape\n",
        "        if N < 2 or T < (VAR_LAGS + 1) * N + 5:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            M, names = fit_var_fevd_matrix(Wp, fevd_h=FEVD_H, lags=VAR_LAGS)\n",
        "            FROM = from_others_from_fevd(M)\n",
        "            k = min(len(names), len(FROM))\n",
        "            if k >= 2:\n",
        "                rows.append(pd.DataFrame({\"Week\": t, \"Ticker\": names[:k], \"yhat_VAR\": FROM[:k]}))\n",
        "        except Exception:\n",
        "            continue\n",
        "    return pd.concat(rows, ignore_index=True) if rows else pd.DataFrame(columns=[\"Week\",\"Ticker\",\"yhat_VAR\"])\n",
        "\n",
        "print(\"[Run] VAR–FEVD forecasts…\")\n",
        "ret_fit = wk_returns.loc[pretest_weeks.union(test_weeks)]\n",
        "yhat_var_long = var_fevd_next_forecasts_long(ret_fit, pretest_weeks, test_weeks, allow_update=ALLOW_UPDATE_ON_TEST)\n",
        "\n",
        "# ---------- Scalar BEKK(1,1) ----------\n",
        "def s_bekk_fit_and_forecast_long(ret_df: pd.DataFrame,\n",
        "                                 pretest_idx: pd.DatetimeIndex,\n",
        "                                 test_idx: pd.DatetimeIndex) -> pd.DataFrame:\n",
        "    pre_end = pretest_idx.max()\n",
        "    if pre_end not in ret_df.index:\n",
        "        pos = ret_df.index.searchsorted(pre_end, side=\"right\") - 1\n",
        "        if pos < 0:\n",
        "            return pd.DataFrame(columns=[\"Week\",\"Ticker\",\"yhat_BEKK\"])\n",
        "        pre_end = ret_df.index[pos]\n",
        "\n",
        "    end_loc = ret_df.index.get_loc(pre_end)\n",
        "    start_loc = max(0, end_loc - ROLL_WINDOW_WEEKS + 1) if ROLL_WINDOW_WEEKS else 0\n",
        "    window = ret_df.iloc[start_loc:end_loc + 1]\n",
        "\n",
        "    cov = window.notna().mean()\n",
        "    keep_cov = cov[cov >= COVERAGE_BEKK].index.tolist()\n",
        "    if len(keep_cov) < 2:\n",
        "        keep_cov = cov[cov >= 0.90].index.tolist()\n",
        "\n",
        "    std_all = window[keep_cov].std().sort_values(ascending=False)\n",
        "    use_cols = std_all.index[:min(len(std_all), BEKK_MAX_N)]\n",
        "    sub = window[use_cols].dropna(how=\"any\")\n",
        "    if sub.shape[1] < 2 or sub.shape[0] < BEKK_MIN_T:\n",
        "        return pd.DataFrame(columns=[\"Week\",\"Ticker\",\"yhat_BEKK\"])\n",
        "\n",
        "    if sub.shape[1] >= 3:\n",
        "        Cabs = sub.corr().abs()\n",
        "        np.fill_diagonal(Cabs.values, 0.0)\n",
        "        while np.nanmax(Cabs.values) >= DROP_COLLINEAR_BK and sub.shape[1] >= 2:\n",
        "            i, j = np.unravel_index(np.nanargmax(Cabs.values), Cabs.shape)\n",
        "            cols = sub.columns.tolist()\n",
        "            drop_col = cols[i] if sub[cols[i]].std() < sub[cols[j]].std() else cols[j]\n",
        "            sub = sub.drop(columns=[drop_col])\n",
        "            if sub.shape[1] < 2:\n",
        "                return pd.DataFrame(columns=[\"Week\",\"Ticker\",\"yhat_BEKK\"])\n",
        "            Cabs = sub.corr().abs(); np.fill_diagonal(Cabs.values, 0.0)\n",
        "\n",
        "    if JITTER_BEKK > 0:\n",
        "        sub = sub.astype(\"float64\") + np.random.normal(scale=JITTER_BEKK, size=sub.shape)\n",
        "\n",
        "    X = sub.values.astype(\"float64\")\n",
        "    mu = X.mean(axis=0)\n",
        "    E  = X - mu\n",
        "    S  = np.cov(E.T)\n",
        "\n",
        "    a_grid = np.array([0.02, 0.04, 0.06, 0.08, 0.10, 0.12])\n",
        "    b_grid = np.array([0.80, 0.88, 0.92, 0.94, 0.96, 0.975])\n",
        "    pairs  = [(a,b) for a in a_grid for b in b_grid if (a*a + b*b) < 0.995]\n",
        "\n",
        "    def qml_negloglik(a, b, E, S):\n",
        "        H = S.copy()\n",
        "        const = (1.0 - a*a - b*b)\n",
        "        if const <= 1e-8:\n",
        "            return np.inf\n",
        "        constS = const * S\n",
        "        nll = 0.0\n",
        "        for t in range(E.shape[0]):\n",
        "            et = E[t].reshape(-1,1)\n",
        "            H = constS + (a*a) * (et @ et.T) + (b*b) * H\n",
        "            H = make_spd(H, RIDGE_H)\n",
        "            try:\n",
        "                L = np.linalg.cholesky(H)\n",
        "            except np.linalg.LinAlgError:\n",
        "                return np.inf\n",
        "            ll_det = 2.0 * np.sum(np.log(np.diag(L)))\n",
        "            y = np.linalg.solve(L, et)\n",
        "            quad = float((y.T @ y))\n",
        "            nll += (ll_det + quad)\n",
        "        return nll\n",
        "\n",
        "    best = None; best_val = np.inf\n",
        "    for (a,b) in pairs:\n",
        "        val = qml_negloglik(a, b, E, S)\n",
        "        if val < best_val:\n",
        "            best_val = val; best = (a,b)\n",
        "    if best is None:\n",
        "        return pd.DataFrame(columns=[\"Week\",\"Ticker\",\"yhat_BEKK\"])\n",
        "    a_hat, b_hat = best\n",
        "\n",
        "    constS = (1.0 - a_hat*a_hat - b_hat*b_hat) * S\n",
        "    H = S.copy()\n",
        "    for t in range(E.shape[0]):\n",
        "        et = E[t].reshape(-1,1)\n",
        "        H = constS + (a_hat*a_hat) * (et @ et.T) + (b_hat*b_hat) * H\n",
        "        H = make_spd(H, RIDGE_H)\n",
        "    H_T = H.copy()\n",
        "\n",
        "    rows = []\n",
        "    tickers = list(sub.columns)\n",
        "    for t in sorted([w for w in test_idx if w in ret_df.index]):\n",
        "        r = ret_df.loc[t, tickers]\n",
        "        if r.isna().any():\n",
        "            continue\n",
        "        et = (r.values.astype(\"float64\") - mu).reshape(-1,1)\n",
        "        H_next = constS + (a_hat*a_hat) * (et @ et.T) + (b_hat*b_hat) * H_T\n",
        "        H_next = make_spd(H_next, RIDGE_H)\n",
        "        shares = from_others_from_cov(H_next)\n",
        "        rows.append(pd.DataFrame({\"Week\": t, \"Ticker\": tickers, \"yhat_BEKK\": shares}))\n",
        "        H_T = H_next\n",
        "\n",
        "    return pd.concat(rows, ignore_index=True) if rows else pd.DataFrame(columns=[\"Week\",\"Ticker\",\"yhat_BEKK\"])\n",
        "\n",
        "print(\"[Run] Scalar BEKK(1,1) forecasts…\")\n",
        "yhat_bekk_long = s_bekk_fit_and_forecast_long(wk_returns, pretest_weeks, test_weeks)\n",
        "\n",
        "# Align VAR universe to S-BEKK\n",
        "if not yhat_bekk_long.empty:\n",
        "    s_tickers = sorted(yhat_bekk_long[\"Ticker\"].unique().tolist())\n",
        "    wk_returns_var = wk_returns.loc[:, s_tickers].copy()\n",
        "    yhat_var_long = var_fevd_next_forecasts_long(\n",
        "        wk_returns_var.loc[pretest_weeks.union(test_weeks)],\n",
        "        pretest_weeks, test_weeks,\n",
        "        allow_update=ALLOW_UPDATE_ON_TEST\n",
        "    )\n",
        "\n",
        "# ---------- Evaluation ----------\n",
        "Y = labels[[\"Week\",\"Ticker\",\"y_next\"]].copy()\n",
        "yhat = Y[Y[\"Week\"].isin(test_weeks)].copy()\n",
        "yhat = yhat.merge(yhat_var_long,  on=[\"Week\",\"Ticker\"], how=\"left\")\n",
        "yhat = yhat.merge(yhat_bekk_long, on=[\"Week\",\"Ticker\"], how=\"left\")\n",
        "yhat = yhat.dropna(subset=[\"yhat_VAR\",\"yhat_BEKK\"], how=\"all\").reset_index(drop=True)\n",
        "\n",
        "print(f\"[Eval] rows={len(yhat)} | weeks={yhat['Week'].nunique()} | tickers={yhat['Ticker'].nunique()}\")\n",
        "\n",
        "# Overall metrics\n",
        "rows = []\n",
        "for name, col in [(\"VAR-FEVD\",\"yhat_VAR\"), (\"S-BEKK(1,1)\",\"yhat_BEKK\")]:\n",
        "    if col not in yhat or yhat[col].isna().all():\n",
        "        continue\n",
        "    rows.append({\n",
        "        \"model\": name,\n",
        "        \"rmse\": rmse(yhat[\"y_next\"], yhat[col]),\n",
        "        \"mae\":  mae(yhat[\"y_next\"], yhat[col]),\n",
        "        \"r2\":   r2(yhat[\"y_next\"], yhat[col]),\n",
        "        \"n_obs\": int(yhat[col].notna().sum())\n",
        "    })\n",
        "overall = pd.DataFrame(rows).sort_values(\"rmse\") if rows else pd.DataFrame(columns=[\"model\",\"rmse\",\"mae\",\"r2\",\"n_obs\"])\n",
        "overall_path = OUT_DIR / \"metrics_overall.csv\"\n",
        "overall.to_csv(overall_path, index=False)\n",
        "\n",
        "# Per-ticker metrics\n",
        "per_ticker = []\n",
        "for name, col in [(\"VAR-FEVD\",\"yhat_VAR\"), (\"S-BEKK(1,1)\",\"yhat_BEKK\")]:\n",
        "    if col not in yhat or yhat[col].isna().all():\n",
        "        continue\n",
        "    for tk, g in yhat[[\"Ticker\",\"y_next\",col]].dropna(subset=[col]).groupby(\"Ticker\"):\n",
        "        per_ticker.append({\n",
        "            \"model\": name, \"ticker\": tk,\n",
        "            \"rmse\": rmse(g[\"y_next\"], g[col]),\n",
        "            \"mae\":  mae(g[\"y_next\"], g[col]),\n",
        "            \"r2\":   r2(g[\"y_next\"], g[col]),\n",
        "            \"n_obs\": int(len(g))\n",
        "        })\n",
        "by_ticker = pd.DataFrame(per_ticker)\n",
        "by_ticker_path = OUT_DIR / \"metrics_by_ticker.csv\"\n",
        "by_ticker.to_csv(by_ticker_path, index=False)\n",
        "\n",
        "# Save long predictions + a unified long format for plotting\n",
        "preds_path = OUT_DIR / \"predictions_long.parquet\"\n",
        "yhat.to_parquet(preds_path, index=False)\n",
        "\n",
        "preds_unified = []\n",
        "for name, col in [(\"VAR-FEVD\",\"yhat_VAR\"), (\"S-BEKK(1,1)\",\"yhat_BEKK\")]:\n",
        "    if col in yhat:\n",
        "        tmp = yhat[[\"Week\",\"Ticker\",\"y_next\",col]].dropna(subset=[col]).copy()\n",
        "        tmp = tmp.rename(columns={col:\"yhat\"})\n",
        "        tmp[\"model\"] = name\n",
        "        preds_unified.append(tmp)\n",
        "preds_unified = pd.concat(preds_unified, ignore_index=True) if preds_unified else pd.DataFrame(columns=[\"Week\",\"Ticker\",\"y_next\",\"yhat\",\"model\"])\n",
        "preds_unified_path = OUT_DIR / \"preds_unified_long.parquet\"\n",
        "preds_unified.to_parquet(preds_unified_path, index=False)\n",
        "\n",
        "# DM test (panel mean squared error per week)\n",
        "panel = preds_unified.pivot_table(index=[\"Week\",\"model\"], values=[\"y_next\",\"yhat\"], aggfunc=\"mean\").reset_index()\n",
        "panel_rmse = (panel.groupby(\"Week\")\n",
        "                 .apply(lambda g: pd.Series({\n",
        "                     \"se_VAR\":  np.mean((g.loc[g[\"model\"]==\"VAR-FEVD\", \"y_next\"].values - g.loc[g[\"model\"]==\"VAR-FEVD\",\"yhat\"].values)**2) if \"VAR-FEVD\" in g[\"model\"].values else np.nan,\n",
        "                     \"se_BEKK\": np.mean((g.loc[g[\"model\"]==\"S-BEKK(1,1)\",\"y_next\"].values - g.loc[g[\"model\"]==\"S-BEKK(1,1)\",\"yhat\"].values)**2) if \"S-BEKK(1,1)\" in g[\"model\"].values else np.nan\n",
        "                 }))\n",
        "                 .dropna()\n",
        "                 .sort_index())\n",
        "dm_stat, p_val = dm_test(panel_rmse[\"se_VAR\"].values, panel_rmse[\"se_BEKK\"].values, max_lag=None)\n",
        "dm_tbl = pd.DataFrame([{\n",
        "    \"model_A\": \"VAR-FEVD\", \"model_B\": \"S-BEKK(1,1)\",\n",
        "    \"loss\": \"MSE (panel mean per week)\",\n",
        "    \"DM_stat\": dm_stat, \"p_value\": p_val, \"n_weeks\": int(len(panel_rmse))\n",
        "}])\n",
        "dm_path = OUT_DIR / \"dm_test.csv\"\n",
        "dm_tbl.to_csv(dm_path, index=False)\n",
        "\n",
        "# ---------- Plots & tables ----------\n",
        "# Table: overall metrics (CSV + LaTeX)\n",
        "overall_sorted = overall.sort_values(\"rmse\").rename(columns=str.upper)\n",
        "overall_sorted.to_csv(TAB_DIR / \"tab_baseline_overall.csv\", index=False)\n",
        "with open(TAB_DIR / \"tab_baseline_overall.tex\",\"w\") as f:\n",
        "    f.write(overall_sorted.to_latex(index=False, float_format=\"%.4f\",\n",
        "            caption=\"Baseline performance on the test set (lower is better).\",\n",
        "            label=\"tab:baseline_overall\"))\n",
        "\n",
        "# Weekly panel RMSE figure\n",
        "def panel_losses(df):\n",
        "    g = df.groupby([\"Week\",\"model\"], as_index=False).apply(\n",
        "        lambda x: pd.Series({\n",
        "            \"RMSE\": np.sqrt(np.mean((x[\"y_next\"] - x[\"yhat\"])**2)),\n",
        "            \"MAE\":  np.mean(np.abs(x[\"y_next\"] - x[\"yhat\"]))\n",
        "        })\n",
        "    ).reset_index(drop=True)\n",
        "    return g.sort_values(\"Week\")\n",
        "\n",
        "panel_curves = panel_losses(preds_unified)\n",
        "plt.figure(figsize=(10,5))\n",
        "for m, g in panel_curves.groupby(\"model\"):\n",
        "    plt.plot(pd.to_datetime(g[\"Week\"]), g[\"RMSE\"], label=m, linewidth=2)\n",
        "plt.title(\"Weekly Panel RMSE by Baseline Model\")\n",
        "plt.xlabel(\"Week\"); plt.ylabel(\"RMSE\")\n",
        "plt.legend(); plt.grid(True, alpha=0.3); plt.tight_layout()\n",
        "fig_weekly_rmse = FIG_DIR / \"fig_baseline_weekly_panel_rmse.png\"\n",
        "plt.savefig(fig_weekly_rmse, dpi=180); plt.show()\n",
        "\n",
        "# Per-ticker RMSE boxplot\n",
        "order = sorted(by_ticker[\"model\"].unique())\n",
        "data = [by_ticker.loc[by_ticker[\"model\"]==m, \"rmse\"].values for m in order]\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.boxplot(data, labels=order, showfliers=False)\n",
        "plt.title(\"Per-Ticker RMSE Distribution (Baselines)\")\n",
        "plt.ylabel(\"RMSE\"); plt.grid(True, axis=\"y\", alpha=0.3); plt.tight_layout()\n",
        "fig_box_rmse = FIG_DIR / \"fig_baseline_box_rmse_by_ticker.png\"\n",
        "plt.savefig(fig_box_rmse, dpi=180); plt.show()\n",
        "\n",
        "# Actual vs Predicted scatter\n",
        "plt.figure(figsize=(6,6))\n",
        "for m in sorted(preds_unified[\"model\"].unique()):\n",
        "    g = preds_unified[preds_unified[\"model\"]==m]\n",
        "    plt.scatter(g[\"y_next\"], g[\"yhat\"], s=10, alpha=0.45, label=m)\n",
        "lims = [min(preds_unified[\"y_next\"].min(), preds_unified[\"yhat\"].min()),\n",
        "        max(preds_unified[\"y_next\"].max(), preds_unified[\"yhat\"].max())]\n",
        "plt.plot(lims, lims, linewidth=2)\n",
        "plt.xlim(lims); plt.ylim(lims)\n",
        "plt.xlabel(\"Actual spillover (y_next)\")\n",
        "plt.ylabel(\"Predicted spillover (ŷ)\")\n",
        "plt.title(\"Actual vs Predicted — Baselines\")\n",
        "plt.legend(); plt.grid(True, alpha=0.3); plt.tight_layout()\n",
        "fig_scatter = FIG_DIR / \"fig_baseline_scatter_actual_vs_pred.png\"\n",
        "plt.savefig(fig_scatter, dpi=180); plt.show()\n",
        "\n",
        "# Per-ticker DM tests\n",
        "def newey_west_var(d, max_lag=None):\n",
        "    d = np.asarray(d, dtype=float); T = len(d)\n",
        "    if max_lag is None: max_lag = int(np.floor(4 * (T/100.0)**(2/9)))\n",
        "    gamma0 = np.dot(d, d) / T; s = gamma0\n",
        "    for h in range(1, max_lag+1):\n",
        "        w = 1.0 - h/(max_lag+1)\n",
        "        gamma = np.dot(d[h:], d[:-h]) / T\n",
        "        s += 2*w*gamma\n",
        "    return s\n",
        "\n",
        "def dm_test_series(e1, e2, max_lag=None):\n",
        "    d = (e1**2 - e2**2)\n",
        "    d_bar = d.mean()\n",
        "    var_nw = newey_west_var(d - d.mean(), max_lag=max_lag)\n",
        "    stat = d_bar / sqrt(var_nw / len(d)) if var_nw > 0 else np.nan\n",
        "    def norm_cdf(z): return 0.5*(1+erf(z/np.sqrt(2)))\n",
        "    p = 2*(1 - norm_cdf(abs(stat))) if np.isfinite(stat) else np.nan\n",
        "    return stat, p, len(d)\n",
        "\n",
        "pl = preds_unified.pivot_table(index=[\"Week\",\"Ticker\"], columns=\"model\", values=[\"y_next\",\"yhat\"])\n",
        "pl.columns = [f\"{a}_{b}\" for a,b in pl.columns]\n",
        "needed = {\"y_next_VAR-FEVD\",\"yhat_VAR-FEVD\",\"y_next_S-BEKK(1,1)\",\"yhat_S-BEKK(1,1)\"}\n",
        "pl = pl.dropna(subset=list(needed), how=\"any\").reset_index()\n",
        "\n",
        "rows_dm = []\n",
        "for tk, g in pl.groupby(\"Ticker\"):\n",
        "    y  = g[\"y_next_VAR-FEVD\"].values\n",
        "    e1 = (y - g[\"yhat_VAR-FEVD\"].values)\n",
        "    e2 = (y - g[\"yhat_S-BEKK(1,1)\"].values)\n",
        "    if len(e1) >= 12:\n",
        "        stat, p, n = dm_test_series(e1, e2, max_lag=None)\n",
        "        rows_dm.append({\"ticker\": tk, \"DM_stat_VAR_vs_BEKK\": stat, \"p_value\": p, \"n_weeks\": n})\n",
        "dm_by_ticker = pd.DataFrame(rows_dm).sort_values(\"p_value\")\n",
        "dm_by_ticker.to_csv(TAB_DIR / \"tab_dm_per_ticker.csv\", index=False)\n",
        "\n",
        "# ---------- Summary ----------\n",
        "print(\"\\n=== BASELINES SUMMARY ===\")\n",
        "print(\"Saved:\")\n",
        "print(\" - Overall metrics ->\", overall_path)\n",
        "print(\" - By-ticker metrics ->\", by_ticker_path)\n",
        "print(\" - Predictions (long) ->\", preds_path)\n",
        "print(\" - Unified preds (long) ->\", preds_unified_path)\n",
        "print(\" - DM test (overall panel) ->\", dm_path)\n",
        "print(\" - Tables:\", TAB_DIR / \"tab_baseline_overall.csv\", TAB_DIR / \"tab_baseline_overall.tex\", TAB_DIR / \"tab_dm_per_ticker.csv\")\n",
        "print(\" - Figures:\", fig_weekly_rmse, fig_box_rmse, fig_scatter)\n",
        "print(f\"[Config] UPDATE_ON_TEST={ALLOW_UPDATE_ON_TEST} | WIN={ROLL_WINDOW_WEEKS} | LAGS={VAR_LAGS} | FEVD_H={FEVD_H}\")\n",
        "print(f\"[S-BEKK] MAX_N={BEKK_MAX_N} | MIN_T={BEKK_MIN_T} | COVERAGE_BEKK={COVERAGE_BEKK}\")\n"
      ],
      "metadata": {
        "id": "XdwMeuRsUFuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. MLP baseline (feature-only, no graph)\n",
        "\n",
        "A simple feed-forward regressor that uses the standardized node features  \n",
        "$X_{i,t}$ (from Section 2) to predict next-week spillover $y_{i,t}$  \n",
        "(labels from Section 3).\n",
        "\n",
        "We train on the same time splits as the graph models (from `graphs/split_weeks.json`).  \n",
        "The loss and metrics are masked to labeled nodes only (semi-supervised setting),  \n",
        "but we also emit predictions for all test nodes to compare coverage.\n"
      ],
      "metadata": {
        "id": "iYn7wvdxUqIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================ Section 6: MLP Baseline ============================\n",
        "from __future__ import annotations\n",
        "from pathlib import Path\n",
        "import os, json, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# ---- Repro helpers  ----\n",
        "def seed_everything(seed: int = 42):\n",
        "    import random, os\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed); np.random.seed(seed)\n",
        "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(42)\n",
        "\n",
        "# -------------------- PATHS (repo-relative) --------------------\n",
        "BASE_DIR     = Path(\".\")\n",
        "DATA_DIR     = BASE_DIR / \"data\"\n",
        "GRAPHS_DIR   = BASE_DIR / \"graphs\"\n",
        "RESULTS_DIR  = BASE_DIR / \"results\"          # unified results folder\n",
        "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "FEATURES_PATH = DATA_DIR / \"node_features_model.parquet\"   # (Week, Ticker, *_z features)\n",
        "XCOLS_PATH    = DATA_DIR / \"X_cols.json\"                   # list of feature columns to use\n",
        "LABELS_PATH   = DATA_DIR / \"labels_from_next.parquet\"      # (Week, Ticker, y_next)\n",
        "SPLIT_JSON    = GRAPHS_DIR / \"split_weeks.json\"            # weeks for train/val/test\n",
        "SUMMARY_CSV   = RESULTS_DIR / \"summary.csv\"\n",
        "\n",
        "MODEL_NAME    = \"MLP\"\n",
        "\n",
        "# -------------------- LOAD FEATURES & LABELS --------------------\n",
        "feat = pd.read_parquet(FEATURES_PATH).copy()\n",
        "feat[\"Week\"] = pd.to_datetime(feat[\"Week\"]).dt.to_period(\"W-FRI\").dt.end_time.dt.normalize()\n",
        "\n",
        "with open(XCOLS_PATH, \"r\") as f:\n",
        "    X_cols = json.load(f)\n",
        "\n",
        "labels = pd.read_parquet(LABELS_PATH).copy()\n",
        "labels[\"Week\"] = pd.to_datetime(labels[\"Week\"]).dt.to_period(\"W-FRI\").dt.end_time.dt.normalize()\n",
        "labels = labels.rename(columns={\"y_next\": \"y_true\"})  # align with evaluation naming\n",
        "\n",
        "# Keep only needed columns from features\n",
        "feat = feat[[\"Week\",\"Ticker\"] + X_cols].copy()\n",
        "\n",
        "# -------------------- ATTACH SPLITS (from split_weeks.json if present) --------------------\n",
        "def _weeks_str_to_wfri(weeks):\n",
        "    if not weeks: return set()\n",
        "    w = pd.to_datetime(pd.Series(weeks))\n",
        "    w = set(w.dt.to_period(\"W-FRI\").dt.end_time.dt.normalize().tolist())\n",
        "    return w\n",
        "\n",
        "if SPLIT_JSON.exists():\n",
        "    with open(SPLIT_JSON, \"r\") as f:\n",
        "        sp = json.load(f)\n",
        "    train_weeks = _weeks_str_to_wfri(sp.get(\"train_weeks\", []))\n",
        "    val_weeks   = _weeks_str_to_wfri(sp.get(\"val_weeks\", []))\n",
        "    test_weeks  = _weeks_str_to_wfri(sp.get(\"test_weeks\", []))\n",
        "else:\n",
        "    # fallback: chronological 70/15/15 over weeks present in features\n",
        "    wk_sorted = sorted(feat[\"Week\"].unique())\n",
        "    W = len(wk_sorted)\n",
        "    i_tr = math.floor(0.70 * W)\n",
        "    i_va = i_tr + math.floor(0.15 * W)\n",
        "    train_weeks = set(wk_sorted[:i_tr])\n",
        "    val_weeks   = set(wk_sorted[i_tr:i_va])\n",
        "    test_weeks  = set(wk_sorted[i_va:])\n",
        "\n",
        "# -------------------- MERGE & PREP DATAFRAME --------------------\n",
        "df = feat.merge(labels[[\"Week\",\"Ticker\",\"y_true\"]], on=[\"Week\",\"Ticker\"], how=\"left\")\n",
        "df[\"split\"] = np.where(df[\"Week\"].isin(train_weeks), \"train\",\n",
        "               np.where(df[\"Week\"].isin(val_weeks),   \"val\",\n",
        "               np.where(df[\"Week\"].isin(test_weeks),  \"test\", \"other\")))\n",
        "df = df[df[\"split\"].isin([\"train\",\"val\",\"test\"])].copy()\n",
        "df = df.sort_values([\"Week\",\"Ticker\"]).reset_index(drop=True)\n",
        "\n",
        "# numeric cast + impute zeros (already z-standardized upstream)\n",
        "for c in X_cols:\n",
        "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "df[X_cols] = df[X_cols].fillna(0.0)\n",
        "\n",
        "# -------------------- BUILD TENSORS (masked by labels for scoring) --------------------\n",
        "def make_tensor_split(dfall: pd.DataFrame, split: str):\n",
        "    d = dfall[dfall[\"split\"] == split].copy()\n",
        "    X = d[X_cols].values.astype(np.float32)\n",
        "    y = d[\"y_true\"].values.astype(np.float32)\n",
        "    mask = ~np.isnan(y)\n",
        "    y = np.nan_to_num(y, nan=0.0).astype(np.float32)  # placeholders ignored by mask\n",
        "    return d, torch.tensor(X), torch.tensor(y), torch.tensor(mask)\n",
        "\n",
        "df_tr, X_tr, y_tr, m_tr = make_tensor_split(df, \"train\")\n",
        "df_va, X_va, y_va, m_va = make_tensor_split(df, \"val\")\n",
        "df_te, X_te, y_te, m_te = make_tensor_split(df, \"test\")\n",
        "\n",
        "print(\"Rows — train:\", len(df_tr), \"| val:\", len(df_va), \"| test:\", len(df_te))\n",
        "print(\"Labeled % — train:\", float(m_tr.float().mean()), \"| val:\", float(m_va.float().mean()), \"| test:\", float(m_te.float().mean()))\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# -------------------- DEFINE MLP --------------------\n",
        "class MLPReg(nn.Module):\n",
        "    def __init__(self, in_dim: int, hidden: int = 256, dropout: float = 0.10):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden, 1),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x).squeeze(-1)\n",
        "\n",
        "# -------------------- TRAINING --------------------\n",
        "HIDDEN    = 256\n",
        "DROPOUT   = 0.10\n",
        "LR        = 1e-3\n",
        "WD        = 1e-4\n",
        "EPOCHS    = 120\n",
        "PATIENCE  = 20\n",
        "CLIP_NORM = 2.0\n",
        "\n",
        "model = MLPReg(in_dim=len(X_cols), hidden=HIDDEN, dropout=DROPOUT).to(device)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WD)\n",
        "\n",
        "X_tr_t, y_tr_t, m_tr_t = X_tr.to(device), y_tr.to(device), m_tr.to(device)\n",
        "X_va_t, y_va_t, m_va_t = X_va.to(device), y_va.to(device), m_va.to(device)\n",
        "X_te_t, y_te_t, m_te_t = X_te.to(device), y_te.to(device), m_te.to(device)\n",
        "\n",
        "def masked_rmse(y_pred, y_true, mask):\n",
        "    if mask.sum().item() == 0: return np.nan\n",
        "    p = y_pred[mask].detach().cpu().numpy()\n",
        "    t = y_true[mask].detach().cpu().numpy()\n",
        "    return float(np.sqrt(((p - t) ** 2).mean()))\n",
        "\n",
        "def masked_mae(y_pred, y_true, mask):\n",
        "    if mask.sum().item() == 0: return np.nan\n",
        "    p = y_pred[mask].detach().cpu().numpy()\n",
        "    t = y_true[mask].detach().cpu().numpy()\n",
        "    return float(np.abs(p - t).mean())\n",
        "\n",
        "best = {\"val_rmse\": np.inf, \"state\": None, \"epoch\": -1}\n",
        "logs = []\n",
        "\n",
        "for ep in range(1, EPOCHS + 1):\n",
        "    # train\n",
        "    model.train()\n",
        "    opt.zero_grad()\n",
        "    pred_tr = model(X_tr_t)\n",
        "    loss = F.mse_loss(pred_tr[m_tr_t], y_tr_t[m_tr_t]) if m_tr_t.sum() > 0 else torch.tensor(0.0, device=device)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n",
        "    opt.step()\n",
        "\n",
        "    # validate\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        pred_va = model(X_va_t)\n",
        "    tr_rmse = masked_rmse(pred_tr, y_tr_t, m_tr_t)\n",
        "    va_rmse = masked_rmse(pred_va, y_va_t, m_va_t)\n",
        "\n",
        "    logs.append({\"epoch\": ep, \"train_rmse\": tr_rmse, \"val_rmse\": va_rmse})\n",
        "    print(f\"[{MODEL_NAME}] epoch {ep:03d} | train RMSE={tr_rmse:.4f} | val RMSE={va_rmse:.4f}\")\n",
        "\n",
        "    cur = va_rmse\n",
        "    if not np.isnan(cur) and cur < best[\"val_rmse\"] - 1e-6:\n",
        "        best.update({\"val_rmse\": cur, \"state\": {k: v.detach().cpu() for k, v in model.state_dict().items()}, \"epoch\": ep})\n",
        "    elif ep - best[\"epoch\"] >= PATIENCE:\n",
        "        print(f\"[{MODEL_NAME}] Early stop at epoch {ep}. Best val RMSE={best['val_rmse']:.4f} (epoch {best['epoch']})\")\n",
        "        break\n",
        "\n",
        "# restore best\n",
        "if best[\"state\"] is not None:\n",
        "    model.load_state_dict({k: v.to(device) for k, v in best[\"state\"].items()})\n",
        "\n",
        "# save checkpoint & logs\n",
        "torch.save(model.state_dict(), RESULTS_DIR / f\"{MODEL_NAME}_best.pt\")\n",
        "pd.DataFrame(logs).to_csv(RESULTS_DIR / f\"{MODEL_NAME}_logs.csv\", index=False)\n",
        "\n",
        "# -------------------- TEST METRICS (on labeled only) --------------------\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    pred_te = model(X_te_t)\n",
        "\n",
        "test_rmse = masked_rmse(pred_te, y_te_t, m_te_t)\n",
        "test_mae  = masked_mae(pred_te, y_te_t, m_te_t)\n",
        "if m_te_t.sum().item() > 0:\n",
        "    p = pred_te[m_te_t].detach().cpu().numpy()\n",
        "    t = y_te_t[m_te_t].detach().cpu().numpy()\n",
        "    test_r2 = float(r2_score(t, p))\n",
        "else:\n",
        "    test_r2 = np.nan\n",
        "\n",
        "print(f\"[{MODEL_NAME}] TEST: RMSE={test_rmse:.4f} | MAE={test_mae:.4f} | R2={test_r2:.4f}\")\n",
        "\n",
        "# -------------------- SAVE PREDICTIONS --------------------\n",
        "# (A) labeled-only predictions for TEST — same schema others use\n",
        "te_labeled = df_te[m_te.numpy()].copy()\n",
        "te_labeled[\"y_pred\"] = pred_te[m_te_t].detach().cpu().numpy()\n",
        "out_a = RESULTS_DIR / f\"{MODEL_NAME}_test_preds.parquet\"\n",
        "te_labeled[[\"Week\",\"Ticker\",\"y_true\",\"y_pred\"]].to_parquet(out_a, index=False)\n",
        "\n",
        "# (B) full TEST predictions (all nodes)\n",
        "te_full = df_te.copy()\n",
        "te_full[\"y_pred\"] = pred_te.detach().cpu().numpy()\n",
        "te_full[\"has_label\"] = ~te_full[\"y_true\"].isna()\n",
        "out_b = RESULTS_DIR / f\"{MODEL_NAME}_test_preds_full.parquet\"\n",
        "te_full[[\"Week\",\"Ticker\",\"y_true\",\"y_pred\",\"has_label\"]].to_parquet(out_b, index=False)\n",
        "\n",
        "print(\"Saved:\")\n",
        "print(\" -\", RESULTS_DIR / f\"{MODEL_NAME}_best.pt\")\n",
        "print(\" -\", RESULTS_DIR / f\"{MODEL_NAME}_logs.csv\")\n",
        "print(\" -\", out_a)\n",
        "print(\" -\", out_b)\n",
        "\n",
        "# -------------------- SUMMARY --------------------\n",
        "row = pd.DataFrame([{\n",
        "    \"Model\": MODEL_NAME,\n",
        "    \"Val_RMSE\": round(float(best[\"val_rmse\"]), 6) if np.isfinite(best[\"val_rmse\"]) else np.nan,\n",
        "    \"Test_RMSE\": round(float(test_rmse), 6) if np.isfinite(test_rmse) else np.nan,\n",
        "    \"Test_MAE\":  round(float(test_mae),  6) if np.isfinite(test_mae)  else np.nan,\n",
        "    \"Test_R2\":   round(float(test_r2),   6) if np.isfinite(test_r2)   else np.nan\n",
        "}])\n",
        "\n",
        "if SUMMARY_CSV.exists():\n",
        "    sm = pd.read_csv(SUMMARY_CSV)\n",
        "    sm = sm[sm[\"Model\"] != MODEL_NAME]\n",
        "    sm = pd.concat([sm, row], ignore_index=True)\n",
        "else:\n",
        "    sm = row\n",
        "sm = sm.sort_values(\"Test_RMSE\").reset_index(drop=True)\n",
        "sm.to_csv(SUMMARY_CSV, index=False)\n",
        "print(\"Updated summary →\", SUMMARY_CSV)\n",
        "print(sm)\n"
      ],
      "metadata": {
        "id": "3Qj8R0yDV1ZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Graph Neural Network Architectures\n",
        "\n",
        "We now train a family of GNN models on the semi-supervised weekly graphs.  \n",
        "Only nodes with available labels contribute to the loss, but all nodes receive predictions.  \n",
        "The following architectures are included:\n",
        "\n",
        "- **WSAGE**: Weighted GraphSAGE with edge-weight normalization.  \n",
        "- **GAT**: Graph Attention Network (multi-head).  \n",
        "- **TAG**: Topology Adaptive GCN (K=3).  \n",
        "- **Chebyshev GCN**: Chebyshev polynomial spectral GCN (K=3).  \n",
        "- **ECC**: Edge-Conditioned Convolution (edge weights passed through an MLP).  \n",
        "- **Temporal-GCN**: GCN applied to sequences of past graphs with a GRU to capture short-term dynamics.  \n",
        "\n",
        "Each model is trained with Adam (lr = 1e-3, wd = 1e-4), hidden dimension = 96, dropout = 0.2,  \n",
        "and early stopping (patience = 20 epochs). Best validation checkpoints are saved,  \n",
        "and predictions are exported in the same schema as the baselines.\n"
      ],
      "metadata": {
        "id": "DqRBqGmrWB9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import argparse\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import (\n",
        "    GATConv, TAGConv, ChebConv, NNConv\n",
        ")\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_scatter import scatter_add\n",
        "\n",
        "\n",
        "# -------------------- CLI --------------------\n",
        "def get_args():\n",
        "    p = argparse.ArgumentParser(description=\"Train & evaluate GNNs on weekly graphs.\")\n",
        "    p.add_argument(\"--graphs_dir\", type=str, default=\"./graphs\", help=\"Folder with saved weekly Data objects\")\n",
        "    p.add_argument(\"--graph_index_csv\", type=str, default=\"./graphs/graphs_index.csv\", help=\"Index CSV with Week,file\")\n",
        "    p.add_argument(\"--split_json\", type=str, default=\"./graphs/split_weeks.json\", help=\"Optional explicit split JSON\")\n",
        "    p.add_argument(\"--results_dir\", type=str, default=\"./results_gnn\", help=\"Where to save checkpoints/logs/preds\")\n",
        "\n",
        "    # training hparams\n",
        "    p.add_argument(\"--hidden\", type=int, default=96)\n",
        "    p.add_argument(\"--dropout\", type=float, default=0.20)\n",
        "    p.add_argument(\"--lr\", type=float, default=1e-3)\n",
        "    p.add_argument(\"--wd\", type=float, default=1e-4)\n",
        "    p.add_argument(\"--epochs\", type=int, default=120)\n",
        "    p.add_argument(\"--patience\", type=int, default=20)\n",
        "\n",
        "    # temporal\n",
        "    p.add_argument(\"--t_hist\", type=int, default=4, help=\"Temporal history length (for Temporal GCN)\")\n",
        "\n",
        "    # misc\n",
        "    p.add_argument(\"--debug\", action=\"store_true\", help=\"Verbose per-graph logs for first few epochs\")\n",
        "    p.add_argument(\"--debug_epochs\", type=int, default=2)\n",
        "    p.add_argument(\"--seed\", type=int, default=42)\n",
        "    return p.parse_args()\n",
        "\n",
        "\n",
        "# -------------------- SEEDING --------------------\n",
        "def seed_everything(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    # Torch backends for determinism (may reduce throughput)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# -------------------- MODELS (only the active ones) --------------------\n",
        "class WeightedSAGEConv(MessagePassing):\n",
        "    \"\"\"Neighbor aggregation with edge weights (normalized per destination).\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, aggr='add', bias=True):\n",
        "        super().__init__(aggr=aggr)\n",
        "        self.lin_neigh = nn.Linear(in_channels, out_channels, bias=False)\n",
        "        self.lin_self  = nn.Linear(in_channels, out_channels, bias=bias)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight=None, use_abs_weight=True,\n",
        "                add_self_loops=True, self_loop_weight=1.0):\n",
        "        N = x.size(0)\n",
        "        if edge_weight is None:\n",
        "            edge_weight = torch.ones(edge_index.size(1), device=x.device, dtype=x.dtype)\n",
        "        if use_abs_weight:\n",
        "            edge_weight = edge_weight.abs()\n",
        "\n",
        "        if add_self_loops:\n",
        "            loop_index = torch.arange(N, device=x.device)\n",
        "            loop_ei = torch.stack([loop_index, loop_index], dim=0)\n",
        "            edge_index = torch.cat([edge_index, loop_ei], dim=1)\n",
        "            loop_w = torch.full((N,), float(self_loop_weight), device=x.device, dtype=x.dtype)\n",
        "            edge_weight = torch.cat([edge_weight, loop_w], dim=0)\n",
        "\n",
        "        src, dst = edge_index[0], edge_index[1]\n",
        "        w_sum = scatter_add(edge_weight, dst, dim=0, dim_size=N).clamp_min(1e-12)\n",
        "        norm_w = edge_weight / w_sum[dst]\n",
        "\n",
        "        x_neigh = self.lin_neigh(x)\n",
        "        out = self.propagate(edge_index, x=x_neigh, weight=norm_w)\n",
        "        out = out + self.lin_self(x)\n",
        "        return out\n",
        "\n",
        "    def message(self, x_j, weight):\n",
        "        return weight.view(-1, 1) * x_j\n",
        "\n",
        "\n",
        "class WeightedSAGEReg(nn.Module):\n",
        "    def __init__(self, in_dim, hidden=64, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.conv1 = WeightedSAGEConv(in_dim, hidden)\n",
        "        self.conv2 = WeightedSAGEConv(hidden, hidden)\n",
        "        self.lin   = nn.Linear(hidden, 1)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight=None):\n",
        "        x = self.conv1(x, edge_index, edge_weight=edge_weight)\n",
        "        x = F.relu(x); x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.conv2(x, edge_index, edge_weight=edge_weight)\n",
        "        x = F.relu(x); x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return self.lin(x).squeeze(-1)\n",
        "\n",
        "\n",
        "class GATReg(nn.Module):\n",
        "    def __init__(self, in_dim, hidden=64, heads=4, dropout=0.2):\n",
        "        super().__init__()\n",
        "        hidden = int(hidden); heads = int(heads)\n",
        "        out_per_head = max(1, hidden // heads)\n",
        "        self.gat1 = GATConv(in_dim, out_per_head, heads=heads, dropout=dropout)\n",
        "        self.gat2 = GATConv(out_per_head*heads, out_per_head, heads=heads, dropout=dropout)\n",
        "        self.lin  = nn.Linear(out_per_head*heads, 1)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight=None):\n",
        "        x = self.gat1(x, edge_index)\n",
        "        x = F.elu(x); x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.gat2(x, edge_index)\n",
        "        x = F.elu(x); x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return self.lin(x).squeeze(-1)\n",
        "\n",
        "\n",
        "class TAGReg(nn.Module):\n",
        "    def __init__(self, in_dim, hidden=64, K=3, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.conv1 = TAGConv(in_dim, hidden, K=K)\n",
        "        self.conv2 = TAGConv(hidden, hidden, K=K)\n",
        "        self.lin   = nn.Linear(hidden, 1)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight=None):\n",
        "        ew = edge_weight.abs() if edge_weight is not None else None\n",
        "        x = self.conv1(x, edge_index, ew)\n",
        "        x = F.relu(x); x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.conv2(x, edge_index, ew)\n",
        "        x = F.relu(x); x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return self.lin(x).squeeze(-1)\n",
        "\n",
        "\n",
        "class ChebReg(nn.Module):\n",
        "    def __init__(self, in_dim, hidden=64, K=3, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.conv1 = ChebConv(in_dim, hidden, K=K)\n",
        "        self.conv2 = ChebConv(hidden, hidden, K=K)\n",
        "        self.lin   = nn.Linear(hidden, 1)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight=None):\n",
        "        ew = edge_weight.abs() if edge_weight is not None else None\n",
        "        x = self.conv1(x, edge_index, ew)\n",
        "        x = F.relu(x); x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.conv2(x, edge_index, ew)\n",
        "        x = F.relu(x); x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return self.lin(x).squeeze(-1)\n",
        "\n",
        "\n",
        "class ECCReg(nn.Module):\n",
        "    def __init__(self, in_dim, hidden=64, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.edge_net1 = nn.Sequential(nn.Linear(1, hidden), nn.ReLU(), nn.Linear(hidden, in_dim * hidden))\n",
        "        self.edge_net2 = nn.Sequential(nn.Linear(1, hidden), nn.ReLU(), nn.Linear(hidden, hidden * hidden))\n",
        "        self.conv1 = NNConv(in_dim, hidden, self.edge_net1, aggr='mean')\n",
        "        self.conv2 = NNConv(hidden, hidden, self.edge_net2, aggr='mean')\n",
        "        self.lin   = nn.Linear(hidden, 1)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight=None):\n",
        "        edge_attr = edge_weight.abs().view(-1, 1) if edge_weight is not None else None\n",
        "        x = self.conv1(x, edge_index, edge_attr)\n",
        "        x = F.relu(x); x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.conv2(x, edge_index, edge_attr)\n",
        "        x = F.relu(x); x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return self.lin(x).squeeze(-1)\n",
        "\n",
        "\n",
        "class TemporalGCN(nn.Module):\n",
        "    \"\"\"GCN over each week, then GRU across time; predicts on final week in the sequence.\"\"\"\n",
        "    def __init__(self, in_dim, hidden=64, dropout=0.2):\n",
        "        super().__init__()\n",
        "        # Use TAGConv or GCNConv—here we reuse TAG-like idea via TAGConv with K=1 to avoid extra import\n",
        "        self.gcn = TAGConv(in_dim, hidden, K=1)\n",
        "        self.gru = nn.GRU(hidden, hidden, batch_first=True)\n",
        "        self.lin = nn.Linear(hidden, 1)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, seq):\n",
        "        H_seq = []\n",
        "        for g in seq:\n",
        "            ew = g.edge_weight.abs() if g.edge_weight is not None else None\n",
        "            h  = F.relu(self.gcn(g.x, g.edge_index, ew))\n",
        "            h  = F.dropout(h, p=self.dropout, training=self.training)\n",
        "            H_seq.append(h.unsqueeze(1))  # [N, 1, H]\n",
        "        H = torch.cat(H_seq, dim=1)       # [N, T, H]\n",
        "        out, _ = self.gru(H)              # [N, T, H]\n",
        "        return self.lin(out[:, -1, :]).squeeze(-1)  # last step\n",
        "\n",
        "\n",
        "# -------------------- METRICS --------------------\n",
        "def masked_metrics(pred, y, mask):\n",
        "    if mask.sum().item() == 0:\n",
        "        return {\"rmse\": np.nan, \"mae\": np.nan, \"r2\": np.nan}\n",
        "    p = pred[mask].detach().cpu().numpy()\n",
        "    t = y[mask].detach().cpu().numpy()\n",
        "    mse  = float(((p - t) ** 2).mean())\n",
        "    rmse = float(np.sqrt(mse))\n",
        "    mae  = float(np.abs(p - t).mean())\n",
        "    var_t = float(np.var(t))\n",
        "    r2 = float(1.0 - (np.sum((p - t) ** 2) / (np.sum((t - t.mean()) ** 2) + 1e-12))) if var_t > 1e-12 else np.nan\n",
        "    return {\"rmse\": rmse, \"mae\": mae, \"r2\": r2}\n",
        "\n",
        "\n",
        "# -------------------- DATA HELPERS --------------------\n",
        "def attach_split_masks_if_missing(g: Data, week_ts: pd.Timestamp, idx_df: pd.DataFrame):\n",
        "    \"\"\"Attach boolean week-level split flags to each node; fall back to 70/15/15 time split.\"\"\"\n",
        "    n = g.x.size(0)\n",
        "    def mask(allw):\n",
        "        return torch.tensor([week_ts in allw] * n, dtype=torch.bool)\n",
        "\n",
        "    if hasattr(g, \"is_train_week\") and hasattr(g, \"is_val_week\") and hasattr(g, \"is_test_week\"):\n",
        "        return g\n",
        "\n",
        "    # determine split from global idx if needed\n",
        "    pos = int(idx_df.index[idx_df[\"Week\"] == week_ts][0])\n",
        "    W   = len(idx_df)\n",
        "    i_train_end = math.floor(0.70 * W)\n",
        "    i_val_end   = i_train_end + math.floor(0.15 * W)\n",
        "    g.is_train_week = torch.tensor([pos < i_train_end] * n, dtype=torch.bool)\n",
        "    g.is_val_week   = torch.tensor([(pos >= i_train_end) and (pos < i_val_end)] * n, dtype=torch.bool)\n",
        "    g.is_test_week  = torch.tensor([pos >= i_val_end] * n, dtype=torch.bool)\n",
        "    return g\n",
        "\n",
        "\n",
        "def align_graph_to_tickers(g: Data, target_tickers: list):\n",
        "    \"\"\"Realign a graph's nodes to a target ticker order; drop missing; remap edges.\"\"\"\n",
        "    idx_map = {tk: i for i, tk in enumerate(g.tickers)}\n",
        "    N_new = len(target_tickers)\n",
        "    x_new = torch.zeros((N_new, g.x.size(1)), dtype=g.x.dtype)\n",
        "    y_new = torch.zeros((N_new,), dtype=g.y.dtype)\n",
        "    label_mask_new = torch.zeros((N_new,), dtype=torch.bool)\n",
        "\n",
        "    for j, tk in enumerate(target_tickers):\n",
        "        i = idx_map.get(tk, None)\n",
        "        if i is not None:\n",
        "            x_new[j] = g.x[i]\n",
        "            y_new[j] = g.y[i]\n",
        "            label_mask_new[j] = g.label_mask[i] if hasattr(g, \"label_mask\") else False\n",
        "\n",
        "    old_to_new = {idx_map[tk]: j for j, tk in enumerate(target_tickers) if tk in idx_map}\n",
        "\n",
        "    if g.edge_index.numel() > 0:\n",
        "        ei = g.edge_index.numpy()\n",
        "        src_old, dst_old = ei[0], ei[1]\n",
        "        mask_pairs = np.array([(s in old_to_new) and (d in old_to_new) for s, d in zip(src_old, dst_old)])\n",
        "        src_new = [old_to_new[s] for s, d in zip(src_old, dst_old) if (s in old_to_new and d in old_to_new)]\n",
        "        dst_new = [old_to_new[d] for s, d in zip(src_old, dst_old) if (s in old_to_new and d in old_to_new)]\n",
        "        edge_index_new = torch.tensor([src_new, dst_new], dtype=torch.long)\n",
        "        if hasattr(g, \"edge_weight\") and g.edge_weight is not None:\n",
        "            edge_weight_new = g.edge_weight[torch.tensor(mask_pairs, dtype=torch.bool)]\n",
        "        else:\n",
        "            edge_weight_new = None\n",
        "    else:\n",
        "        edge_index_new = torch.empty((2, 0), dtype=torch.long)\n",
        "        edge_weight_new = None\n",
        "\n",
        "    g2 = Data(x=x_new, edge_index=edge_index_new, edge_weight=edge_weight_new, y=y_new)\n",
        "    g2.label_mask    = label_mask_new\n",
        "    g2.is_train_week = torch.tensor([g.is_train_week[0]] * N_new, dtype=torch.bool)\n",
        "    g2.is_val_week   = torch.tensor([g.is_val_week[0]] * N_new, dtype=torch.bool)\n",
        "    g2.is_test_week  = torch.tensor([g.is_test_week[0]] * N_new, dtype=torch.bool)\n",
        "    g2.tickers = target_tickers\n",
        "    g2.week    = g.week\n",
        "    return g2\n",
        "\n",
        "\n",
        "def collect_sequences(graphs_all, split_tag, t_hist=4):\n",
        "    \"\"\"Build rolling sequences of length t_hist ending at each target week t (for Temporal GCN).\"\"\"\n",
        "    seqs = []\n",
        "    for t in range(len(graphs_all)):\n",
        "        week_t, g_t = graphs_all[t]\n",
        "        if split_tag == \"train\" and not g_t.is_train_week[0]: continue\n",
        "        if split_tag == \"val\"   and not g_t.is_val_week[0]:   continue\n",
        "        if split_tag == \"test\"  and not g_t.is_test_week[0]:  continue\n",
        "        if t - (t_hist - 1) < 0: continue\n",
        "\n",
        "        tgt_tickers = g_t.tickers\n",
        "        seq = []\n",
        "        for k in range(t_hist):\n",
        "            _, g_prev = graphs_all[t - (t_hist - 1) + k]\n",
        "            seq.append(align_graph_to_tickers(g_prev, tgt_tickers))\n",
        "        seqs.append((week_t, seq))\n",
        "    return seqs\n",
        "\n",
        "\n",
        "# -------------------- EPOCH LOOPS --------------------\n",
        "def run_epoch_static(model, graphs_list, device, optimizer=None, epoch_idx=0,\n",
        "                     model_name=\"\", debug=False, debug_epochs=2):\n",
        "    train = optimizer is not None\n",
        "    model.train() if train else model.eval()\n",
        "    total_loss, total_count = 0.0, 0\n",
        "    agg = {\"rmse\": [], \"mae\": [], \"r2\": []}\n",
        "\n",
        "    for week_ts, g in graphs_list:\n",
        "        x  = g.x.to(device)\n",
        "        ei = g.edge_index.to(device)\n",
        "        ew = g.edge_weight.to(device) if hasattr(g, \"edge_weight\") and g.edge_weight is not None else None\n",
        "        y  = g.y.to(device).float()\n",
        "\n",
        "        lb = getattr(g, \"label_mask\", None)\n",
        "        if lb is None and hasattr(g, \"train_mask\"):\n",
        "            lb = g.train_mask\n",
        "        if lb is None:\n",
        "            lb = torch.zeros_like(g.y, dtype=torch.bool)\n",
        "\n",
        "        if train:\n",
        "            mask = (lb & g.is_train_week).to(device)\n",
        "        else:\n",
        "            split_mask = g.is_val_week if g.is_val_week[0] else g.is_test_week\n",
        "            mask = (lb & split_mask).to(device)\n",
        "\n",
        "        pred = model(x, ei, edge_weight=ew)\n",
        "\n",
        "        used = int(mask.sum().item()); loss_val = np.nan\n",
        "        if used > 0:\n",
        "            loss = F.mse_loss(pred[mask], y[mask])\n",
        "            loss_val = float(loss.detach().cpu().item())\n",
        "            if train:\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
        "                optimizer.step()\n",
        "            total_loss += loss_val * used\n",
        "            total_count += used\n",
        "            m = masked_metrics(pred, y, mask)\n",
        "            agg[\"rmse\"].append(m[\"rmse\"]); agg[\"mae\"].append(m[\"mae\"]); agg[\"r2\"].append(m[\"r2\"])\n",
        "\n",
        "        if debug and epoch_idx < debug_epochs:\n",
        "            e_cnt = int(ei.size(1))\n",
        "            ew_min = float(ew.min().item()) if ew is not None and ew.numel() else float(\"nan\")\n",
        "            ew_max = float(ew.max().item()) if ew is not None and ew.numel() else float(\"nan\")\n",
        "            print(f\"[{model_name}][ep{epoch_idx:02d}] {str(week_ts)[:10]} N={x.size(0)} E={e_cnt} \"\n",
        "                  f\"used={used} loss={loss_val:.6f} ew=[{ew_min:.3f},{ew_max:.3f}]\")\n",
        "\n",
        "    mean_loss = (total_loss / total_count) if total_count > 0 else np.nan\n",
        "    mean_metrics = {k: (np.nanmean(v) if len(v) else np.nan) for k, v in agg.items()}\n",
        "    return mean_loss, mean_metrics\n",
        "\n",
        "\n",
        "def run_epoch_temporal(model, seqs_list, device, optimizer=None, epoch_idx=0,\n",
        "                       model_name=\"\", debug=False, debug_epochs=2):\n",
        "    train = optimizer is not None\n",
        "    model.train() if train else model.eval()\n",
        "    total_loss, total_count = 0.0, 0\n",
        "    agg = {\"rmse\": [], \"mae\": [], \"r2\": []}\n",
        "\n",
        "    for week_t, seq in seqs_list:\n",
        "        # move each graph in the sequence to device\n",
        "        seq_dev = []\n",
        "        for g in seq:\n",
        "            g_dev = Data(\n",
        "                x=g.x.to(device),\n",
        "                edge_index=g.edge_index.to(device),\n",
        "                edge_weight=(g.edge_weight.to(device) if g.edge_weight is not None else None),\n",
        "                y=g.y.to(device).float()\n",
        "            )\n",
        "            g_dev.label_mask    = g.label_mask.to(device)\n",
        "            g_dev.is_train_week = g.is_train_week.to(device)\n",
        "            g_dev.is_val_week   = g.is_val_week.to(device)\n",
        "            g_dev.is_test_week  = g.is_test_week.to(device)\n",
        "            g_dev.tickers = g.tickers\n",
        "            g_dev.week    = g.week\n",
        "            seq_dev.append(g_dev)\n",
        "\n",
        "        gT = seq_dev[-1]\n",
        "        y  = gT.y\n",
        "        lb = gT.label_mask\n",
        "\n",
        "        if train:\n",
        "            mask = (lb & gT.is_train_week)\n",
        "        else:\n",
        "            split_mask = gT.is_val_week if gT.is_val_week[0] else gT.is_test_week\n",
        "            mask = (lb & split_mask)\n",
        "\n",
        "        pred = model(seq_dev)\n",
        "\n",
        "        used = int(mask.sum().item()); loss_val = np.nan\n",
        "        if used > 0:\n",
        "            loss = F.mse_loss(pred[mask], y[mask])\n",
        "            loss_val = float(loss.detach().cpu().item())\n",
        "            if train:\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
        "                optimizer.step()\n",
        "            total_loss += loss_val * used\n",
        "            total_count += used\n",
        "            m = masked_metrics(pred, y, mask)\n",
        "            agg[\"rmse\"].append(m[\"rmse\"]); agg[\"mae\"].append(m[\"mae\"]); agg[\"r2\"].append(m[\"r2\"])\n",
        "\n",
        "        if debug and epoch_idx < debug_epochs:\n",
        "            print(f\"[{model_name}][ep{epoch_idx:02d}] {str(week_t)[:10]} used={used} loss={loss_val:.6f} (temporal)\")\n",
        "\n",
        "    mean_loss = (total_loss / total_count) if total_count > 0 else np.nan\n",
        "    mean_metrics = {k: (np.nanmean(v) if len(v) else np.nan) for k, v in agg.items()}\n",
        "    return mean_loss, mean_metrics\n",
        "\n",
        "\n",
        "# -------------------- PREDICTION EXPORT --------------------\n",
        "def predict_static_on_split(model, graphs_list, device, split=\"test\", save_all_nodes=False):\n",
        "    \"\"\"\n",
        "    If save_all_nodes=False: return only rows evaluated (mask==True).\n",
        "    If save_all_nodes=True:  return ALL nodes; y_true is NaN where mask==False and has_label marks availability.\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    model.eval()\n",
        "    for week_ts, g in graphs_list:\n",
        "        # split filter\n",
        "        if split == \"test\" and not g.is_test_week[0]:   continue\n",
        "        if split == \"val\"  and not g.is_val_week[0]:    continue\n",
        "        if split == \"train\" and not g.is_train_week[0]: continue\n",
        "\n",
        "        x  = g.x.to(device)\n",
        "        ei = g.edge_index.to(device)\n",
        "        ew = g.edge_weight.to(device) if hasattr(g, \"edge_weight\") and g.edge_weight is not None else None\n",
        "        y  = g.y.to(device).float()\n",
        "\n",
        "        lb = getattr(g, \"label_mask\", None)\n",
        "        if lb is None and hasattr(g, \"train_mask\"):\n",
        "            lb = g.train_mask\n",
        "        if lb is None:\n",
        "            lb = torch.zeros_like(g.y, dtype=torch.bool)\n",
        "\n",
        "        split_mask = (g.is_test_week if split == \"test\" else g.is_val_week if split == \"val\" else g.is_train_week)\n",
        "        mask = (lb & split_mask).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred = model(x, ei, edge_weight=ew)\n",
        "\n",
        "        if save_all_nodes:\n",
        "            use_idx = np.arange(g.x.size(0))\n",
        "        else:\n",
        "            use_idx = torch.where(mask)[0].cpu().numpy()\n",
        "\n",
        "        for i in use_idx:\n",
        "            i = int(i)\n",
        "            has_label = bool(lb[i].item())\n",
        "            y_true = float(y[i].item()) if has_label else np.nan\n",
        "            rows.append({\n",
        "                \"Week\": str(g.week)[:10],\n",
        "                \"Ticker\": g.tickers[i],\n",
        "                \"y_true\": y_true,\n",
        "                \"y_pred\": float(pred[i].detach().cpu().item()),\n",
        "                \"has_label\": has_label\n",
        "            })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "def predict_temporal_on_split(model, graphs_all, device, split=\"test\", t_hist=4, save_all_nodes=False):\n",
        "    \"\"\"Temporal: target is the last graph in each sequence.\"\"\"\n",
        "    seqs = collect_sequences(graphs_all, split, t_hist)\n",
        "    rows = []\n",
        "    model.eval()\n",
        "    for week_t, seq in seqs:\n",
        "        # to device\n",
        "        seq_dev = []\n",
        "        for g in seq:\n",
        "            g_dev = Data(\n",
        "                x=g.x.to(device),\n",
        "                edge_index=g.edge_index.to(device),\n",
        "                edge_weight=(g.edge_weight.to(device) if g.edge_weight is not None else None),\n",
        "                y=g.y.to(device).float()\n",
        "            )\n",
        "            g_dev.label_mask    = g.label_mask.to(device)\n",
        "            g_dev.is_train_week = g.is_train_week.to(device)\n",
        "            g_dev.is_val_week   = g.is_val_week.to(device)\n",
        "            g_dev.is_test_week  = g.is_test_week.to(device)\n",
        "            g_dev.tickers = g.tickers\n",
        "            g_dev.week    = g.week\n",
        "            seq_dev.append(g_dev)\n",
        "\n",
        "        gT = seq_dev[-1]\n",
        "        y  = gT.y\n",
        "        lb = gT.label_mask\n",
        "        split_mask = (gT.is_test_week if split == \"test\" else gT.is_val_week if split == \"val\" else gT.is_train_week)\n",
        "        mask = (lb & split_mask)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred = model(seq_dev)\n",
        "\n",
        "        if save_all_nodes:\n",
        "            use_idx = np.arange(gT.x.size(0))\n",
        "        else:\n",
        "            use_idx = torch.where(mask)[0].cpu().numpy()\n",
        "\n",
        "        for i in use_idx:\n",
        "            i = int(i)\n",
        "            has_label = bool(lb[i].item())\n",
        "            y_true = float(y[i].item()) if has_label else np.nan\n",
        "            rows.append({\n",
        "                \"Week\": str(gT.week)[:10],\n",
        "                \"Ticker\": gT.tickers[i],\n",
        "                \"y_true\": y_true,\n",
        "                \"y_pred\": float(pred[i].detach().cpu().item()),\n",
        "                \"has_label\": has_label\n",
        "            })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "# -------------------- TRAINERS --------------------\n",
        "def train_static(name, model_ctor, feat_dim, device, train_graphs, val_graphs, test_graphs,\n",
        "                 results_dir, epochs, hidden, dropout, lr, wd, patience, debug, debug_epochs):\n",
        "    model = model_ctor(feat_dim, hidden, dropout=dropout).to(device)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    best = {\"val_rmse\": np.inf, \"state\": None, \"epoch\": -1}\n",
        "    logs = []\n",
        "\n",
        "    for ep in range(1, epochs + 1):\n",
        "        tr_loss, tr_m = run_epoch_static(model, train_graphs, device, optimizer=opt,\n",
        "                                         epoch_idx=ep, model_name=name, debug=debug, debug_epochs=debug_epochs)\n",
        "        vl_loss, vl_m = run_epoch_static(model, val_graphs, device, optimizer=None,\n",
        "                                         epoch_idx=ep, model_name=name, debug=debug, debug_epochs=debug_epochs)\n",
        "        logs.append({\n",
        "            \"epoch\": ep,\n",
        "            \"train_loss\": float(tr_loss), \"val_loss\": float(vl_loss),\n",
        "            \"train_rmse\": float(tr_m[\"rmse\"]), \"val_rmse\": float(vl_m[\"rmse\"]),\n",
        "            \"train_mae\":  float(tr_m[\"mae\"]),  \"val_mae\":  float(vl_m[\"mae\"]),\n",
        "            \"train_r2\":   float(tr_m[\"r2\"]),   \"val_r2\":   float(vl_m[\"r2\"]),\n",
        "        })\n",
        "\n",
        "        print(f\"[{name}] epoch {ep:03d} | \"\n",
        "              f\"train RMSE={tr_m['rmse']:.4f} (MSE={tr_loss:.5f}) | \"\n",
        "              f\"val RMSE={vl_m['rmse']:.4f} (MSE={vl_loss:.5f})\")\n",
        "\n",
        "        cur = vl_m[\"rmse\"]\n",
        "        if not np.isnan(cur) and cur < best[\"val_rmse\"] - 1e-6:\n",
        "            best.update({\"val_rmse\": cur,\n",
        "                         \"state\": {k: v.detach().cpu() for k, v in model.state_dict().items()},\n",
        "                         \"epoch\": ep})\n",
        "        elif ep - best[\"epoch\"] >= patience:\n",
        "            print(f\"[{name}] Early stop at epoch {ep}. Best val RMSE={best['val_rmse']:.4f} (epoch {best['epoch']})\")\n",
        "            break\n",
        "\n",
        "    # restore best\n",
        "    if best[\"state\"] is not None:\n",
        "        model.load_state_dict({k: v.to(device) for k, v in best[\"state\"].items()})\n",
        "\n",
        "    # save checkpoint + logs\n",
        "    os.makedirs(results_dir, exist_ok=True)\n",
        "    try:\n",
        "        torch.save(model.state_dict(), os.path.join(results_dir, f\"{name}_best.pt\"))\n",
        "        pd.DataFrame(logs).to_csv(os.path.join(results_dir, f\"{name}_logs.csv\"), index=False)\n",
        "    except Exception as e:\n",
        "        print(f\"[{name}] WARN: saving checkpoint/logs failed: {e}\")\n",
        "\n",
        "    # test metrics\n",
        "    _, ts_m = run_epoch_static(model, test_graphs, device, optimizer=None, epoch_idx=0, model_name=name)\n",
        "\n",
        "    # predictions\n",
        "    try:\n",
        "        preds_scored = predict_static_on_split(model, graphs, device, split=\"test\", save_all_nodes=False)\n",
        "        preds_scored[\"Model\"] = name\n",
        "        preds_scored.to_parquet(os.path.join(results_dir, f\"{name}_test_preds.parquet\"), index=False)\n",
        "\n",
        "        preds_full = predict_static_on_split(model, graphs, device, split=\"test\", save_all_nodes=True)\n",
        "        preds_full[\"Model\"] = name\n",
        "        preds_full.to_parquet(os.path.join(results_dir, f\"{name}_test_preds_full.parquet\"), index=False)\n",
        "\n",
        "        print(f\"[{name}] saved preds: {len(preds_scored)} scored, {len(preds_full)} full\")\n",
        "    except Exception as e:\n",
        "        print(f\"[{name}] WARN: saving predictions failed: {e}\")\n",
        "\n",
        "    return model, {\"val_rmse\": best[\"val_rmse\"], \"test\": ts_m}\n",
        "\n",
        "\n",
        "def train_temporal(name, device, graphs, results_dir, epochs, hidden, dropout, lr, wd, patience,\n",
        "                   t_hist, debug, debug_epochs):\n",
        "    train_seqs = collect_sequences(graphs, \"train\", t_hist)\n",
        "    val_seqs   = collect_sequences(graphs, \"val\",   t_hist)\n",
        "    test_seqs  = collect_sequences(graphs, \"test\",  t_hist)\n",
        "    print(f\"[{name}] sequences → train={len(train_seqs)}, val={len(val_seqs)}, test={len(test_seqs)}\")\n",
        "\n",
        "    feat_dim = graphs[0][1].x.size(1)\n",
        "    model = TemporalGCN(feat_dim, hidden=hidden, dropout=dropout).to(device)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    best = {\"val_rmse\": np.inf, \"state\": None, \"epoch\": -1}\n",
        "    logs = []\n",
        "\n",
        "    for ep in range(1, epochs + 1):\n",
        "        tr_loss, tr_m = run_epoch_temporal(model, train_seqs, device, optimizer=opt,\n",
        "                                           epoch_idx=ep, model_name=name, debug=debug, debug_epochs=debug_epochs)\n",
        "        vl_loss, vl_m = run_epoch_temporal(model, val_seqs, device, optimizer=None,\n",
        "                                           epoch_idx=ep, model_name=name, debug=debug, debug_epochs=debug_epochs)\n",
        "        logs.append({\n",
        "            \"epoch\": ep,\n",
        "            \"train_loss\": float(tr_loss), \"val_loss\": float(vl_loss),\n",
        "            \"train_rmse\": float(tr_m[\"rmse\"]), \"val_rmse\": float(vl_m[\"rmse\"]),\n",
        "            \"train_mae\":  float(tr_m[\"mae\"]),  \"val_mae\":  float(vl_m[\"mae\"]),\n",
        "            \"train_r2\":   float(tr_m[\"r2\"]),   \"val_r2\":   float(vl_m[\"r2\"]),\n",
        "        })\n",
        "\n",
        "        print(f\"[{name}] epoch {ep:03d} | \"\n",
        "              f\"train RMSE={tr_m['rmse']:.4f} (MSE={tr_loss:.5f}) | \"\n",
        "              f\"val RMSE={vl_m['rmse']:.4f} (MSE={vl_loss:.5f})\")\n",
        "\n",
        "        cur = vl_m[\"rmse\"]\n",
        "        if not np.isnan(cur) and cur < best[\"val_rmse\"] - 1e-6:\n",
        "            best.update({\"val_rmse\": cur,\n",
        "                         \"state\": {k: v.detach().cpu() for k, v in model.state_dict().items()},\n",
        "                         \"epoch\": ep})\n",
        "        elif ep - best[\"epoch\"] >= patience:\n",
        "            print(f\"[{name}] Early stop at epoch {ep}. Best val RMSE={best['val_rmse']:.4f} (epoch {best['epoch']})\")\n",
        "            break\n",
        "\n",
        "    if best[\"state\"] is not None:\n",
        "        model.load_state_dict({k: v.to(device) for k, v in best[\"state\"].items()})\n",
        "\n",
        "    os.makedirs(results_dir, exist_ok=True)\n",
        "    try:\n",
        "        torch.save(model.state_dict(), os.path.join(results_dir, f\"{name}_best.pt\"))\n",
        "        pd.DataFrame(logs).to_csv(os.path.join(results_dir, f\"{name}_logs.csv\"), index=False)\n",
        "    except Exception as e:\n",
        "        print(f\"[{name}] WARN: saving checkpoint/logs failed: {e}\")\n",
        "\n",
        "    # test metrics\n",
        "    _, ts_m = run_epoch_temporal(model, test_seqs, device, optimizer=None, epoch_idx=0, model_name=name)\n",
        "\n",
        "    # predictions\n",
        "    try:\n",
        "        preds_scored = predict_temporal_on_split(model, graphs, device, split=\"test\", t_hist=t_hist, save_all_nodes=False)\n",
        "        preds_scored[\"Model\"] = name\n",
        "        preds_scored.to_parquet(os.path.join(results_dir, f\"{name}_test_preds.parquet\"), index=False)\n",
        "\n",
        "        preds_full = predict_temporal_on_split(model, graphs, device, split=\"test\", t_hist=t_hist, save_all_nodes=True)\n",
        "        preds_full[\"Model\"] = name\n",
        "        preds_full.to_parquet(os.path.join(results_dir, f\"{name}_test_preds_full.parquet\"), index=False)\n",
        "\n",
        "        print(f\"[{name}] saved preds: {len(preds_scored)} scored, {len(preds_full)} full\")\n",
        "    except Exception as e:\n",
        "        print(f\"[{name}] WARN: saving predictions failed: {e}\")\n",
        "\n",
        "    return model, {\"val_rmse\": best[\"val_rmse\"], \"test\": ts_m}\n",
        "\n",
        "\n",
        "# -------------------- MAIN --------------------\n",
        "if __name__ == \"__main__\":\n",
        "    args = get_args()\n",
        "    seed_everything(args.seed)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Device:\", device)\n",
        "\n",
        "    GRAPHS_DIR      = args.graphs_dir\n",
        "    GRAPH_INDEX_CSV = args.graph_index_csv\n",
        "    SPLIT_JSON      = args.split_json\n",
        "    RESULTS_DIR     = args.results_dir\n",
        "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "    # Load index & optional split weeks\n",
        "    idx = pd.read_csv(GRAPH_INDEX_CSV)\n",
        "    idx[\"Week\"] = pd.to_datetime(idx[\"Week\"]).dt.tz_localize(None)\n",
        "    idx = idx.sort_values(\"Week\").reset_index(drop=True)\n",
        "\n",
        "    split_weeks = None\n",
        "    if Path(SPLIT_JSON).exists():\n",
        "        with open(SPLIT_JSON, \"r\") as f:\n",
        "            sp = json.load(f)\n",
        "        split_weeks = {\n",
        "            \"train\": set(pd.to_datetime(sp[\"train_weeks\"]).tz_localize(None)),\n",
        "            \"val\":   set(pd.to_datetime(sp[\"val_weeks\"]).tz_localize(None)),\n",
        "            \"test\":  set(pd.to_datetime(sp[\"test_weeks\"]).tz_localize(None)),\n",
        "        }\n",
        "\n",
        "    # Load all graphs\n",
        "    graphs = []\n",
        "    for _, row in idx.iterrows():\n",
        "        fpath = os.path.join(GRAPHS_DIR, row[\"file\"])\n",
        "        if not os.path.exists(fpath):\n",
        "            print(f\"[WARN] Missing graph file: {fpath} (skipping)\")\n",
        "            continue\n",
        "        g = torch.load(fpath, map_location=\"cpu\", weights_only=False)\n",
        "        # tolerate legacy 'train_mask' as label mask\n",
        "        if not hasattr(g, \"label_mask\") and hasattr(g, \"train_mask\"):\n",
        "            g.label_mask = g.train_mask\n",
        "\n",
        "        # attach split flags if missing\n",
        "        if split_weeks is not None:\n",
        "            n = g.x.size(0)\n",
        "            def mask(allw): return torch.tensor([row[\"Week\"] in allw] * n, dtype=torch.bool)\n",
        "            g.is_train_week = mask(split_weeks[\"train\"])\n",
        "            g.is_val_week   = mask(split_weeks[\"val\"])\n",
        "            g.is_test_week  = mask(split_weeks[\"test\"])\n",
        "        else:\n",
        "            g = attach_split_masks_if_missing(g, row[\"Week\"], idx)\n",
        "\n",
        "        # basic sanity\n",
        "        assert torch.isfinite(g.x).all() and torch.isfinite(g.y).all(), \"Found non-finite x/y\"\n",
        "        if hasattr(g, \"edge_weight\") and g.edge_weight is not None:\n",
        "            assert torch.isfinite(g.edge_weight).all(), \"Found non-finite edge_weight\"\n",
        "\n",
        "        graphs.append((row[\"Week\"], g))\n",
        "\n",
        "    assert graphs, \"No graphs found.\"\n",
        "    feat_dim = graphs[0][1].x.size(1)\n",
        "    print(f\"Loaded {len(graphs)} graphs; feature dim = {feat_dim}\")\n",
        "\n",
        "    # Split lists\n",
        "    train_graphs = [(w, g) for (w, g) in graphs if g.is_train_week[0]]\n",
        "    val_graphs   = [(w, g) for (w, g) in graphs if g.is_val_week[0]]\n",
        "    test_graphs  = [(w, g) for (w, g) in graphs if g.is_test_week[0]]\n",
        "    print(f\"Split → train={len(train_graphs)}, val={len(val_graphs)}, test={len(test_graphs)}\")\n",
        "\n",
        "    # Active model constructors (only the ones you left uncommented)\n",
        "    STATIC_MODELS = {\n",
        "        \"WSAGE\": lambda in_dim, hidden, dropout: WeightedSAGEReg(in_dim, hidden, dropout=dropout),\n",
        "        \"GAT\":   lambda in_dim, hidden, dropout: GATReg(in_dim, hidden, heads=4, dropout=dropout),\n",
        "        \"TAG\":   lambda in_dim, hidden, dropout: TAGReg(in_dim, hidden, K=3, dropout=dropout),\n",
        "        \"CHEB\":  lambda in_dim, hidden, dropout: ChebReg(in_dim, hidden, K=3, dropout=dropout),\n",
        "        \"ECC\":   lambda in_dim, hidden, dropout: ECCReg(in_dim, hidden, dropout=dropout),\n",
        "    }\n",
        "\n",
        "    results, trained = [], {}\n",
        "\n",
        "    # Train static models\n",
        "    for name, ctor in STATIC_MODELS.items():\n",
        "        try:\n",
        "            m, stats = train_static(\n",
        "                name, ctor, feat_dim, device,\n",
        "                train_graphs, val_graphs, test_graphs,\n",
        "                RESULTS_DIR,\n",
        "                epochs=args.epochs, hidden=args.hidden, dropout=args.dropout,\n",
        "                lr=args.lr, wd=args.wd, patience=args.patience,\n",
        "                debug=args.debug, debug_epochs=args.debug_epochs\n",
        "            )\n",
        "            trained[name] = m\n",
        "            results.append({\n",
        "                \"Model\": name,\n",
        "                \"Val_RMSE\": round(stats[\"val_rmse\"], 6),\n",
        "                \"Test_RMSE\": round(stats[\"test\"][\"rmse\"], 6),\n",
        "                \"Test_MAE\":  round(stats[\"test\"][\"mae\"], 6),\n",
        "                \"Test_R2\":   round(stats[\"test\"][\"r2\"], 6)\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"[{name}] ERROR during training: {e}\")\n",
        "\n",
        "    # Train temporal model\n",
        "    try:\n",
        "        m, stats = train_temporal(\n",
        "            \"TEMPORAL_GCN\", device, graphs, RESULTS_DIR,\n",
        "            epochs=args.epochs, hidden=args.hidden, dropout=args.dropout,\n",
        "            lr=args.lr, wd=args.wd, patience=args.patience,\n",
        "            t_hist=args.t_hist, debug=args.debug, debug_epochs=args.debug_epochs\n",
        "        )\n",
        "        trained[\"TEMPORAL_GCN\"] = m\n",
        "        results.append({\n",
        "            \"Model\": \"TEMPORAL_GCN\",\n",
        "            \"Val_RMSE\": round(stats[\"val_rmse\"], 6),\n",
        "            \"Test_RMSE\": round(stats[\"test\"][\"rmse\"], 6),\n",
        "            \"Test_MAE\":  round(stats[\"test\"][\"mae\"], 6),\n",
        "            \"Test_R2\":   round(stats[\"test\"][\"r2\"], 6)\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"[TEMPORAL_GCN] ERROR during training: {e}\")\n",
        "\n",
        "    # Summary\n",
        "    summary = pd.DataFrame(results).sort_values(\"Test_RMSE\").reset_index(drop=True)\n",
        "    summary_path = os.path.join(RESULTS_DIR, \"summary.csv\")\n",
        "    summary.to_csv(summary_path, index=False)\n",
        "    print(\"\\n=== SUMMARY (lower is better) ===\")\n",
        "    print(summary)\n",
        "    print(f\"\\nSaved summary → {summary_path}\")\n",
        "\n",
        "    # Combine predictions: scored-only\n",
        "    pred_files = [p for p in os.listdir(RESULTS_DIR) if p.endswith(\"_test_preds.parquet\")]\n",
        "    all_preds = [pd.read_parquet(os.path.join(RESULTS_DIR, p)) for p in pred_files] if pred_files else []\n",
        "    if all_preds:\n",
        "        preds_all = pd.concat(all_preds, ignore_index=True)\n",
        "        preds_all[\"Week\"]     = pd.to_datetime(preds_all[\"Week\"], errors=\"coerce\").dt.tz_localize(None)\n",
        "        preds_all[\"Ticker\"]   = preds_all[\"Ticker\"].astype(\"string\")\n",
        "        preds_all[\"y_true\"]   = pd.to_numeric(preds_all[\"y_true\"], errors=\"coerce\")\n",
        "        preds_all[\"y_pred\"]   = pd.to_numeric(preds_all[\"y_pred\"], errors=\"coerce\")\n",
        "        preds_all[\"has_label\"]= preds_all[\"has_label\"].astype(\"boolean\")\n",
        "        preds_all = preds_all.sort_values([\"Week\", \"Ticker\"]).reset_index(drop=True)\n",
        "        out_path = os.path.join(RESULTS_DIR, \"all_models_test_preds.parquet\")\n",
        "        preds_all.to_parquet(out_path, index=False)\n",
        "        print(\"Saved combined test preds →\", out_path)\n",
        "    else:\n",
        "        print(\"No scored-only prediction files found to combine.\")\n",
        "\n",
        "    # Combine predictions: full-graph\n",
        "    pred_files_full = [p for p in os.listdir(RESULTS_DIR) if p.endswith(\"_test_preds_full.parquet\")]\n",
        "    all_preds_full = [pd.read_parquet(os.path.join(RESULTS_DIR, p)) for p in pred_files_full] if pred_files_full else []\n",
        "    if all_preds_full:\n",
        "        preds_all_full = pd.concat(all_preds_full, ignore_index=True)\n",
        "        preds_all_full[\"Week\"]      = pd.to_datetime(preds_all_full[\"Week\"], errors=\"coerce\").dt.tz_localize(None)\n",
        "        preds_all_full[\"Ticker\"]    = preds_all_full[\"Ticker\"].astype(\"string\")\n",
        "        preds_all_full[\"y_true\"]    = pd.to_numeric(preds_all_full[\"y_true\"], errors=\"coerce\")\n",
        "        preds_all_full[\"y_pred\"]    = pd.to_numeric(preds_all_full[\"y_pred\"], errors=\"coerce\")\n",
        "        preds_all_full[\"has_label\"] = preds_all_full[\"has_label\"].astype(\"boolean\")\n",
        "        preds_all_full = preds_all_full.sort_values([\"Week\", \"Ticker\"]).reset_index(drop=True)\n",
        "        out_path_full = os.path.join(RESULTS_DIR, \"all_models_test_preds_full.parquet\")\n",
        "        preds_all_full.to_parquet(out_path_full, index=False)\n",
        "        print(\"Saved combined FULL test preds →\", out_path_full)\n",
        "    else:\n",
        "        print(\"No full-graph prediction files found to combine.\")\n"
      ],
      "metadata": {
        "id": "zk4OEnD_XaFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Hyper-parameter Tuning (Top-3: ECC, TAG, WSAGE)\n",
        "\n",
        "This section grid-searches compact, sensible hyper-parameter spaces for our three best static GNNs:\n",
        "\n",
        "- **ECC:** (Signed or Abs weights) with small edge MLPs generating per-edge filters.  \n",
        "- **TAG:** with polynomial order $K \\in \\{2,3,4\\}$.  \n",
        "- **Weighted GraphSAGE (WSAGE):** with normalized edge-weighted aggregation.\n",
        "\n",
        "We optimise validation RMSE with early stopping, log every trial, and export:\n",
        "\n",
        "- Per-trial logs and checkpoints under `results_gnn_tune_top3/`.  \n",
        "- Model-specific tuning tables (`tune_ECC.csv`, `tune_TAG.csv`, `tune_WSAGE.csv`).  \n",
        "- A consolidated leaderboard across all three (`top3_tuning_all.csv`).  \n",
        "- The overall best-of-three checkpoint + JSON metadata (`best_of_top3_best.pt`, `best_of_top3_meta.json`).  \n",
        "- Scored-only and full-graph test predictions for the best trial.\n"
      ],
      "metadata": {
        "id": "87PiqzxsbzM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os, json, math, argparse, random, itertools\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import TAGConv, NNConv\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_scatter import scatter_add\n",
        "\n",
        "\n",
        "# -------------------- CLI --------------------\n",
        "def get_args():\n",
        "    p = argparse.ArgumentParser(description=\"Hyper-tune ECC/TAG/WSAGE on weekly graphs.\")\n",
        "    # data IO\n",
        "    p.add_argument(\"--graphs_dir\", type=str, default=\"./graphs\")\n",
        "    p.add_argument(\"--graph_index_csv\", type=str, default=\"./graphs/graphs_index.csv\")\n",
        "    p.add_argument(\"--split_json\", type=str, default=\"./graphs/split_weeks.json\")\n",
        "    # outputs\n",
        "    p.add_argument(\"--results_dir\", type=str, default=\"./results_gnn\")\n",
        "    p.add_argument(\"--tune_dir\", type=str, default=\"./results_gnn_tune_top3\")\n",
        "    # base hparams / early stop\n",
        "    p.add_argument(\"--epochs\", type=int, default=120)\n",
        "    p.add_argument(\"--patience\", type=int, default=20)\n",
        "    # seed & debug\n",
        "    p.add_argument(\"--seed\", type=int, default=42)\n",
        "    p.add_argument(\"--debug\", action=\"store_true\")\n",
        "    p.add_argument(\"--debug_epochs\", type=int, default=2)\n",
        "    # optional throttle\n",
        "    p.add_argument(\"--max_trials_per_model\", type=int, default=None,\n",
        "                   help=\"Optional cap on common-grid trials per model (random subset).\")\n",
        "    return p.parse_args()\n",
        "\n",
        "\n",
        "def seed_everything(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# -------------------- MODELS (only active: WSAGE, TAG, ECC-Signed) --------------------\n",
        "class WeightedSAGEConv(MessagePassing):\n",
        "    \"\"\"Edge-weighted neighbor aggregation with per-destination normalization.\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, aggr='add', bias=True):\n",
        "        super().__init__(aggr=aggr)\n",
        "        self.lin_neigh = nn.Linear(in_channels, out_channels, bias=False)\n",
        "        self.lin_self  = nn.Linear(in_channels, out_channels, bias=bias)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight=None, use_abs_weight=True,\n",
        "                add_self_loops=True, self_loop_weight=1.0):\n",
        "        N = x.size(0)\n",
        "        if edge_weight is None:\n",
        "            edge_weight = torch.ones(edge_index.size(1), device=x.device, dtype=x.dtype)\n",
        "        if use_abs_weight:\n",
        "            edge_weight = edge_weight.abs()\n",
        "\n",
        "        if add_self_loops:\n",
        "            loop = torch.arange(N, device=x.device)\n",
        "            loop_ei = torch.stack([loop, loop], dim=0)\n",
        "            edge_index = torch.cat([edge_index, loop_ei], dim=1)\n",
        "            loop_w = torch.full((N,), float(self_loop_weight), device=x.device, dtype=x.dtype)\n",
        "            edge_weight = torch.cat([edge_weight, loop_w], dim=0)\n",
        "\n",
        "        src, dst = edge_index[0], edge_index[1]\n",
        "        w_sum = scatter_add(edge_weight, dst, dim=0, dim_size=N).clamp_min(1e-12)\n",
        "        norm_w = edge_weight / w_sum[dst]\n",
        "\n",
        "        x_neigh = self.lin_neigh(x)\n",
        "        out = self.propagate(edge_index, x=x_neigh, weight=norm_w) + self.lin_self(x)\n",
        "        return out\n",
        "\n",
        "    def message(self, x_j, weight):  # j: source index\n",
        "        return weight.view(-1, 1) * x_j\n",
        "\n",
        "\n",
        "class WeightedSAGEReg(nn.Module):\n",
        "    def __init__(self, in_dim, hidden=64, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.conv1 = WeightedSAGEConv(in_dim, hidden)\n",
        "        self.conv2 = WeightedSAGEConv(hidden, hidden)\n",
        "        self.lin   = nn.Linear(hidden, 1)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight=None):\n",
        "        x = self.conv1(x, edge_index, edge_weight=edge_weight)\n",
        "        x = F.relu(x); x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.conv2(x, edge_index, edge_weight=edge_weight)\n",
        "        x = F.relu(x); x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return self.lin(x).squeeze(-1)\n",
        "\n",
        "\n",
        "class TAGReg(nn.Module):\n",
        "    def __init__(self, in_dim, hidden=64, K=3, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.conv1 = TAGConv(in_dim, hidden, K=K)\n",
        "        self.conv2 = TAGConv(hidden, hidden, K=K)\n",
        "        self.lin   = nn.Linear(hidden, 1)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight=None):\n",
        "        ew = edge_weight.abs() if edge_weight is not None else None\n",
        "        x = self.conv1(x, edge_index, ew)\n",
        "        x = F.relu(x); x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.conv2(x, edge_index, ew)\n",
        "        x = F.relu(x); x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return self.lin(x).squeeze(-1)\n",
        "\n",
        "\n",
        "class ECCRegSigned(nn.Module):\n",
        "    \"\"\"\n",
        "    ECC variant for tuning that can optionally preserve edge sign (keep_sign=True)\n",
        "    or use absolute weights (keep_sign=False).\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim, hidden=64, dropout=0.2, keep_sign=False, edge_mlp=128):\n",
        "        super().__init__()\n",
        "        self.keep_sign = bool(keep_sign)\n",
        "        self.dropout   = float(dropout)\n",
        "        self.edge_net1 = nn.Sequential(nn.Linear(1, edge_mlp), nn.ReLU(), nn.Linear(edge_mlp, in_dim * hidden))\n",
        "        self.edge_net2 = nn.Sequential(nn.Linear(1, edge_mlp), nn.ReLU(), nn.Linear(edge_mlp, hidden * hidden))\n",
        "        self.conv1 = NNConv(in_dim, hidden, self.edge_net1, aggr='mean')\n",
        "        self.conv2 = NNConv(hidden, hidden, self.edge_net2, aggr='mean')\n",
        "        self.lin   = nn.Linear(hidden, 1)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight=None):\n",
        "        edge_attr = None\n",
        "        if edge_weight is not None:\n",
        "            ea = edge_weight.view(-1, 1)\n",
        "            edge_attr = ea if self.keep_sign else ea.abs()\n",
        "        h = self.conv1(x, edge_index, edge_attr); h = torch.relu(h); h = F.dropout(h, p=self.dropout, training=self.training)\n",
        "        h = self.conv2(h, edge_index, edge_attr); h = torch.relu(h); h = F.dropout(h, p=self.dropout, training=self.training)\n",
        "        return self.lin(h).squeeze(-1)\n",
        "\n",
        "\n",
        "# -------------------- METRICS --------------------\n",
        "def masked_metrics(pred, y, mask):\n",
        "    if mask.sum().item() == 0:\n",
        "        return {\"rmse\": np.nan, \"mae\": np.nan, \"r2\": np.nan}\n",
        "    p = pred[mask].detach().cpu().numpy()\n",
        "    t = y[mask].detach().cpu().numpy()\n",
        "    mse  = float(((p - t) ** 2).mean())\n",
        "    rmse = float(np.sqrt(mse))\n",
        "    mae  = float(np.abs(p - t).mean())\n",
        "    var_t = float(np.var(t))\n",
        "    r2 = float(1.0 - (np.sum((p - t) ** 2) / (np.sum((t - t.mean()) ** 2) + 1e-12))) if var_t > 1e-12 else np.nan\n",
        "    return {\"rmse\": rmse, \"mae\": mae, \"r2\": r2}\n",
        "\n",
        "\n",
        "# -------------------- DATA HELPERS --------------------\n",
        "def attach_split_masks_if_missing(g: Data, week_ts: pd.Timestamp, idx_df: pd.DataFrame):\n",
        "    \"\"\"Attach week-level split flags per node using 70/15/15 chronological split when needed.\"\"\"\n",
        "    n = g.x.size(0)\n",
        "    if hasattr(g, \"is_train_week\") and hasattr(g, \"is_val_week\") and hasattr(g, \"is_test_week\"):\n",
        "        return g\n",
        "    pos = int(idx_df.index[idx_df[\"Week\"] == week_ts][0])\n",
        "    W   = len(idx_df)\n",
        "    i_train_end = math.floor(0.70 * W)\n",
        "    i_val_end   = i_train_end + math.floor(0.15 * W)\n",
        "    g.is_train_week = torch.tensor([pos < i_train_end] * n, dtype=torch.bool)\n",
        "    g.is_val_week   = torch.tensor([(pos >= i_train_end) and (pos < i_val_end)] * n, dtype=torch.bool)\n",
        "    g.is_test_week  = torch.tensor([pos >= i_val_end] * n, dtype=torch.bool)\n",
        "    return g\n",
        "\n",
        "\n",
        "def load_graph_bundle(graphs_dir, graph_index_csv, split_json=None):\n",
        "    \"\"\"Load all graphs + index, normalize dates, enforce splits.\"\"\"\n",
        "    idx = pd.read_csv(graph_index_csv)\n",
        "    idx[\"Week\"] = pd.to_datetime(idx[\"Week\"]).dt.tz_localize(None)\n",
        "    idx = idx.sort_values(\"Week\").reset_index(drop=True)\n",
        "\n",
        "    split_weeks = None\n",
        "    if split_json and Path(split_json).exists():\n",
        "        with open(split_json, \"r\") as f:\n",
        "            sp = json.load(f)\n",
        "        split_weeks = {\n",
        "            \"train\": set(pd.to_datetime(sp[\"train_weeks\"]).tz_localize(None)),\n",
        "            \"val\":   set(pd.to_datetime(sp[\"val_weeks\"]).tz_localize(None)),\n",
        "            \"test\":  set(pd.to_datetime(sp[\"test_weeks\"]).tz_localize(None)),\n",
        "        }\n",
        "\n",
        "    graphs = []\n",
        "    for _, row in idx.iterrows():\n",
        "        fpath = os.path.join(graphs_dir, row[\"file\"])\n",
        "        if not os.path.exists(fpath):\n",
        "            print(f\"[WARN] Missing graph file: {fpath} (skipping)\")\n",
        "            continue\n",
        "        g = torch.load(fpath, map_location=\"cpu\", weights_only=False)\n",
        "\n",
        "        # tolerate legacy label naming\n",
        "        if not hasattr(g, \"label_mask\") and hasattr(g, \"train_mask\"):\n",
        "            g.label_mask = g.train_mask\n",
        "\n",
        "        # attach split flags\n",
        "        if split_weeks is not None:\n",
        "            n = g.x.size(0)\n",
        "            def mask(allw): return torch.tensor([row[\"Week\"] in allw] * n, dtype=torch.bool)\n",
        "            g.is_train_week = mask(split_weeks[\"train\"])\n",
        "            g.is_val_week   = mask(split_weeks[\"val\"])\n",
        "            g.is_test_week  = mask(split_weeks[\"test\"])\n",
        "        else:\n",
        "            g = attach_split_masks_if_missing(g, row[\"Week\"], idx)\n",
        "\n",
        "        assert torch.isfinite(g.x).all() and torch.isfinite(g.y).all(), \"Found non-finite x/y\"\n",
        "        if hasattr(g, \"edge_weight\") and g.edge_weight is not None:\n",
        "            assert torch.isfinite(g.edge_weight).all(), \"Found non-finite edge_weight\"\n",
        "\n",
        "        graphs.append((row[\"Week\"], g))\n",
        "\n",
        "    assert graphs, \"No graphs found.\"\n",
        "    feat_dim = graphs[0][1].x.size(1)\n",
        "    train_graphs = [(w, g) for (w, g) in graphs if g.is_train_week[0]]\n",
        "    val_graphs   = [(w, g) for (w, g) in graphs if g.is_val_week[0]]\n",
        "    test_graphs  = [(w, g) for (w, g) in graphs if g.is_test_week[0]]\n",
        "\n",
        "    return idx, graphs, feat_dim, train_graphs, val_graphs, test_graphs\n",
        "\n",
        "\n",
        "# -------------------- EPOCH LOOPS & PREDICTION --------------------\n",
        "def run_epoch_static(model, graphs_list, device, optimizer=None, epoch_idx=0,\n",
        "                     model_name=\"\", debug=False, debug_epochs=2):\n",
        "    train = optimizer is not None\n",
        "    model.train() if train else model.eval()\n",
        "    total_loss, total_count = 0.0, 0\n",
        "    agg = {\"rmse\": [], \"mae\": [], \"r2\": []}\n",
        "\n",
        "    for week_ts, g in graphs_list:\n",
        "        x  = g.x.to(device)\n",
        "        ei = g.edge_index.to(device)\n",
        "        ew = g.edge_weight.to(device) if hasattr(g, \"edge_weight\") and g.edge_weight is not None else None\n",
        "        y  = g.y.to(device).float()\n",
        "\n",
        "        lb = getattr(g, \"label_mask\", None) or torch.zeros_like(g.y, dtype=torch.bool)\n",
        "        if train:\n",
        "            mask = (lb & g.is_train_week).to(device)\n",
        "        else:\n",
        "            split_mask = g.is_val_week if g.is_val_week[0] else g.is_test_week\n",
        "            mask = (lb & split_mask).to(device)\n",
        "\n",
        "        pred = model(x, ei, edge_weight=ew)\n",
        "\n",
        "        used = int(mask.sum().item()); loss_val = np.nan\n",
        "        if used > 0:\n",
        "            loss = F.mse_loss(pred[mask], y[mask])\n",
        "            loss_val = float(loss.detach().cpu().item())\n",
        "            if train:\n",
        "                optimizer.zero_grad(); loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
        "                optimizer.step()\n",
        "            total_loss += loss_val * used\n",
        "            total_count += used\n",
        "            m = masked_metrics(pred, y, mask)\n",
        "            agg[\"rmse\"].append(m[\"rmse\"]); agg[\"mae\"].append(m[\"mae\"]); agg[\"r2\"].append(m[\"r2\"])\n",
        "\n",
        "        if debug and epoch_idx < debug_epochs:\n",
        "            e_cnt = int(ei.size(1))\n",
        "            ew_min = float(ew.min().item()) if ew is not None and ew.numel() else float(\"nan\")\n",
        "            ew_max = float(ew.max().item()) if ew is not None and ew.numel() else float(\"nan\")\n",
        "            print(f\"[{model_name}][ep{epoch_idx:02d}] {str(week_ts)[:10]} N={x.size(0)} E={e_cnt} \"\n",
        "                  f\"used={used} loss={loss_val:.6f} ew=[{ew_min:.3f},{ew_max:.3f}]\")\n",
        "\n",
        "    mean_loss = (total_loss / total_count) if total_count > 0 else np.nan\n",
        "    mean_metrics = {k: (np.nanmean(v) if len(v) else np.nan) for k, v in agg.items()}\n",
        "    return mean_loss, mean_metrics\n",
        "\n",
        "\n",
        "def predict_static_on_split(model, graphs_list, device, split=\"test\", save_all_nodes=False):\n",
        "    rows = []\n",
        "    model.eval()\n",
        "    for week_ts, g in graphs_list:\n",
        "        if split == \"test\" and not g.is_test_week[0]:   continue\n",
        "        if split == \"val\"  and not g.is_val_week[0]:    continue\n",
        "        if split == \"train\" and not g.is_train_week[0]: continue\n",
        "\n",
        "        x  = g.x.to(device)\n",
        "        ei = g.edge_index.to(device)\n",
        "        ew = g.edge_weight.to(device) if hasattr(g, \"edge_weight\") and g.edge_weight is not None else None\n",
        "        y  = g.y.to(device).float()\n",
        "\n",
        "        lb = getattr(g, \"label_mask\", None) or torch.zeros_like(g.y, dtype=torch.bool)\n",
        "        split_mask = (g.is_test_week if split == \"test\" else g.is_val_week if split == \"val\" else g.is_train_week)\n",
        "        mask = (lb & split_mask).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred = model(x, ei, edge_weight=ew)\n",
        "\n",
        "        use_idx = np.arange(g.x.size(0)) if save_all_nodes else torch.where(mask)[0].cpu().numpy()\n",
        "        for i in use_idx:\n",
        "            i = int(i)\n",
        "            has_label = bool(lb[i].item())\n",
        "            y_true = float(y[i].item()) if has_label else np.nan\n",
        "            rows.append({\n",
        "                \"Week\": str(g.week)[:10],\n",
        "                \"Ticker\": g.tickers[i],\n",
        "                \"y_true\": y_true,\n",
        "                \"y_pred\": float(pred[i].detach().cpu().item()),\n",
        "                \"has_label\": has_label\n",
        "            })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "# -------------------- TRIAL TRAINER --------------------\n",
        "def train_static_hp(name, model_ctor, device, feat_dim, train_graphs, val_graphs, test_graphs,\n",
        "                    *, hidden, dropout, lr, wd, epochs, patience,\n",
        "                    log_prefix=\"\", save_dir=\"./results_gnn_tune_top3\",\n",
        "                    debug=False, debug_epochs=2):\n",
        "    model = model_ctor(feat_dim, int(hidden), float(dropout)).to(device)\n",
        "    opt   = torch.optim.Adam(model.parameters(), lr=float(lr), weight_decay=float(wd))\n",
        "    best  = {\"val_rmse\": np.inf, \"state\": None, \"epoch\": -1}\n",
        "    logs  = []\n",
        "\n",
        "    for ep in range(1, int(epochs) + 1):\n",
        "        tr_loss, tr_m = run_epoch_static(model, train_graphs, device, optimizer=opt,\n",
        "                                         epoch_idx=ep, model_name=name, debug=debug, debug_epochs=debug_epochs)\n",
        "        vl_loss, vl_m = run_epoch_static(model,  val_graphs, device, optimizer=None,\n",
        "                                         epoch_idx=ep, model_name=name, debug=debug, debug_epochs=debug_epochs)\n",
        "\n",
        "        logs.append({\n",
        "            \"epoch\": ep,\n",
        "            \"train_loss\": float(tr_loss), \"val_loss\": float(vl_loss),\n",
        "            \"train_rmse\": float(tr_m[\"rmse\"]), \"val_rmse\": float(vl_m[\"rmse\"]),\n",
        "            \"train_mae\":  float(tr_m[\"mae\"]),  \"val_mae\":  float(vl_m[\"mae\"]),\n",
        "            \"train_r2\":   float(tr_m[\"r2\"]),   \"val_r2\":   float(vl_m[\"r2\"]),\n",
        "        })\n",
        "\n",
        "        cur = vl_m[\"rmse\"]\n",
        "        if not np.isnan(cur) and cur < best[\"val_rmse\"] - 1e-9:\n",
        "            best.update({\"val_rmse\": cur,\n",
        "                         \"state\": {k: v.detach().cpu() for k, v in model.state_dict().items()},\n",
        "                         \"epoch\": ep})\n",
        "        elif ep - best[\"epoch\"] >= int(patience):\n",
        "            break\n",
        "\n",
        "    if best[\"state\"] is not None:\n",
        "        model.load_state_dict({k: v.to(device) for k, v in best[\"state\"].items()})\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    tag = f\"{log_prefix}{name}_H{hidden}_do{dropout}_lr{lr}_wd{wd}\"\n",
        "    try:\n",
        "        torch.save(model.state_dict(), os.path.join(save_dir, f\"{tag}_best.pt\"))\n",
        "        pd.DataFrame(logs).to_csv(os.path.join(save_dir, f\"{tag}_logs.csv\"), index=False)\n",
        "    except Exception as e:\n",
        "        print(f\"[{name}] WARN: save failed: {e}\")\n",
        "\n",
        "    _, ts_m = run_epoch_static(model, test_graphs, device, optimizer=None, epoch_idx=0, model_name=name)\n",
        "    return model, best, ts_m, tag\n",
        "\n",
        "\n",
        "# -------------------- GRID SEARCH RUNNER --------------------\n",
        "def spec_to_tag(spec: dict):\n",
        "    if not spec: return \"base\"\n",
        "    parts = []\n",
        "    for k, v in spec.items():\n",
        "        if isinstance(v, bool): v = \"T\" if v else \"F\"\n",
        "        parts.append(f\"{k}{v}\")\n",
        "    return \"_\".join(parts)\n",
        "\n",
        "\n",
        "def run_grid_for_model(model_name, ctor_factory, model_specific_grid, common_grid, device, feat_dim,\n",
        "                       train_graphs, val_graphs, test_graphs, epochs, patience, save_dir,\n",
        "                       max_trials=None, debug=False, debug_epochs=2):\n",
        "    rows = []\n",
        "    best_global = {\"val_rmse\": np.inf, \"bundle\": None, \"test\": None, \"ckpt\": None, \"tag\": None}\n",
        "\n",
        "    common = list(common_grid)\n",
        "    if (max_trials is not None) and (max_trials < len(common)):\n",
        "        random.seed(42)\n",
        "        common = random.sample(common, max_trials)\n",
        "\n",
        "    for spec in model_specific_grid:\n",
        "        ctor = ctor_factory(**spec) if spec is not None else ctor_factory()\n",
        "        spec_tag = spec_to_tag(spec or {})\n",
        "        for (hidden, dropout, lr, wd) in common:\n",
        "            trial_prefix = f\"{model_name}_{spec_tag}__\"\n",
        "            model, best, ts_m, tag = train_static_hp(\n",
        "                name=model_name, model_ctor=ctor, device=device, feat_dim=feat_dim,\n",
        "                train_graphs=train_graphs, val_graphs=val_graphs, test_graphs=test_graphs,\n",
        "                hidden=hidden, dropout=dropout, lr=lr, wd=wd,\n",
        "                epochs=epochs, patience=patience, log_prefix=trial_prefix, save_dir=save_dir,\n",
        "                debug=debug, debug_epochs=debug_epochs\n",
        "            )\n",
        "            row = {\n",
        "                \"Model\": model_name, **{f\"spec_{k}\": v for k, v in (spec or {}).items()},\n",
        "                \"hidden\": hidden, \"dropout\": dropout, \"lr\": lr, \"wd\": wd,\n",
        "                \"Val_RMSE\": float(best[\"val_rmse\"]),\n",
        "                \"Test_RMSE\": float(ts_m[\"rmse\"]), \"Test_MAE\": float(ts_m[\"mae\"]), \"Test_R2\": float(ts_m[\"r2\"]),\n",
        "                \"best_epoch\": int(best[\"epoch\"]), \"ckpt_tag\": tag\n",
        "            }\n",
        "            rows.append(row)\n",
        "\n",
        "            if best[\"val_rmse\"] < best_global[\"val_rmse\"]:\n",
        "                best_global = {\"val_rmse\": best[\"val_rmse\"], \"bundle\": (spec, hidden, dropout, lr, wd),\n",
        "                               \"test\": ts_m, \"ckpt\": os.path.join(save_dir, f\"{tag}_best.pt\"), \"tag\": tag}\n",
        "\n",
        "    df = pd.DataFrame(rows).sort_values(\"Val_RMSE\").reset_index(drop=True)\n",
        "    out_csv = os.path.join(save_dir, f\"tune_{model_name}.csv\")\n",
        "    df.to_csv(out_csv, index=False)\n",
        "    print(f\"[{model_name}] Saved tuning table → {out_csv}\")\n",
        "    print(f\"[{model_name}] BEST (by Val RMSE):\", best_global[\"bundle\"], \"Val_RMSE=\", best_global[\"val_rmse\"])\n",
        "    return df, best_global\n",
        "\n",
        "\n",
        "# -------------------- MAIN --------------------\n",
        "if __name__ == \"__main__\":\n",
        "    args = get_args()\n",
        "    seed_everything(args.seed)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Device:\", device)\n",
        "\n",
        "    os.makedirs(args.results_dir, exist_ok=True)\n",
        "    os.makedirs(args.tune_dir, exist_ok=True)\n",
        "\n",
        "    # Load graphs\n",
        "    idx, graphs, feat_dim, train_graphs, val_graphs, test_graphs = load_graph_bundle(\n",
        "        args.graphs_dir, args.graph_index_csv, split_json=args.split_json if args.split_json else None\n",
        "    )\n",
        "    print(f\"Loaded {len(graphs)} graphs; feature dim = {feat_dim}\")\n",
        "    print(f\"Split → train={len(train_graphs)}, val={len(val_graphs)}, test={len(test_graphs)}\")\n",
        "\n",
        "    # ----- Search spaces (compact, sensible) -----\n",
        "    HIDDEN_SPACE   = [64, 96, 128]\n",
        "    DROPOUT_SPACE  = [0.1, 0.2, 0.3]\n",
        "    LR_SPACE       = [1e-3, 5e-4]\n",
        "    WD_SPACE       = [1e-4, 5e-5]\n",
        "    COMMON_GRID = list(itertools.product(HIDDEN_SPACE, DROPOUT_SPACE, LR_SPACE, WD_SPACE))\n",
        "\n",
        "    ECC_SPECS   = [{\"keep_sign\": s, \"edge_mlp\": w} for s in [False, True] for w in [64, 128]]\n",
        "    TAG_SPECS   = [{\"K\": K} for K in [2, 3, 4]]\n",
        "    WSAGE_SPECS = [None]  # only common hparams\n",
        "\n",
        "    # ----- Ctor factories -----\n",
        "    make_tag_ctor   = lambda K: (lambda in_dim, hidden, dropout: TAGReg(in_dim, hidden, K=int(K), dropout=float(dropout)))\n",
        "    make_wsage_ctor = lambda : (lambda in_dim, hidden, dropout: WeightedSAGEReg(in_dim, hidden, dropout=float(dropout)))\n",
        "    make_ecc_ctor   = lambda keep_sign=False, edge_mlp=128: (\n",
        "        lambda in_dim, hidden, dropout: ECCRegSigned(in_dim, hidden, dropout=float(dropout),\n",
        "                                                     keep_sign=bool(keep_sign), edge_mlp=int(edge_mlp))\n",
        "    )\n",
        "\n",
        "    # ----- Run tuning for each model -----\n",
        "    ecc_df, ecc_best = run_grid_for_model(\n",
        "        \"ECC\",  make_ecc_ctor,  ECC_SPECS,  COMMON_GRID, device, feat_dim,\n",
        "        train_graphs, val_graphs, test_graphs,\n",
        "        epochs=args.epochs, patience=args.patience, save_dir=args.tune_dir,\n",
        "        max_trials=args.max_trials_per_model, debug=args.debug, debug_epochs=args.debug_epochs\n",
        "    )\n",
        "    tag_df, tag_best = run_grid_for_model(\n",
        "        \"TAG\",  make_tag_ctor,  TAG_SPECS,  COMMON_GRID, device, feat_dim,\n",
        "        train_graphs, val_graphs, test_graphs,\n",
        "        epochs=args.epochs, patience=args.patience, save_dir=args.tune_dir,\n",
        "        max_trials=args.max_trials_per_model, debug=args.debug, debug_epochs=args.debug_epochs\n",
        "    )\n",
        "    ws_df, ws_best = run_grid_for_model(\n",
        "        \"WSAGE\", make_wsage_ctor, WSAGE_SPECS, COMMON_GRID, device, feat_dim,\n",
        "        train_graphs, val_graphs, test_graphs,\n",
        "        epochs=args.epochs, patience=args.patience, save_dir=args.tune_dir,\n",
        "        max_trials=args.max_trials_per_model, debug=args.debug, debug_epochs=args.debug_epochs\n",
        "    )\n",
        "\n",
        "    # ----- Consolidated leaderboard -----\n",
        "    leader = pd.concat([\n",
        "        ecc_df.assign(ModelGroup=\"ECC\"),\n",
        "        tag_df.assign(ModelGroup=\"TAG\"),\n",
        "        ws_df.assign(ModelGroup=\"WSAGE\")\n",
        "    ], ignore_index=True).sort_values(\"Val_RMSE\").reset_index(drop=True)\n",
        "\n",
        "    leader_path = os.path.join(args.tune_dir, \"top3_tuning_all.csv\")\n",
        "    leader.to_csv(leader_path, index=False)\n",
        "    print(\"\\nSaved consolidated leaderboard →\", leader_path)\n",
        "    print(leader.head(10))\n",
        "\n",
        "    # ----- Pick best-of-three and persist -----\n",
        "    all_bests = [(\"ECC\", ecc_best), (\"TAG\", tag_best), (\"WSAGE\", ws_best)]\n",
        "    best_name, best_info = min(all_bests, key=lambda x: x[1][\"val_rmse\"])\n",
        "\n",
        "    best_ckpt   = best_info[\"ckpt\"]\n",
        "    best_tag    = best_info[\"tag\"]\n",
        "    best_bundle = best_info[\"bundle\"]  # (spec, hidden, dropout, lr, wd)\n",
        "    best_test   = best_info[\"test\"]\n",
        "\n",
        "    print(f\"\\n[BEST OF TOP3] {best_name}  Val_RMSE={best_info['val_rmse']:.6f}  Test={best_test}\")\n",
        "\n",
        "    best_clean_ckpt = os.path.join(args.tune_dir, \"best_of_top3_best.pt\")\n",
        "    try:\n",
        "        state = torch.load(best_ckpt, map_location=\"cpu\")\n",
        "        torch.save(state, best_clean_ckpt)\n",
        "    except Exception as e:\n",
        "        print(\"WARN: could not re-save best ckpt:\", e)\n",
        "\n",
        "    meta = {\n",
        "        \"best_model_name\": best_name,\n",
        "        \"ckpt_source_tag\": best_tag,\n",
        "        \"bundle\": {\n",
        "            \"spec\": best_bundle[0], \"hidden\": best_bundle[1], \"dropout\": best_bundle[2],\n",
        "            \"lr\": best_bundle[3], \"wd\": best_bundle[4]\n",
        "        },\n",
        "        \"val_rmse\": float(best_info[\"val_rmse\"]),\n",
        "        \"test_metrics\": {k: float(v) for k, v in best_test.items()}\n",
        "    }\n",
        "    with open(os.path.join(args.tune_dir, \"best_of_top3_meta.json\"), \"w\") as f:\n",
        "        json.dump(meta, f, indent=2)\n",
        "    print(\"Saved best-of-top3 checkpoint + meta in:\", args.tune_dir)\n",
        "\n",
        "    # ----- Save test predictions for the best-of-three (scored-only + full) -----\n",
        "    spec, hidden, dropout, lr, wd = best_bundle\n",
        "    if best_name == \"ECC\":\n",
        "        ctor = make_ecc_ctor(**(spec or {}))\n",
        "    elif best_name == \"TAG\":\n",
        "        ctor = make_tag_ctor(**(spec or {}))\n",
        "    else:\n",
        "        ctor = make_wsage_ctor()\n",
        "\n",
        "    best_model = ctor(feat_dim, int(hidden), float(dropout)).to(device)\n",
        "    best_model.load_state_dict({k: v.to(device) for k, v in state.items()})\n",
        "\n",
        "    try:\n",
        "        preds_scored = predict_static_on_split(best_model, graphs, device, split=\"test\", save_all_nodes=False)\n",
        "        preds_scored[\"Model\"] = f\"BEST_OF_TOP3_{best_name}\"\n",
        "        preds_scored.to_parquet(os.path.join(args.tune_dir, \"best_of_top3_test_preds.parquet\"), index=False)\n",
        "\n",
        "        preds_full = predict_static_on_split(best_model, graphs, device, split=\"test\", save_all_nodes=True)\n",
        "        preds_full[\"Model\"] = f\"BEST_OF_TOP3_{best_name}\"\n",
        "        preds_full.to_parquet(os.path.join(args.tune_dir, \"best_of_top3_test_preds_full.parquet\"), index=False)\n",
        "\n",
        "        print(f\"Saved best-of-top3 predictions → {args.tune_dir}\")\n",
        "    except Exception as e:\n",
        "        print(\"WARN: saving best-of-top3 predictions failed:\", e)\n"
      ],
      "metadata": {
        "id": "3M2QM-iTb1Ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Evaluation Figures & Tables\n",
        "\n",
        "This section compiles all evaluation artifacts for the **baselines** and **GNNs**:\n",
        "\n",
        "**Baselines** (from the baseline pipeline):\n",
        "- Table: overall metrics (CSV + LaTeX).  \n",
        "- Weekly panel RMSE line plot (e.g., VAR vs. BEKK).  \n",
        "- Per-ticker RMSE boxplot.  \n",
        "- Actual vs. predicted scatter plots.  \n",
        "- Diebold-Mariano (DM) tests: overall (from file) + per-ticker (computed here).  \n",
        "\n",
        "**GNNs** (from Section 7/8 outputs):\n",
        "- Grid of learning curves across trained GNNs.  \n",
        "- Best tuned run per model curves.  \n",
        "- Bar chart: Test RMSE before vs. after hyper-parameter tuning (top-3 models).\n",
        "\n"
      ],
      "metadata": {
        "id": "5AogavhzdrH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import glob\n",
        "import json\n",
        "from math import sqrt\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# ----------------------------- CLI ----------------------------- #\n",
        "def get_args():\n",
        "    p = argparse.ArgumentParser(\"Make evaluation figures & tables\")\n",
        "    p.add_argument(\"--base_dir\", type=str, default=\".\")\n",
        "    p.add_argument(\"--baselines_dir\", type=str, default=\"./Baselines\",\n",
        "                   help=\"Where baseline pipeline artifacts live\")\n",
        "    p.add_argument(\"--results_gnn\", type=str, default=\"./results_gnn\",\n",
        "                   help=\"Where Section 7 training artifacts live\")\n",
        "    p.add_argument(\"--tune_dir\", type=str, default=\"./results_gnn_tune_top3\",\n",
        "                   help=\"Where Section 8 tuning artifacts live\")\n",
        "    p.add_argument(\"--fig_dir\", type=str, default=\"./figures\")\n",
        "    p.add_argument(\"--tab_dir\", type=str, default=\"./tables\")\n",
        "    return p.parse_args()\n",
        "\n",
        "\n",
        "# -------------------------- Utilities -------------------------- #\n",
        "def ensure_dirs(*paths):\n",
        "    for p in paths:\n",
        "        os.makedirs(p, exist_ok=True)\n",
        "\n",
        "\n",
        "def _safe_read_csv(path):\n",
        "    if os.path.exists(path):\n",
        "        return pd.read_csv(path)\n",
        "    print(f\"[WARN] Missing CSV: {path}\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def _safe_read_parquet(path):\n",
        "    if os.path.exists(path):\n",
        "        return pd.read_parquet(path)\n",
        "    print(f\"[WARN] Missing Parquet: {path}\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# -------------------- Baseline: Tables & Plots ------------------ #\n",
        "def make_overall_baseline_table(overall_df: pd.DataFrame, tab_dir: str):\n",
        "    \"\"\"Save overall baseline metrics as CSV + LaTeX.\"\"\"\n",
        "    overall_sorted = overall_df.sort_values(\"rmse\").rename(columns=str.upper)\n",
        "    out_csv = os.path.join(tab_dir, \"tab_baseline_overall.csv\")\n",
        "    out_tex = os.path.join(tab_dir, \"tab_baseline_overall.tex\")\n",
        "    overall_sorted.to_csv(out_csv, index=False)\n",
        "    with open(out_tex, \"w\") as f:\n",
        "        f.write(overall_sorted.to_latex(\n",
        "            index=False, float_format=\"%.4f\",\n",
        "            caption=\"Baseline performance on test set (lower is better).\",\n",
        "            label=\"tab:baseline_overall\"\n",
        "        ))\n",
        "    print(\"Saved:\", out_csv, \"|\", out_tex)\n",
        "    return overall_sorted\n",
        "\n",
        "\n",
        "def _panel_losses(preds_long: pd.DataFrame):\n",
        "    \"\"\"Weekly RMSE/MAE aggregation across panel.\"\"\"\n",
        "    g = preds_long.groupby([\"Week\", \"model\"], as_index=False).apply(\n",
        "        lambda x: pd.Series({\n",
        "            \"RMSE\": np.sqrt(np.mean((x[\"y_next\"] - x[\"yhat\"])**2)),\n",
        "            \"MAE\":  np.mean(np.abs(x[\"y_next\"] - x[\"yhat\"]))\n",
        "        })\n",
        "    ).reset_index(drop=True)\n",
        "    return g.sort_values(\"Week\")\n",
        "\n",
        "\n",
        "def fig_weekly_panel_rmse(preds_long: pd.DataFrame, fig_dir: str):\n",
        "    panel = _panel_losses(preds_long)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    for m, g in panel.groupby(\"model\"):\n",
        "        plt.plot(pd.to_datetime(g[\"Week\"]), g[\"RMSE\"], label=m, linewidth=2)\n",
        "    plt.title(\"Weekly Panel RMSE by Baseline Model\")\n",
        "    plt.xlabel(\"Week\"); plt.ylabel(\"RMSE\")\n",
        "    plt.legend(); plt.grid(True, alpha=0.3); plt.tight_layout()\n",
        "    out = os.path.join(fig_dir, \"fig_baseline_weekly_panel_rmse.png\")\n",
        "    plt.savefig(out, dpi=180); plt.show()\n",
        "    print(\"Saved:\", out)\n",
        "\n",
        "\n",
        "def fig_per_ticker_rmse_box(by_ticker: pd.DataFrame, fig_dir: str):\n",
        "    order = sorted(by_ticker[\"model\"].unique())\n",
        "    data = [by_ticker.loc[by_ticker[\"model\"] == m, \"rmse\"].values for m in order]\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    plt.boxplot(data, labels=order, showfliers=False)\n",
        "    plt.title(\"Per-Ticker RMSE Distribution (Baselines)\")\n",
        "    plt.ylabel(\"RMSE\"); plt.grid(True, axis=\"y\", alpha=0.3); plt.tight_layout()\n",
        "    out = os.path.join(fig_dir, \"fig_baseline_box_rmse_by_ticker.png\")\n",
        "    plt.savefig(out, dpi=180); plt.show()\n",
        "    print(\"Saved:\", out)\n",
        "\n",
        "\n",
        "def fig_actual_vs_pred_scatter(preds_long: pd.DataFrame, fig_dir: str):\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    for m in sorted(preds_long[\"model\"].unique()):\n",
        "        g = preds_long[preds_long[\"model\"] == m]\n",
        "        plt.scatter(g[\"y_next\"], g[\"yhat\"], s=10, alpha=0.45, label=m)\n",
        "    lims = [\n",
        "        min(preds_long[\"y_next\"].min(), preds_long[\"yhat\"].min()),\n",
        "        max(preds_long[\"y_next\"].max(), preds_long[\"yhat\"].max())\n",
        "    ]\n",
        "    plt.plot(lims, lims, linewidth=2)  # 45°\n",
        "    plt.xlim(lims); plt.ylim(lims)\n",
        "    plt.xlabel(\"Actual spillover (y_next)\")\n",
        "    plt.ylabel(\"Predicted spillover (ŷ)\")\n",
        "    plt.title(\"Actual vs Predicted — Baselines\")\n",
        "    plt.legend(); plt.grid(True, alpha=0.3); plt.tight_layout()\n",
        "    out = os.path.join(fig_dir, \"fig_baseline_scatter_actual_vs_pred.png\")\n",
        "    plt.savefig(out, dpi=180); plt.show()\n",
        "    print(\"Saved:\", out)\n",
        "\n",
        "\n",
        "# ------------------------- DM Test Utils ------------------------ #\n",
        "def _newey_west_var(d, max_lag=None):\n",
        "    d = np.asarray(d, dtype=float)\n",
        "    T = len(d)\n",
        "    if T == 0:\n",
        "        return np.nan\n",
        "    if max_lag is None:\n",
        "        max_lag = int(np.floor(4 * (T / 100.0)**(2/9)))\n",
        "    gamma0 = np.dot(d, d) / T\n",
        "    s = gamma0\n",
        "    for h in range(1, max_lag + 1):\n",
        "        w = 1.0 - h / (max_lag + 1)\n",
        "        gamma = np.dot(d[h:], d[:-h]) / T\n",
        "        s += 2 * w * gamma\n",
        "    return s\n",
        "\n",
        "\n",
        "def _dm_test_series(e1, e2, max_lag=None):\n",
        "    d = (e1**2 - e2**2)\n",
        "    d_bar = d.mean()\n",
        "    var_nw = _newey_west_var(d - d.mean(), max_lag=max_lag)\n",
        "    if not np.isfinite(var_nw) or var_nw <= 0:\n",
        "        return np.nan, np.nan, len(d)\n",
        "    stat = d_bar / sqrt(var_nw / len(d))\n",
        "    from math import erf\n",
        "    def norm_cdf(z): return 0.5 * (1 + erf(z / np.sqrt(2)))\n",
        "    p = 2 * (1 - norm_cdf(abs(stat)))\n",
        "    return stat, p, len(d)\n",
        "\n",
        "\n",
        "def dm_per_ticker_VAR_vs_BEKK(preds_long: pd.DataFrame, tab_dir: str, dm_panel_df: pd.DataFrame | None):\n",
        "    # Pivot into per-model columns and align\n",
        "    pl = preds_long.pivot_table(index=[\"Week\", \"Ticker\"], columns=\"model\", values=[\"y_next\", \"yhat\"])\n",
        "    pl.columns = [f\"{a}_{b}\" for a, b in pl.columns]  # e.g., y_next_VAR-FEVD\n",
        "    needed = [\"y_next_VAR-FEVD\", \"yhat_VAR-FEVD\", \"y_next_S-BEKK(1,1)\", \"yhat_S-BEKK(1,1)\"]\n",
        "    pl = pl.dropna(subset=[c for c in needed if c in pl.columns], how=\"any\").reset_index()\n",
        "\n",
        "    rows = []\n",
        "    if all(c in pl.columns for c in needed):\n",
        "        for tk, g in pl.groupby(\"Ticker\"):\n",
        "            y = g[\"y_next_VAR-FEVD\"].values  # same as BEKK y_next after alignment\n",
        "            e_var  = (y - g[\"yhat_VAR-FEVD\"].values)\n",
        "            e_bekk = (y - g[\"yhat_S-BEKK(1,1)\"].values)\n",
        "            if len(e_var) >= 12:\n",
        "                stat, p, n = _dm_test_series(e_var, e_bekk, max_lag=None)\n",
        "                rows.append({\"ticker\": tk, \"DM_stat_VAR_vs_BEKK\": stat, \"p_value\": p, \"n_weeks\": n})\n",
        "    else:\n",
        "        print(\"[WARN] Could not find both VAR-FEVD and S-BEKK(1,1) columns for DM per-ticker test.\")\n",
        "\n",
        "    dm_by_ticker = pd.DataFrame(rows).sort_values(\"p_value\") if rows else pd.DataFrame(columns=[\"ticker\",\"DM_stat_VAR_vs_BEKK\",\"p_value\",\"n_weeks\"])\n",
        "    out_per = os.path.join(tab_dir, \"tab_dm_per_ticker.csv\")\n",
        "    dm_by_ticker.to_csv(out_per, index=False)\n",
        "    print(\"Saved per-ticker DM:\", out_per)\n",
        "\n",
        "    if dm_panel_df is not None:\n",
        "        out_panel = os.path.join(tab_dir, \"tab_dm_overall_panel.csv\")\n",
        "        dm_panel_df.to_csv(out_panel, index=False)\n",
        "        print(\"Saved overall DM (panel):\", out_panel)\n",
        "\n",
        "    return dm_by_ticker\n",
        "\n",
        "\n",
        "# -------------------- GNN Learning Curves (Grid) ------------------- #\n",
        "def learning_curves_grid(results_dir: str, fig_out_png: str, fig_out_pdf: str,\n",
        "                         smooth_alpha: float = 0.2, ncols: int = 3, sharey: bool = True):\n",
        "    log_files = sorted(glob.glob(os.path.join(results_dir, \"*_logs.csv\")))\n",
        "    if not log_files:\n",
        "        print(f\"[WARN] No '*_logs.csv' files found in {results_dir}\")\n",
        "        return\n",
        "\n",
        "    # drop MLP explicitly\n",
        "    log_files = [f for f in log_files if \"MLP\" not in os.path.basename(f)]\n",
        "\n",
        "    order_pref = [\"ECC\", \"TAG\", \"WSAGE\", \"CHEB\", \"GAT\", \"TEMPORAL_GCN\"]\n",
        "    name2path = {os.path.basename(p).replace(\"_logs.csv\", \"\"): p for p in log_files}\n",
        "    names = [n for n in order_pref if n in name2path] + [n for n in name2path if n not in order_pref]\n",
        "\n",
        "    import math\n",
        "    n = len(names)\n",
        "    nrows = math.ceil(n / ncols)\n",
        "\n",
        "    # precompute global y-lims\n",
        "    import numpy as np\n",
        "    ymins, ymaxs, dfs = [], [], {}\n",
        "    for name in names:\n",
        "        df = pd.read_csv(name2path[name])\n",
        "        for col in [\"train_rmse\", \"val_rmse\"]:\n",
        "            if col in df:\n",
        "                df[f\"{col}_ema\"] = df[col].ewm(alpha=smooth_alpha).mean()\n",
        "        dfs[name] = df\n",
        "        ys = []\n",
        "        for col in [\"train_rmse\", \"val_rmse\", \"train_rmse_ema\", \"val_rmse_ema\"]:\n",
        "            if col in df:\n",
        "                ys.append(df[col].values)\n",
        "        if ys:\n",
        "            arr = np.concatenate(ys)\n",
        "            ymins.append(np.nanmin(arr)); ymaxs.append(np.nanmax(arr))\n",
        "    ylo = float(np.nanmin(ymins)) if sharey and ymins else None\n",
        "    yhi = float(np.nanmax(ymaxs)) if sharey and ymaxs else None\n",
        "    if sharey and ylo is not None and yhi is not None:\n",
        "        pad = 0.05 * (yhi - ylo + 1e-9)\n",
        "        ylo, yhi = ylo - pad, yhi + pad\n",
        "\n",
        "    fig, axes = plt.subplots(nrows, ncols, figsize=(5 * ncols, 3.8 * nrows), squeeze=False)\n",
        "    for idx, name in enumerate(names):\n",
        "        r, c = divmod(idx, ncols)\n",
        "        ax = axes[r][c]\n",
        "        df = dfs[name]\n",
        "        if \"train_rmse\" in df: ax.plot(df[\"epoch\"], df[\"train_rmse\"], label=\"train RMSE\", linewidth=1.2)\n",
        "        if \"val_rmse\"   in df: ax.plot(df[\"epoch\"], df[\"val_rmse\"],   label=\"val RMSE\",   linewidth=1.2)\n",
        "        if \"train_rmse_ema\" in df: ax.plot(df[\"epoch\"], df[\"train_rmse_ema\"], label=\"train RMSE (EMA)\", linestyle=\"--\")\n",
        "        if \"val_rmse_ema\"   in df: ax.plot(df[\"epoch\"], df[\"val_rmse_ema\"],   label=\"val RMSE (EMA)\",   linestyle=\"--\")\n",
        "        ax.set_title(name, fontsize=14, pad=6)\n",
        "        ax.set_xlabel(\"Epoch\"); ax.set_ylabel(\"RMSE\")\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        if sharey and ylo is not None and yhi is not None:\n",
        "            ax.set_ylim(ylo, yhi)\n",
        "        if idx == 0: ax.legend(loc=\"upper right\", fontsize=10)\n",
        "\n",
        "    # remove empty panels\n",
        "    for k in range(len(names), nrows * ncols):\n",
        "        r, c = divmod(k, ncols)\n",
        "        fig.delaxes(axes[r][c])\n",
        "\n",
        "    fig.suptitle(\"Learning Curves — Selected GNNs\", fontsize=16, y=0.995)\n",
        "    plt.tight_layout(rect=[0, 0.00, 1, 0.97])\n",
        "    fig.savefig(fig_out_png, dpi=220)\n",
        "    fig.savefig(fig_out_pdf)\n",
        "    plt.show()\n",
        "    print(\"Saved:\", fig_out_png, \"|\", fig_out_pdf)\n",
        "\n",
        "\n",
        "# ---------------- Best tuned curves + tuned-vs-default ------------- #\n",
        "def best_tuned_curves(tune_dir: str, results_gnn: str):\n",
        "    path = os.path.join(tune_dir, \"best_by_model.csv\")\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"[WARN] Missing best_by_model.csv at {path}; skipping best tuned curves.\")\n",
        "        return\n",
        "    best_by_model = pd.read_csv(path)\n",
        "    for _, row in best_by_model.iterrows():\n",
        "        logs_path = row.get(\"logs_path\")\n",
        "        if not isinstance(logs_path, str) or not os.path.exists(logs_path):\n",
        "            print(f\"Skip plotting (no logs): {logs_path}\")\n",
        "            continue\n",
        "        logs = pd.read_csv(logs_path)\n",
        "        plt.figure()\n",
        "        if \"train_rmse\" in logs: plt.plot(logs[\"epoch\"], logs[\"train_rmse\"], label=\"train RMSE\")\n",
        "        if \"val_rmse\"   in logs: plt.plot(logs[\"epoch\"], logs[\"val_rmse\"],   label=\"val RMSE\")\n",
        "        plt.title(f\"Tuned {row['Model']} — RMSE (best run)\")\n",
        "        plt.xlabel(\"Epoch\"); plt.ylabel(\"RMSE\"); plt.legend(); plt.grid(True, alpha=0.3)\n",
        "        out = os.path.join(results_gnn, f\"fig_tuned_{row['Model']}_learning_curve.png\")\n",
        "        plt.savefig(out, dpi=180); plt.show()\n",
        "        print(\"Saved:\", out)\n",
        "\n",
        "\n",
        "def tuned_vs_default_bar(results_gnn: str, tune_dir: str, top3=None):\n",
        "    if top3 is None:\n",
        "        top3 = [\"ECC\", \"TAG\", \"WSAGE\"]\n",
        "    default_path = os.path.join(results_gnn, \"summary.csv\")\n",
        "    tuned_path   = os.path.join(tune_dir, \"best_by_model.csv\")\n",
        "\n",
        "    if not (os.path.exists(default_path) and os.path.exists(tuned_path)):\n",
        "        print(\"[WARN] Missing summary.csv or best_by_model.csv; skipping tuned-vs-default plot.\")\n",
        "        return\n",
        "\n",
        "    default = pd.read_csv(default_path)\n",
        "    tuned   = pd.read_csv(tuned_path)\n",
        "    default = default[default[\"Model\"].isin(top3)]\n",
        "    tuned   = tuned[tuned[\"Model\"].isin(top3)]\n",
        "\n",
        "    default[\"Version\"] = \"Default\"\n",
        "    tuned[\"Version\"]   = \"Tuned\"\n",
        "\n",
        "    df = pd.concat([default[[\"Model\", \"Test_RMSE\", \"Version\"]],\n",
        "                    tuned[[\"Model\", \"Test_RMSE\", \"Version\"]]])\n",
        "\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    df.pivot(index=\"Model\", columns=\"Version\", values=\"Test_RMSE\").plot(kind=\"bar\", ax=plt.gca(), width=0.7)\n",
        "    plt.ylabel(\"Test RMSE\")\n",
        "    plt.title(\"Test RMSE before vs after hyperparameter tuning (Top-3)\")\n",
        "    plt.grid(axis=\"y\", alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    out = os.path.join(tune_dir, \"fig_rmse_before_after_tuning.png\")\n",
        "    plt.savefig(out, dpi=180); plt.show()\n",
        "    print(\"Saved:\", out)\n",
        "\n",
        "\n",
        "# ---------------------------  MLP -------------------------- #\n",
        "def optional_mlp_plots(results_gnn: str):\n",
        "    \"\"\"If MLP artifacts exist, make its learning curve and scatter plot.\"\"\"\n",
        "    # learning curve(s)\n",
        "    mlp_logs = glob.glob(os.path.join(results_gnn, \"MLP_logs.csv\"))\n",
        "    for f in mlp_logs:\n",
        "        df = pd.read_csv(f)\n",
        "        # EMA smoothing\n",
        "        for col in [\"train_rmse\", \"val_rmse\", \"train_loss\", \"val_loss\"]:\n",
        "            if col in df:\n",
        "                df[f\"{col}_ema\"] = df[col].ewm(alpha=0.2).mean()\n",
        "        plt.figure()\n",
        "        if \"train_rmse\" in df: plt.plot(df[\"epoch\"], df[\"train_rmse\"], label=\"train RMSE\", linewidth=1)\n",
        "        if \"val_rmse\"   in df: plt.plot(df[\"epoch\"], df[\"val_rmse\"],   label=\"val RMSE\",   linewidth=1)\n",
        "        if \"train_rmse_ema\" in df: plt.plot(df[\"epoch\"], df[\"train_rmse_ema\"], label=\"train RMSE (EMA)\", linestyle=\"--\")\n",
        "        if \"val_rmse_ema\"   in df: plt.plot(df[\"epoch\"], df[\"val_rmse_ema\"],   label=\"val RMSE (EMA)\",   linestyle=\"--\")\n",
        "        plt.title(\"MLP — RMSE\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"RMSE\")\n",
        "        plt.legend(); plt.grid(True, alpha=0.3)\n",
        "        out = os.path.join(results_gnn, \"fig_mlp_learning_curve.png\")\n",
        "        plt.savefig(out, dpi=180); plt.show()\n",
        "        print(\"Saved:\", out)\n",
        "\n",
        "    # scatter\n",
        "    mlp_preds = os.path.join(results_gnn, \"MLP_test_preds.parquet\")\n",
        "    if os.path.exists(mlp_preds):\n",
        "        df = pd.read_parquet(mlp_preds)  # labeled only\n",
        "        x, y = df[\"y_true\"].values, df[\"y_pred\"].values\n",
        "        lims = [min(x.min(), y.min()), max(x.max(), y.max())]\n",
        "        plt.figure(figsize=(5.5, 5.5))\n",
        "        plt.scatter(x, y, s=10, alpha=0.5)\n",
        "        plt.plot(lims, lims, linewidth=2)  # 45°\n",
        "        plt.xlim(lims); plt.ylim(lims)\n",
        "        plt.xlabel(\"Actual spillover (y)\"); plt.ylabel(\"Predicted (ŷ)\")\n",
        "        plt.title(\"Actual vs Predicted — MLP\")\n",
        "        plt.grid(True, alpha=0.3); plt.tight_layout()\n",
        "        out = os.path.join(results_gnn, \"fig_mlp_scatter_actual_vs_pred.png\")\n",
        "        plt.savefig(out, dpi=180); plt.show()\n",
        "        print(\"Saved:\", out)\n",
        "\n",
        "\n",
        "# ------------------------------ MAIN ------------------------------ #\n",
        "def main():\n",
        "    args = get_args()\n",
        "    fig_dir = args.fig_dir\n",
        "    tab_dir = args.tab_dir\n",
        "\n",
        "    ensure_dirs(fig_dir, tab_dir)\n",
        "\n",
        "    # ---- Baseline artifacts ----\n",
        "    overall_path       = os.path.join(args.baselines_dir, \"metrics_overall.csv\")\n",
        "    by_ticker_path     = os.path.join(args.baselines_dir, \"metrics_by_ticker.csv\")\n",
        "    preds_unified_path = os.path.join(args.baselines_dir, \"preds_unified_long.parquet\")\n",
        "    dm_panel_path      = os.path.join(args.baselines_dir, \"dm_test.csv\")\n",
        "\n",
        "    overall_df   = _safe_read_csv(overall_path)\n",
        "    by_ticker_df = _safe_read_csv(by_ticker_path)\n",
        "    preds_long   = _safe_read_parquet(preds_unified_path)\n",
        "    dm_panel_df  = _safe_read_csv(dm_panel_path) if os.path.exists(dm_panel_path) else None\n",
        "\n",
        "    if overall_df is not None:\n",
        "        make_overall_baseline_table(overall_df, tab_dir)\n",
        "    if preds_long is not None:\n",
        "        fig_weekly_panel_rmse(preds_long, fig_dir)\n",
        "        fig_actual_vs_pred_scatter(preds_long, fig_dir)\n",
        "    if by_ticker_df is not None:\n",
        "        fig_per_ticker_rmse_box(by_ticker_df, fig_dir)\n",
        "    if preds_long is not None:\n",
        "        dm_per_ticker_VAR_vs_BEKK(preds_long, tab_dir, dm_panel_df)\n",
        "\n",
        "    # ---- GNN learning curves (grid across models) ----\n",
        "    out_png = os.path.join(args.results_gnn, \"fig_learning_curves_grid_noMLP.png\")\n",
        "    out_pdf = os.path.join(args.results_gnn, \"fig_learning_curves_grid_noMLP.pdf\")\n",
        "    learning_curves_grid(args.results_gnn, out_png, out_pdf, smooth_alpha=0.2, ncols=3, sharey=True)\n",
        "\n",
        "    # ---- Best tuned curves (if tuning artifacts exist) ----\n",
        "    # also creates best_by_model.csv if you run the Section 8 script\n",
        "    best_tuned_curves(args.tune_dir, args.results_gnn)\n",
        "\n",
        "    # ---- Tuned vs default RMSE comparison (top-3) ----\n",
        "    tuned_vs_default_bar(args.results_gnn, args.tune_dir, top3=[\"ECC\", \"TAG\", \"WSAGE\"])\n",
        "\n",
        "    # ---- Optional: if MLP artifacts exist, plot them too ----\n",
        "    optional_mlp_plots(args.results_gnn)\n",
        "\n",
        "    print(\"\\nAll done.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "o1GY54DoeJxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Final Aggregation & Graph Visualisation\n",
        "\n",
        "This section consolidates all evaluation results into a single table (CSV + LaTeX): Baselines, the default GNN runs from Section 7, and the hyper-tuned top-3 from Section 8.\n",
        "It also loads the best model discovered during tuning (by Test RMSE), locates its test predictions parquet, and renders a weekly graph where node colors/sizes reflect predicted spillover for that week.\n",
        "If multiple tuned models’ predictions are available, you can optionally produce a side-by-side panel to compare their spatial risk patterns."
      ],
      "metadata": {
        "id": "9KJbVairfIAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os, re, glob, argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import networkx as nx\n",
        "\n",
        "# ----------------------------- CLI ----------------------------- #\n",
        "def get_args():\n",
        "    p = argparse.ArgumentParser(\"Final outputs: tables + graph overlays\")\n",
        "    p.add_argument(\"--base_dir\", type=str, default=\".\")\n",
        "    p.add_argument(\"--baselines_dir\", type=str, default=\"./Baselines\")\n",
        "    p.add_argument(\"--results_gnn\", type=str, default=\"./results_gnn\")\n",
        "    p.add_argument(\"--tune_dir\", type=str, default=\"./results_gnn_tune_top3\")\n",
        "    p.add_argument(\"--graphs_dir\", type=str, default=\"./graphs\")\n",
        "    p.add_argument(\"--graph_index_csv\", type=str, default=\"./graphs/graphs_index.csv\")\n",
        "    p.add_argument(\"--fig_dir\", type=str, default=\"./figures\")\n",
        "    p.add_argument(\"--save_panels\", action=\"store_true\", help=\"Save multi-model panel as PNG\")\n",
        "    return p.parse_args()\n",
        "\n",
        "\n",
        "# ----------------------- Utilities & I/O ----------------------- #\n",
        "def ensure_dirs(*paths):\n",
        "    for p in paths:\n",
        "        os.makedirs(p, exist_ok=True)\n",
        "\n",
        "def first_existing(paths):\n",
        "    for p in paths:\n",
        "        if p and os.path.exists(p):\n",
        "            return p\n",
        "    return None\n",
        "\n",
        "def _normalize_weeks(df):\n",
        "    w = pd.to_datetime(df[\"Week\"], errors=\"coerce\")\n",
        "    df = df.copy()\n",
        "    df[\"_week_dt\"]   = w\n",
        "    df[\"_week_date\"] = w.dt.date.astype(\"string\")\n",
        "    df[\"_week_str\"]  = df[\"Week\"].astype(\"string\")\n",
        "    return df\n",
        "\n",
        "def tokens(name):\n",
        "    base = os.path.basename(name).lower()\n",
        "    return re.split(r\"[^a-z0-9]+\", base)\n",
        "\n",
        "def infer_model_from_filename(path):\n",
        "    toks = tokens(path)\n",
        "    if \"wsage\" in toks or (\"weighted\" in toks and \"sage\" in toks):\n",
        "        return \"WSAGE\"\n",
        "    if \"ecc\" in toks:\n",
        "        return \"ECC\"\n",
        "    if \"tag\" in toks and \"best\" not in toks:  # avoid matching 'best_of_top3'\n",
        "        return \"TAG\"\n",
        "    return None\n",
        "\n",
        "def find_preds_file_for_model(tune_dir, model_raw, ckpt_tag=None):\n",
        "    model_raw = str(model_raw).upper().strip()\n",
        "    if not os.path.isdir(tune_dir):\n",
        "        return None\n",
        "    cands = []\n",
        "    for fname in os.listdir(tune_dir):\n",
        "        if not fname.lower().endswith(\".parquet\"):\n",
        "            continue\n",
        "        if \"test\" not in fname.lower() or \"pred\" not in fname.lower():\n",
        "            continue\n",
        "        path = os.path.join(tune_dir, fname)\n",
        "        m_from_name = infer_model_from_filename(path)\n",
        "        if m_from_name == model_raw:\n",
        "            cands.append(path)\n",
        "        elif ckpt_tag and ckpt_tag.lower() in fname.lower():\n",
        "            cands.append(path)\n",
        "    # prefer *full* predictions\n",
        "    cands_full = [p for p in cands if p.lower().endswith(\"test_preds_full.parquet\")]\n",
        "    return sorted(cands_full)[0] if cands_full else (sorted(cands)[0] if cands else None)\n",
        "\n",
        "def load_and_normalize_preds(parquet_path, model_label=None):\n",
        "    df = pd.read_parquet(parquet_path)\n",
        "    df.columns = [str(c).strip() for c in df.columns]\n",
        "    PRED_CANDS = [\"yhat\",\"y_hat\",\"y_pred\",\"pred\",\"prediction\",\"preds\",\"pred_mean\",\"yhat_pred\",\"y_pred_mean\"]\n",
        "    TRUE_CANDS = [\"y_next\",\"ytrue\",\"y_true\",\"label\",\"target\",\"y\"]\n",
        "    def _find_col(cands, cols):\n",
        "        cl = {c.lower(): c for c in cols}\n",
        "        for c in cands:\n",
        "            if c.lower() in cl:\n",
        "                return cl[c.lower()]\n",
        "        return None\n",
        "    pred_col = _find_col(PRED_CANDS, df.columns)\n",
        "    true_col = _find_col(TRUE_CANDS, df.columns)\n",
        "    if pred_col is None:\n",
        "        raise ValueError(f\"No prediction column found in {parquet_path}.\")\n",
        "    rename_map = {pred_col: \"yhat\"}\n",
        "    if true_col is not None:\n",
        "        rename_map[true_col] = \"y_next\"\n",
        "    df = df.rename(columns=rename_map)\n",
        "    if \"Model\" not in df.columns:\n",
        "        df[\"Model\"] = model_label if model_label else \"MODEL\"\n",
        "    if \"Week\" not in df.columns or \"Ticker\" not in df.columns:\n",
        "        raise ValueError(f\"Missing Week/Ticker columns in {parquet_path}\")\n",
        "    df[\"Week\"] = pd.to_datetime(df[\"Week\"], errors=\"coerce\")\n",
        "    # Collapse accidental duplicates\n",
        "    agg = {\"yhat\":\"mean\"}\n",
        "    if \"y_next\" in df.columns: agg[\"y_next\"] = \"mean\"\n",
        "    return df.groupby([\"Week\",\"Ticker\",\"Model\"], as_index=False).agg(agg)\n",
        "\n",
        "def load_all_model_preds_if_available(tune_dir, rows_from_best_csv):\n",
        "    dfs = []\n",
        "    for _, r in rows_from_best_csv.iterrows():\n",
        "        m = str(r[\"Model\"]).upper().strip()\n",
        "        ckpt_tag = str(r[\"ckpt_tag\"]) if \"ckpt_tag\" in r and pd.notna(r[\"ckpt_tag\"]) else None\n",
        "        p = find_preds_file_for_model(tune_dir, m, ckpt_tag=ckpt_tag)\n",
        "        if p is None:\n",
        "            print(f\"[WARN] No preds parquet for {m}.\")\n",
        "            continue\n",
        "        try:\n",
        "            dfm = load_and_normalize_preds(p, model_label=m)\n",
        "            dfs.append(dfm)\n",
        "            print(f\"[INFO] Loaded preds for {m}: {os.path.basename(p)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] Failed to load preds for {m}: {e}\")\n",
        "    return pd.concat(dfs, ignore_index=True) if dfs else None\n",
        "\n",
        "\n",
        "# --------------------- Combined Evaluation Table -------------------- #\n",
        "def assign_family(m):\n",
        "    if m in [\"S-BEKK\", \"VAR-FEVD\"]:\n",
        "        return \"Baseline\"\n",
        "    elif m == \"MLP\":\n",
        "        return \"MLP\"\n",
        "    elif m in [\"ECC\",\"TAG\",\"WSAGE\",\"CHEB\",\"GAT\",\"GCN\",\"SAGE\",\"GraphSAGE\",\"ChebNet\",\"GATv2\",\"TEMPORAL_GCN\",\"TAGConv\"]:\n",
        "        return \"GNN (Selected)\"\n",
        "    else:\n",
        "        return \"Other\"\n",
        "\n",
        "def build_eval_table(baselines_dir, results_gnn, tune_dir):\n",
        "    # Load sources\n",
        "    metrics_overall = pd.read_csv(os.path.join(baselines_dir, \"metrics_overall.csv\"))\n",
        "    summary         = pd.read_csv(os.path.join(results_gnn, \"summary.csv\"))\n",
        "    tuned_csv       = os.path.join(tune_dir, \"best_by_model.csv\")\n",
        "    tuned = pd.read_csv(tuned_csv) if os.path.exists(tuned_csv) else None\n",
        "\n",
        "    # Normalise columns\n",
        "    metrics_overall = metrics_overall.rename(columns={\"model\":\"Model\",\"rmse\":\"Test_RMSE\",\"mae\":\"Test_MAE\",\"r2\":\"Test_R2\"})\n",
        "    metrics_overall[\"Val_RMSE\"] = None\n",
        "\n",
        "    summary[\"Family\"] = summary[\"Model\"].map(assign_family)\n",
        "    metrics_overall[\"Family\"] = \"Baseline\"\n",
        "\n",
        "    frames = [metrics_overall, summary]\n",
        "    if tuned is not None:\n",
        "        need = [\"Model\",\"Val_RMSE\",\"Test_RMSE\",\"Test_MAE\",\"Test_R2\"]\n",
        "        tuned = tuned.rename(columns={c:c for c in need})\n",
        "        tuned = tuned[need].copy()\n",
        "        tuned[\"Family\"] = \"GNN (Hyper-tuned)\"\n",
        "        tuned[\"Model\"] = tuned[\"Model\"].astype(str) + \" (tuned)\"\n",
        "        frames.append(tuned)\n",
        "\n",
        "    cols = [\"Family\",\"Model\",\"Val_RMSE\",\"Test_RMSE\",\"Test_MAE\",\"Test_R2\"]\n",
        "    eval_all = pd.concat([f[cols] for f in frames], ignore_index=True).drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "    cat_order = pd.CategoricalDtype([\"Baseline\",\"MLP\",\"GNN (Selected)\",\"GNN (Hyper-tuned)\"], ordered=True)\n",
        "    eval_all[\"Family\"] = eval_all[\"Family\"].astype(cat_order)\n",
        "    eval_all = eval_all.sort_values([\"Family\",\"Test_RMSE\",\"Test_MAE\"]).reset_index(drop=True)\n",
        "    return eval_all\n",
        "\n",
        "\n",
        "# ---------------------- Graph Loading & Drawing --------------------- #\n",
        "def load_graph_index(graph_index_csv):\n",
        "    gi = pd.read_csv(graph_index_csv)\n",
        "    return _normalize_weeks(gi)\n",
        "\n",
        "def load_week_graph(graphs_dir, graph_index, week):\n",
        "    # Match by date string or exact timestamp\n",
        "    w_dt = pd.to_datetime(week, errors=\"coerce\")\n",
        "    w_date = (w_dt.date().isoformat() if pd.notna(w_dt) else str(week))\n",
        "    w_str = str(week)\n",
        "\n",
        "    row = graph_index.loc[graph_index[\"_week_date\"] == w_date]\n",
        "    if row.empty: row = graph_index.loc[graph_index[\"_week_str\"] == w_str]\n",
        "    if row.empty and pd.notna(w_dt): row = graph_index.loc[graph_index[\"_week_dt\"] == w_dt]\n",
        "    if row.empty:\n",
        "        raise FileNotFoundError(f\"Week {week} not found in index.\")\n",
        "\n",
        "    fpath = os.path.join(graphs_dir, row.iloc[0][\"file\"])\n",
        "    g = torch.load(fpath, weights_only=False, map_location=\"cpu\")\n",
        "\n",
        "    edges_np = g.edge_index.cpu().numpy().T\n",
        "    w_np     = g.edge_weight.cpu().numpy() if getattr(g, \"edge_weight\", None) is not None else np.zeros(len(edges_np))\n",
        "    tickers  = list(g.tickers)\n",
        "\n",
        "    G = nx.DiGraph()\n",
        "    for i, tk in enumerate(tickers):\n",
        "        lab = float(g.y[i].item()) if (getattr(g, \"y\", None) is not None and i < len(g.y)) else np.nan\n",
        "        G.add_node(i, ticker=tk, label=lab)\n",
        "    for (s, d), w in zip(edges_np, w_np):\n",
        "        G.add_edge(int(s), int(d), weight=float(w))\n",
        "    return G, tickers, g\n",
        "\n",
        "def spring_layout_cached(G, seed=42):\n",
        "    # Compute layout once\n",
        "    return nx.spring_layout(G, seed=seed, k=1.7, iterations=100)\n",
        "\n",
        "def overlay_node_values(G, pos, values_by_idx, *, title, cmap=\"viridis\", size_scale=1600, alpha=0.95, vmin=None, vmax=None, edge_alpha=0.25, save_path=None):\n",
        "    vals = np.array([values_by_idx.get(i, np.nan) for i in G.nodes], dtype=float)\n",
        "    if vmin is None: vmin = np.nanmin(vals)\n",
        "    if vmax is None: vmax = np.nanmax(vals)\n",
        "    sizes = 300 + size_scale * (np.nan_to_num(vals - vmin) / (vmax - vmin + 1e-9))\n",
        "\n",
        "    eweights = np.array([G[u][v].get(\"weight\", 0.0) for u, v in G.edges()], dtype=float)\n",
        "    norm_vals = (eweights + 1.0) / 2.0\n",
        "    edge_colors = plt.cm.bwr(norm_vals)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    nx.draw_networkx_edges(G, pos, ax=ax, edge_color=edge_colors, alpha=edge_alpha)\n",
        "\n",
        "    sc = nx.draw_networkx_nodes(\n",
        "        G, pos, ax=ax, node_color=vals, cmap=cmap,\n",
        "        node_size=sizes, alpha=alpha, edgecolors=\"black\", linewidths=0.8,\n",
        "        vmin=vmin, vmax=vmax\n",
        "    )\n",
        "    nx.draw_networkx_labels(G, pos, ax=ax, labels={i: G.nodes[i][\"ticker\"] for i in G.nodes}, font_size=8)\n",
        "\n",
        "    cb = plt.colorbar(sc, ax=ax); cb.set_label(\"Predicted spillover\")\n",
        "    ax.set_title(title); ax.axis(\"off\"); plt.tight_layout()\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=200)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# -------------------- Best Model & Predictions --------------------- #\n",
        "def pick_example_week(preds_full, graph_index):\n",
        "    ws_preds = pd.to_datetime(preds_full[\"Week\"].unique())\n",
        "    ws_graph = pd.to_datetime(graph_index[\"_week_dt\"].unique())\n",
        "    inter = sorted(set(ws_preds).intersection(set(ws_graph)))\n",
        "    return inter[-1] if inter else ws_preds[-1]\n",
        "\n",
        "def node_values_from_preds(preds_df, model_name, week_dt, tickers, kind=\"yhat\"):\n",
        "    df = preds_df[(preds_df[\"Model\"].astype(str).str.upper() == str(model_name).upper()) &\n",
        "                  (preds_df[\"Week\"] == pd.to_datetime(week_dt))].copy()\n",
        "    if df.empty:\n",
        "        return {i: np.nan for i in range(len(tickers))}\n",
        "    df = df.groupby(\"Ticker\", as_index=True)[kind].mean().to_frame()\n",
        "    vals = {}\n",
        "    for i, tk in enumerate(tickers):\n",
        "        vals[i] = float(df.loc[tk, kind]) if tk in df.index else np.nan\n",
        "    return vals\n",
        "\n",
        "\n",
        "# ------------------------------- MAIN ------------------------------ #\n",
        "def main():\n",
        "    args = get_args()\n",
        "    ensure_dirs(args.fig_dir)\n",
        "\n",
        "    # 1) Build & save the combined evaluation table\n",
        "    eval_all = build_eval_table(args.baselines_dir, args.results_gnn, args.tune_dir)\n",
        "    out_csv = os.path.join(args.results_gnn, \"table_eval_all_models_incl_mlp_tuned.csv\")\n",
        "    out_tex = os.path.join(args.results_gnn, \"table_eval_all_models_incl_mlp_tuned.tex\")\n",
        "    eval_all.to_csv(out_csv, index=False)\n",
        "    with open(out_tex, \"w\") as f:\n",
        "        f.write(eval_all.to_latex(index=False, float_format=\"%.4f\",\n",
        "                                  caption=\"Evaluation metrics on the test set (baselines, MLP, selected GNNs, and hyper-tuned GNNs).\",\n",
        "                                  label=\"tab:eval_all_models_tuned\"))\n",
        "    print(\"Saved:\", out_csv, \"|\", out_tex)\n",
        "    print(eval_all.head(12))\n",
        "\n",
        "    # 2) Load best tuned model row (if available)\n",
        "    best_csv_candidates = [\n",
        "        os.path.join(args.tune_dir, \"best_by_model.csv\"),\n",
        "        \"/mnt/data/best_by_model.csv\",\n",
        "    ]\n",
        "    best_csv_path = first_existing(best_csv_candidates)\n",
        "    if best_csv_path is None:\n",
        "        print(\"[WARN] No best_by_model.csv found; skipping graph visualisation.\")\n",
        "        return\n",
        "    best_df = pd.read_csv(best_csv_path)\n",
        "    for col in [\"Val_RMSE\",\"Test_RMSE\",\"Test_MAE\",\"Test_R2\"]:\n",
        "        if col in best_df.columns:\n",
        "            best_df[col] = pd.to_numeric(best_df[col], errors=\"coerce\")\n",
        "    best_df = best_df.sort_values(by=[\"Test_RMSE\",\"Val_RMSE\",\"Test_R2\"], ascending=[True, True, False], kind=\"mergesort\").reset_index(drop=True)\n",
        "    best_model_raw = str(best_df.loc[0, \"Model\"]).upper().strip()\n",
        "    best_ckpt_tag = str(best_df.loc[0, \"ckpt_tag\"]) if \"ckpt_tag\" in best_df.columns and pd.notna(best_df.loc[0, \"ckpt_tag\"]) else None\n",
        "    print(f\"[INFO] Best model by Test_RMSE: {best_model_raw}\")\n",
        "\n",
        "    # 3) Load predictions parquet for best model (fallback: best_of_top3 file)\n",
        "    preds_path = find_preds_file_for_model(args.tune_dir, best_model_raw, ckpt_tag=best_ckpt_tag)\n",
        "    preds_full = None\n",
        "    if preds_path:\n",
        "        preds_full = load_and_normalize_preds(preds_path, model_label=best_model_raw)\n",
        "        print(f\"[INFO] Using predictions: {os.path.basename(preds_path)}\")\n",
        "    else:\n",
        "        fallback = os.path.join(args.tune_dir, \"best_of_top3_test_preds_full.parquet\")\n",
        "        if os.path.exists(fallback):\n",
        "            preds_full = load_and_normalize_preds(fallback, model_label=\"UNKNOWN\")\n",
        "            print(f\"[WARN] Using fallback predictions: {os.path.basename(fallback)}\")\n",
        "        else:\n",
        "            print(\"[WARN] No predictions parquet found; skipping visualisation.\")\n",
        "            return\n",
        "\n",
        "    # 4) Load graph index + a target week\n",
        "    graph_index = load_graph_index(args.graph_index_csv)\n",
        "    week = pick_example_week(preds_full, graph_index)\n",
        "    G, tickers, tg = load_week_graph(args.graphs_dir, graph_index, week)\n",
        "    pos = spring_layout_cached(G, seed=42)\n",
        "\n",
        "    # 5) Overlay: single best model\n",
        "    vals_best = node_values_from_preds(preds_full, best_model_raw, week, tickers, kind=\"yhat\")\n",
        "    overlay_node_values(\n",
        "        G, pos, vals_best,\n",
        "        title=f\"{best_model_raw} — predictions (Week {pd.to_datetime(week).date()})\",\n",
        "        cmap=\"viridis\", size_scale=1600,\n",
        "        save_path=os.path.join(args.fig_dir, f\"graph_overlay_{best_model_raw}_{pd.to_datetime(week).date()}.png\")\n",
        "    )\n",
        "\n",
        "    # 6) Multi-panel comparison across tuned models (if preds exist)\n",
        "    combined = load_all_model_preds_if_available(args.tune_dir, best_df.copy())\n",
        "    if combined is not None:\n",
        "        # sort models by Test_RMSE for ordering\n",
        "        mlist = (best_df.sort_values(by=[\"Test_RMSE\",\"Val_RMSE\",\"Test_R2\"], ascending=[True, True, False])\n",
        "                        [\"Model\"].astype(str).tolist())\n",
        "        # shared color scale\n",
        "        all_vals = []\n",
        "        for m in mlist:\n",
        "            d = combined[(combined[\"Model\"].astype(str).str.upper()==m.upper()) &\n",
        "                         (combined[\"Week\"]==pd.to_datetime(week))]\n",
        "            if not d.empty:\n",
        "                all_vals.extend(d[\"yhat\"].values.tolist())\n",
        "        if all_vals:\n",
        "            vmin, vmax = float(np.min(all_vals)), float(np.max(all_vals))\n",
        "        else:\n",
        "            vmin, vmax = None, None\n",
        "\n",
        "        k = len(mlist)\n",
        "        fig, axes = plt.subplots(1, k, figsize=(5*k, 8))\n",
        "        axes = [axes] if k == 1 else axes.ravel()\n",
        "\n",
        "        eweights = np.array([G[u][v].get(\"weight\", 0.0) for u, v in G.edges()], dtype=float)\n",
        "        norm_vals = (eweights + 1.0) / 2.0\n",
        "        edge_colors = plt.cm.bwr(norm_vals)\n",
        "\n",
        "        sc = None\n",
        "        for ax, m in zip(axes, mlist):\n",
        "            d = combined[(combined[\"Model\"].astype(str).str.upper()==m.upper()) &\n",
        "                         (combined[\"Week\"]==pd.to_datetime(week))]\n",
        "            if d.empty:\n",
        "                ax.set_axis_off(); ax.set_title(f\"{m}\\n(no preds)\"); continue\n",
        "            d = d.groupby(\"Ticker\", as_index=True)[\"yhat\"].mean().to_frame()\n",
        "            vmap = {i: float(d.loc[tk, \"yhat\"]) if tk in d.index else np.nan for i, tk in enumerate(tickers)}\n",
        "            vals = np.array([vmap.get(i, np.nan) for i in G.nodes], dtype=float)\n",
        "            sizes = 300 + 1600 * (np.nan_to_num(vals - (vmin if vmin is not None else np.nanmin(vals))) /\n",
        "                                  ((vmax if vmax is not None else np.nanmax(vals)) - (vmin if vmin is not None else np.nanmin(vals)) + 1e-9))\n",
        "            nx.draw_networkx_edges(G, pos, ax=ax, edge_color=edge_colors, alpha=0.25)\n",
        "            sc = nx.draw_networkx_nodes(\n",
        "                G, pos, ax=ax, node_color=vals, cmap=\"viridis\",\n",
        "                node_size=sizes, alpha=0.95, edgecolors=\"black\", linewidths=0.8,\n",
        "                vmin=vmin, vmax=vmax\n",
        "            )\n",
        "            nx.draw_networkx_labels(G, pos, ax=ax, labels={i: G.nodes[i][\"ticker\"] for i in G.nodes}, font_size=7)\n",
        "            ax.set_title(f\"{m}\\nWeek {pd.to_datetime(week).date()}\"); ax.axis(\"off\")\n",
        "\n",
        "        cbar = fig.colorbar(sc, ax=axes, fraction=0.02, pad=0.02)\n",
        "        cbar.set_label(\"Predicted spillover\")\n",
        "        plt.tight_layout()\n",
        "        if args.save_panels:\n",
        "            outp = os.path.join(args.fig_dir, f\"graph_overlay_panel_{pd.to_datetime(week).date()}.png\")\n",
        "            plt.savefig(outp, dpi=200)\n",
        "            print(\"Saved:\", outp)\n",
        "        plt.show()\n",
        "\n",
        "    print(\"\\nDone.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "J3aSVX5cfHZ_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}